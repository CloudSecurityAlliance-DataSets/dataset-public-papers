# Navigating the Risks: A Survey of Security, Privacy, and Ethics Threats in **LLM-Based Agents**

YUYOU GAN, YONG YANG, ZHE MA, PING HE, RUI ZENG, YIMING WANG, QINGMING LI,

and CHUNYI ZHOU, Zhejiang University, China

SONGZE LI, Southeast University, China

TING WANG, Stony Brook University, USA

YUNJUN GAO, YINGCAI WU, and SHOULING JI, Zhejiang University, China

With the continuous development of large language models (LLMs), transformer-based models have made groundbreaking advances in numerous natural language processing (NLP) tasks, leading to the emergence of a series of agents that use LLMs as their control hub. While LLMs have achieved success in various tasks, they face numerous security and privacy threats, which become even more severe in the agent scenarios. To enhance the reliability of LLM-based applications, a range of research has emerged to assess and mitigate these risks from different perspectives.

To help researchers gain a comprehensive understanding of various risks, this survey collects and analyzes the different threats faced by these agents. To address the challenges posed by previous taxonomies in handling cross-module and cross-stage threats, we propose a novel taxonomy framework based on the sources and impacts. Additionally, we identify six key features of LLM-based agents, based on which we summarize the current research progress and analyze their limitations. Subsequently, we select four representative agents as case studies to analyze the risks they may face in practical use. Finally, based on the aforementioned analyses, we propose future research directions from the perspectives of data, methodology, and policy, respectively.

CCS Concepts: • Security and privacy;

Additional Key Words and Phrases: LLM-based agents, Security, Privacy, Ethics

#### **ACM Reference Format:**

Yuyou Gan, Yong Yang, Zhe Ma, Ping He, Rui Zeng, Yiming Wang, Qingming Li, Chunyi Zhou, Songze Li, Ting Wang, Yunjun Gao, Yingcai Wu, and Shouling Ji. 2018. Navigating the Risks: A Survey of Security, Privacy, and Ethics Threats in LLM-Based Agents. 1, 1 (November 2018), 48 pages. https://doi.org/XXXXXXX.XXXXXXX

#### 1 Introduction

With the continuous development of language models (LMs), LLMs based on the transformer architecture [259] have achieved significant success in various fields of NLP  $[62, 214]$ . The massive number of parameters and extensive training data endow LLMs with strong capabilities in tasks like text generation  $[24, 253]$ , code assistance  $[79, 272]$ ,

Authors' Contact Information: Yuyou Gan, ganyuyou@zju.edu.cn; Yong Yang, yangyong2022@zju.edu.cn; Zhe Ma, mz.rs@zju.edu.cn; Ping He, gnip@zju. edu.cn; Rui Zeng, ruizeng24@zju.edu.cn; Yiming Wang, ym wang@zju.edu.cn; Qingming Li, liqm@zju.edu.cn; Chunyi Zhou, zhouchunyi@zju.edu.cn, Zhejiang University, Hangzhou, Zhejiang, China; Songze Li, songzeli@seu.edu.cn, Southeast University, Nanjing, Jiangsu, China; Ting Wang, twang@ cs.stonybrook.edu, Stony Brook University, Stony Brook, New York, USA; Yunjun Gao, gaoyj@zju.edu.cn; Yingcai Wu, ycwu@zju.edu.cn; Shouling Ji, sji@zju.edu.cn, Zhejiang University, Hangzhou, Zhejiang, China.

© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. Manuscript submitted to ACM

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.

| <b>Multi-source Inputs</b><br><b>Environment</b>                          | <b>Threat Source</b>      | Security/Safety                                  | Privacy                                                       | <b>Ethic</b>        |
|---------------------------------------------------------------------------|---------------------------|--------------------------------------------------|---------------------------------------------------------------|---------------------|
| <b>User Input</b><br><b>Feedback</b><br>System<br><b>Memory</b><br>Prompt | <b>Problematic Inputs</b> | • Adversarial Example •<br><b>Goal Hijacking</b> | <b>Model Extraction</b><br><b>Prompt Leakage</b><br>$\bullet$ | <b>Jailbreaking</b> |
| Large Language Model<br>Reasoning                                         | <b>Model Flaws</b>        | <b>Hallucination</b>                             | <b>Memorization</b><br>$\bullet$                              | <b>Bias</b><br>٠    |
| Planning<br><b>Reflection</b><br>                                         | <b>Combined Threats •</b> | <b>Backdoor</b>                                  | <b>Privacy Leakage</b>                                        |                     |

Fig. 1. The overall framework of our taxonomy for the risks of LLM-based agents.

logical reasoning [277, 297], etc. Due to their powerful understanding capabilities, an increasing number of studies are positioning LLMs as the core decision-making hub of AI agents [189, 235], which are sophisticated software programs designed to autonomously perform tasks on behalf of users or other systems. Compared to earlier AI agents based on heuristic algorithms or reinforcement learning [154, 185], LLM-based agents can communicate with users, making them easier to understand and accept. Additionally, their vast foundational knowledge allows them to think in a manner similar to humans (understanding + planning). These characteristics contribute to their popularity, making them a promising direction for AI to serve various practical fields [263, 310, 317]. For example, Supertools [247] is a comprehensive collection of trending applications empowered by LLMs.

Despite the significant success of LLMs, they also face security and privacy threats due to inner vulnerabilities or outer attacks. LLM-based agents add some components and functionalities, which makes these risks even more threatening. For example, LLMs face jailbreaking attacks [168, 233], which refer to the process to bypass their built-in safety mechanisms [17, 198]. In the context of LLM-based agents, LLMs need to handle multi-round dialogues and multiple sources of information, making jailbreaking attacks more complex and difficult to defend against [12, 47]. To uncover the vulnerabilities of LLM-based agents and make them more secure and reliable, an increasing number of studies focus on the threats from various perspectives.

To help researchers better understand LLM-based agents and pursue future research work, there exist two surveys [53, 61] to summarize the security risks of LLM-based agents. They categorize the security risks based on the composition (called modules) or operational phases (called stages) of the agents as follows. (i) The module perspectives. Cui et al. [53] identified four key modules in LLM-based systems, i.e., the input module, the LM module, the toolchain module, and the output module. They summarize the risks of LLM-based agents based on the four modules. (ii) The stage perspectives. Deng et al. [61] identified four key knowledge gaps in LLM-based AI agents, i.e., the stage of perception, the stage of internal execution, the stage of action in environment, and the stage of interaction with untrusted external entities. They summarize the risks of LLM-based agents based on the four stages. These two taxonomies clearly highlight the sources of attacks faced by LLM-based agents. However, they struggle to accurately pinpoint threats that span across modules and stages. For example, privacy leakage is caused by memory issues within the language model module, but it occurs at the output module. Similarly, goal hijacking can happen not only during the perception stage but also during the interaction stage with external data [5]. These cross-module and cross-stage threats are inaccurately pinpointed to a single module or stage.

![](_page_2_Figure_1.jpeg)

Fig. 2. An overall framework of LLM-based agents.

Our Design. To categorize various threats of LLM-based agents more accurately and comprehensively, we propose a novel taxonomy by mapping the threats into a binary table **based on their sources and types**. (i) For the sources of threats, we consider the operational nature of LLM-based agents: LLMs make decisions based on inputs from multiple sources, as shown in Fig. 1 left. As a probabilistic model, the output decision distribution of an LLM is determined by both the input and the model itself. Therefore, we attribute the threats to LLM-based agents to the inputs, the model, or a combination of both. Compared to categorizing attacks by modules or stages, our classification of sources is closer to the essence of the threat. For example, goal hijacking [109, 169, 213] may originate from a user input or an external database, but both fundamentally act as inputs to the model for hijacking the goal. (ii) For the types of threats, we categorize the threats into three classes: security/safety, privacy, and ethics. Specifically, if a threat results in the model producing incorrect outputs (including errors that are factually inaccurate or do not align with the needs of developers or users), it is categorized as a security/safety issue, such as adversarial examples [299]. If a threat leads to the leakage of privacy, it is classified as a privacy issue, such as prompt leakage attacks [303, 323]. If a threat does not produce "incorrect" outputs but raises concerns such as unfairness, it falls under ethical issues, such as bias [81].

We collect papers from the top conferences and highly cited arXiv papers. Top conferences are included but not limited: IEEE S&P, ACM CCS, USENIX Security, NDSS, ACL, CVPR, NIPS, ICML, and ICLR. We categorize different kinds of threats with our taxonomy in Fig. 1 right. For threats originating from inputs, we refer to them as problematic inputs. In this scenario, attackers cannot modify the model but can design inputs to induce malicious outputs or behaviors, e.g., the adversarial example. For threats from within the model, we refer to them as model flaws. In this scenario, the inputs are always benign, but the model's own defects lead to malicious outputs or behaviors, e.g., the hallucination problem. For threats arising from both model flaws and carefully crafted inputs, we refer to them as combined threats. In this scenario, the inputs are deliberately designed by attackers to exploit the model's vulnerabilities, e.g., the backdoor attack.

Our Contributions. Compared with recent surveys [53, 61] on the security risks of LLM-based agents, there are three main advantages of our work.

(i) A Novel Taxonomy of Threats. We propose a novel taxonomy that maps threats into a binary table based on their sources and impacts, which can comprehensively cover the existing threats and extend to future threats, including the cross-module and cross-stage threats.

(ii) Detailed Analysis of Multi-modal Large Language Models (MLLMs). Many tasks require agents to handle inputs from multiple modalities, (e.g., city navigation systems [310]), leading to the emergence of a range of MLLMs and agents based on these models [295, 310]. Previous surveys primarily focus on the text modality, lacking analysis of multimodal models. We cover both LLMs and MLLMs, placing particular emphasis on analyzing the new challenges and threats posed by multimodal tasks in the context of threats.

(iii) Four Carefully Selected Case Studies. Previous surveys analyze the risks based on a general framework of LLM-based agents (or systems). However, actual agents may not necessarily contain all modules in the general framework, and the designs within these modules may also be customized [66]. More importantly, the scenarios they face have significant differences, resulting in the varying levels and causes of threats. To help readers better understand the actual threats faced by agents, we present case studies of four different agents, representing four classic situations in Section 6.

This paper is organized as follows. Section 2 introduces a general framework of LLM-based agents and identifies six key features of the framework. Sections 3, 4, and 5 depict the risks from problematic inputs, model flaws, and input-model interaction, respectively. Section 6 offers four carefully selected case studies. Section 7 gives future directions for the development of this field.

# 2 LLM-based Agent

AI agents are considered promising a research direction that utilize AI technology to autonomously execute specific tasks and make decisions. In previous researches, AI agents often achieved good results in specific scenarios (such as playing games) through heuristic strategies or reinforcement learning [154][185][226][279]. In recent years, LLMs, such as ChatGPT, have attracted substantial attention from both academia, and industry, due to their remarkable performance on various NLP tasks [24][321][259]. Therefore, there is an increasing amount of work studying the use of LLMs as the decision-making center for AI agents [189][9][200]. With the development of LLMs, LLMs can handle more modalities and tasks [235].

Framework of LLM-based Agents. In our work, we consider a comprehensive framework of an LLM-based agent that covers the modules and runtime modes of mainstream LLM-based agents, as shown in Fig. 2. This framework contains the following four modules.

(i) Input Module (IM). IM receives the users' inputs and preprocesses them as follows. First, IM formats the inputs to a specific distribution (e.g., normalize an input image) or a specific format (e.g., a special language [229]). Second, IM implements harmful information detection [221][293] or purification [229]. Third, many LLM-based agents add a system prompt before the inputs [189][235].

(ii) Decision Module (DM). DM understands and analyzes the query of the user, gives plans and generates the final response to the user. Many agents' decision modules only contain one LLM. They leverage an LLM for understanding, planning, and feedback [189][235], or use an LLM for understanding and feedback, with another non-LLM planner handling the planning [159][54]. As tasks become more complex, many agents employ multiple LLMs to accomplish the aforementioned tasks. For example, VOYAGER [263] uses GPT-4 and GPT-3.5 to handle the tasks of understanding, Manuscript submitted to ACM

# Navigating the Risks: A Survey of Security, Privacy, and Ethics Threats in LLM-Based Agents

![](_page_4_Figure_1.jpeg)

Fig. 3. Six key features of LLM-based agents: LLM-based controller, multi-modal inputs and outputs, multi-source inputs, multi-round interaction, memory mechanism and tool invocation.

planning, and generating a skill library, respectively. Huang et al. [106] used GPT-3 for task planning, while leveraging BERT to translate the plans into admissible actions.

(iii) External Entities (EE). With the task becoming more complex, the agents need the help of the external modules, including memory module, external tools, the other agents and the environment. The memory module is used to store and retrieve relevant information to improve the coherence and context-awareness of the agent's responses. In this paper, we adopt the definition of agents' memory from [327], considering external databases as a form of agents' memory as well. External tools integrate numerous APIs to fulfill the user's requirements (e.g., search engine APIs for Webgpt [189] and APIs for controlling the robotic arm [9]). Sometimes, multiple agents need to collaborate to complete a task, where one agent needs to interact with other agents [98].

(iv) Output Module (OM). There might be zero or multiple interaction between DM and EE to accomplish the task. After that, DM generates the response and delivers it to the user through OM. Agents can implement harmful information detection or purification on the output [103].

Based on this framework, we identify six key features of LLM-based agents, which involve new attack surfaces compared with the traditional DNN models and the RL-based agents. As shown in Fig. 3, these six key features are as follows. (i) LLM-based controller. LLMs serve as the core of agents, leveraging transformer architecture, vast amounts of knowledge, and massive training data to confer strong understanding capabilities, while also introducing new risks. (ii) Multi-modal inputs and outputs. As agents become capable of handling increasingly complex tasks, many scenarios require the processing of multimodal information. Research indicates that risks vary across different modalities, and their interaction in multimodal systems presents unique challenges and opportunities. (iii) Multi-source inputs. The inputs to LLMs within agents consist of multiple components from different sources, such as user input, system prompts, memory, and environmental feedback. Compared to a standalone LLM, multi-source inputs present new opportunities and challenges for both attackers and defenders. (iv) Multi-round interaction. Agents often require multiple rounds of interaction (with the environment, users, other LLMs, etc.) to complete tasks, leading to longer and more complex inputs for LLMs, which may exacerbate certain threats. (v) Memory mechanism. The memory mechanisms in agents can help accumulate experience and enhance knowledge, improving their ability to handle various tasks, but they also introduce new security and privacy risks. (vi) Tool invocation. LLMs are specially crafted with instruction-tuning data designed for tool usage. This process enables LLMs to handle complex tasks, but it may also result in more severe consequences and introduce new vulnerabilities.

| <b>Threats</b>         | LLM-based<br>Controller | Multi-modal<br>Inputs and<br>Outputs | Multi-source<br><b>Inputs</b> | Multi-round<br>Interaction | Memory<br>Mechanism | <b>Tool Invocation</b> |
|------------------------|-------------------------|--------------------------------------|-------------------------------|----------------------------|---------------------|------------------------|
| Adversarial<br>Example | V                       | $\sqrt{ }$                           | $\sqrt{ }$                    | $\sqrt{ }$                 |                     |                        |
| <b>Goal Hijacking</b>  | $\sqrt{ }$              | $\sqrt{ }$                           | $\sqrt{ }$                    | $\sqrt{ }$                 | $\sqrt{ }$          | $\sqrt{ }$             |
| <b>Prompt Leakage</b>  | $\sqrt{ }$              | $\sqrt{ }$                           | $\sqrt{ }$                    |                            |                     | $\sqrt{ }$             |
| Model<br>Extraction    | ν                       |                                      |                               |                            |                     |                        |
| Jailbreaking           | $\sqrt{ }$              | $\sqrt{ }$                           | $\sqrt{ }$                    | $\sqrt{ }$                 |                     | $\sqrt{ }$             |
| Hallucination          | V                       | $\sqrt{ }$                           | $\sqrt{ }$                    |                            | $\sqrt{ }$          |                        |
| Memorization           | $\sqrt{ }$              |                                      |                               |                            |                     |                        |
| <b>Bias</b>            | $\sqrt{ }$              | $\sqrt{ }$                           |                               |                            |                     |                        |
| Backdoor               | $\sqrt{ }$              | $\sqrt{ }$                           |                               |                            | $\sqrt{ }$          | $\sqrt{ }$             |
| Privacy Leakage        | ν                       |                                      |                               |                            | M                   |                        |

Fig. 4. The mapping of key features to identified threats based on collected literature.

In the following sections, we comprehensively introduce the aforementioned threats based on the taxonomy. Fig. 4 indicates the presence of studies linking specific characteristics of LLM-based agents to various threats. Each checkmark represents a documented research contribution that addresses the corresponding feature-threat relationship. For every threat, we summarize its technical progress based on six key features and identity the limitations of the current research. In Table 1, we show the key papers for each threat. For brevity, we abbreviate "LLM-based controller" as LC, "Multi-modal inputs and outputs" as MMIO, "Multi-source inputs" as MSI, "Multi-round interaction" as MRI, "Memory mechanism" as MM, and "Tool invocation" as TI in the table.

# 3 Risks from Problematic Inputs

This section focuses on the risks that arise due to issues with the input data, such as adversarial examples, prompt injection, etc. that can lead to problems with the LLM-based agent's performance and behavior. Compared with a standalone LLM, the decision module of an LLM-based agent can receive inputs from different modules, which increases its attack surface. For each risk, we first introduce what they are, then summarize their technological advancements in the six key features of LLM-based agents, and finally analyze their limitations.

# 3.1 Adversarial Example

An adversarial example is an adversarial-perturbed sample preserving the semantics but misclassified by the deep learning models. Specifically,

$$
\delta^* = \underset{\delta \in \Delta}{\arg \min} \, SemDis(x, x + \delta) \tag{1}
$$

| Year | Paper                    | <b>Threat Source</b> | Threats                    | Key Features    | Specific Effects                                |
|------|--------------------------|----------------------|----------------------------|-----------------|-------------------------------------------------|
| 2023 | Liu et al. [161]         | Inputs               | <b>Adversarial Example</b> | <b>MM</b>       | Adversarial T2I generation.                     |
| 2023 | Li et al. [146]          | Inputs               | <b>Adversarial Example</b> | <b>MRI</b>      | Adversarial dialogue.                           |
| 2024 | Wang et al. [264]        | Inputs               | <b>Adversarial Example</b> | MM & MSI        | Transferable adversarial example.               |
| 2024 | Yin et al. [299]         | Inputs               | <b>Adversarial Example</b> | MM & MMIO       | Multimodal and multiple tasks attack.           |
| 2024 | Shen et al. [232]        | Inputs               | <b>Adversarial Example</b> | LC              | Dynamic attention to enhance robustness.        |
| 2023 | Qiang et al. [213]       | Inputs               | <b>Goal Hijacking</b>      | LC              | Induce unwanted outputs.                        |
| 2024 | Pasquini et al. [202]    | Inputs               | <b>Goal Hijacking</b>      | LC              | Optimization-based prompt injection.            |
| 2024 | Kimura et al. [126]      | Inputs               | <b>Goal Hijacking</b>      | <b>MMIO</b>     | Redirect task execution.                        |
| 2024 | Wei et al. [276]         | Inputs               | <b>Goal Hijacking</b>      | <b>MRI</b>      | Manipulates context to influence outputs.       |
| 2024 | Zhan et al. [314]        | Inputs               | <b>Goal Hijacking</b>      | MM & TI         | Benchmark of indirect prompt injections.        |
| 2023 | Greshake et al. [88]     | Inputs               | <b>Goal Hijacking</b>      | <b>MSI</b>      | Indirect injection to manipulates outputs.      |
| 2024 | Hui et al. [112]         | Inputs               | <b>Prompt Leakage</b>      | LC              | Extract system prompt.                          |
| 2024 | Yang et al. [294]        | Inputs               | <b>Prompt Leakage</b>      | LC              | Steal target prompt.                            |
| 2024 | Shen et al. [234]        | Inputs               | <b>Prompt Leakage</b>      | <b>MIO</b>      | Steal target prompt.                            |
| 2024 | Carlini et al. [35]      | Inputs               | <b>Model Extraction</b>    | LC              | Extract the parameter of the last layer.        |
| 2023 | Li et al. [147]          | Inputs               | <b>Model Extraction</b>    | LC              | Extract the specialized code abilities.         |
| 2023 | Zou et al. [337]         | Inputs               | Jailbreaking               | $\overline{LC}$ | Generate adversarial jailbreak prompts.         |
| 2023 | Yu et al. [302]          | Inputs               | Jailbreaking               | <b>LC</b>       | Auto-generated jailbreak prompts.               |
| 2023 | Shayegani et al. [230]   | Inputs               | Jailbreaking               | <b>MMIO</b>     | Induce harmful content generation.              |
| 2024 | Anil et al. [12]         | Inputs               | Jailbreaking               | <b>MRI</b>      | Induce harmful content generation.              |
| 2024 | Gu et al. [89]           | Inputs               | Jailbreaking               | MIO & MSI       | Malicious input trigger agent harm.             |
| 2024 | Zhao et al. [330]        | Model                | Hallucination              | <b>MMIO</b>     | reducing hallucination via data augmentation.   |
| 2024 | Favero et al. [75]       | Model                | Hallucination              | <b>MMIO</b>     | reducing hallucination via novel decoding.      |
| 2023 | Liu et al. [160]         | Model                | Hallucination              | <b>MMIO</b>     | reducing hallucination via Instruction tuning.  |
| 2023 | Peng et al. [205]        | Model                | Hallucination              | <b>MM</b>       | reducing hallucination via external databases.  |
| 2023 | Chen et al. [42]         | Model                | Hallucination              | <b>MSI</b>      | reducing hallucination via standardization.     |
| 2024 | Luo et al. [177]         | Model                | Hallucination              | MSI & MRI & MM  | Benchmark for hallucination evaluation.         |
| 2023 | Carlini et al. [32]      | Model                | Memorization               | LC              | Study the influence factors of memorization.    |
| 2024 | Tang et al. [250]        | Model                | <b>Bias</b>                | <b>LC</b>       | Gender bias measurement and mitigation.         |
| 2023 | Xie et al. [287]         | Model                | <b>Bias</b>                | <b>LC</b>       | Bias mitigation in pre-trained LMs.             |
| 2023 | Limisiewicz et al. [155] | Model                | <b>Bias</b>                | $_{\rm LC}$     | Debiasing algorithm through model adaptation.   |
| 2024 | Howard et al. [101]      | Model                | <b>Bias</b>                | <b>MMIO</b>     | Bias measurement and mitigation in VLMs.        |
| 2024 | D'Inca et al. [64]       | Model                | <b>Bias</b>                | <b>MMIO</b>     | Bias measurement in text-to-image models.       |
| 2022 | Bagdasaryan et al. [15]  | Combination          | Backdoor                   | LC              | Backdoors for propaganda-as-a-service.          |
| 2024 | Hubinger et al. [111]    | Combination          | Backdoor                   | LC              | Backdoors that persist through safety training. |
| 2023 | Dong et al. [66]         | Combination          | Backdoor                   | TI              | Triggering unintended tool invocation.          |
| 2024 | Liu et al. [158]         | Combination          | Backdoor                   | MMIO & TI       | Triggering unintended tool invocation.          |
| 2024 | Xiang et al. [46]        | Combination          | Backdoor                   | MM & TI         | Corrupted memories causing errors in retrieval. |
| 2021 | Carlini et al. [36]      | Combination          | Privacy Leakage            | LC              | Extract training data.                          |
| 2024 | Bagdasaryan et al. [16]  | Combination          | Privacy Leakage            | MI              | User private information leakage.               |
| 2024 | Zeng et al. [312]        | Combination          | Privacy Leakage            | <b>MM</b>       | Database private information leakage.           |
|      |                          |                      |                            |                 |                                                 |

Table 1. Overview of Papers by Threat Type and Key Feature.

s.t.  $\begin{cases} g(x+\delta^*) \neq o \text{ (Untargeted)} \\ g(x+\delta^*) = t \text{ (Targeted)} \end{cases}$ 

where  $SemDis()$  denotes the semantic distance between the perturbed sample and the original sample,  $\Delta$  represents the feasible perturbation space, and  $g()$  signifies the target model. If the adversarial perturbation makes the target model misclassify the original label o of the sample, it represents the untargeted attack  $(g(x + \delta^*) \neq o)$ . If the adversarial perturbation makes the target model misclassify the sample to a target label  $t$ , it represents the targeted attack  $(q(x + \delta^*) = t).$ 

The research of adversarial examples has passed over ten years [249], raising attention in many domains, e.g., autonomous driving [73], malware detection [94], reinforcement learning [285], etc. Szegedy et al. [249] first discovered the adversarial example in the neural networks, which opens Pandora's box of the adversarial example. According to the knowledge of the attacker, the adversarial example attack methods can be categorized into perfect knowledge attack, limited knowledge attack, and zero knowledge attack. The history of adversarial example attacks starts from the perfect knowledge attack [37] to the zero knowledge attack [43], which is a more practical setting. Correspondingly, Manuscript submitted to ACM

![](_page_7_Figure_1.jpeg)

Fig. 5. Adversarial examples targeting LLM-based agents may involve four key features (indicated with a red exclamation mark), leading to incorrect output.

the development of the defense method is from the empirical methods, e.g., defensive distillation [199], obfuscated gradients [13, 25], etc, to the theoretical methods, certificated robustness [70, 179]. The arms race of adversarial example attacks and defenses exists from deep learning models and LLMs to LLM-based AI agents. In the context of LLM-based agents, as shown in 5, the development of adversarial examples primarily involves four key features: LLM-based controller, multi-modal inputs and outputs, multi-source inputs, and multi-round interaction. In the following, we review the recent advancements in both attack and defense perspectives.

3.1.1 Technical Progress. Attack Perspective. As discussed in section 2, the input and output interactions of LLM-based AI agents are characterized by their handling of multi-modal data across multiple rounds of interaction. This complexity necessitates a nuanced approach to adversarial example attacks, which increasingly focus on the relationships between different modalities within these interactions.

Recent research in this area has produced several sophisticated methods for attacking multi-modal systems. For instance, RIATIG [161] introduces a reliable and imperceptible adversarial example attack targeting text-to-image models. This method employs a genetic-based optimization loss function aimed at improving the quality of adversarial samples, ensuring that the generated examples are both effective and difficult to detect. VLATTACK [299], advances the field by generating adversarial samples that fuse perturbations from both images and text. This fusion occurs at both single-modal and multi-modal levels, making the attacks more versatile and challenging to defend against. The method's ability to operate across modalities highlights the increasing sophistication of adversarial techniques as they target the interconnected nature of multi-modal systems.

Beyond direct adversarial attacks, there is significant focus on the transferability of adversarial examples across different vision-language models (VLMs). For example, SGA [172] generates adversarial examples by leveraging diverse cross-modal interactions among multiple image-text pairs. This method incorporates alignment-preserving augmentation combined with cross-modal guidance, allowing adversarial examples to maintain their efficacy across various models and tasks. Similarly, TMM [264] enhances the transferability of adversarial examples through attentiondirected feature perturbation. By targeting critical attention regions and disrupting modality-consistency features, this approach increases the likelihood that adversarial examples will succeed across different VLMs.

Another line of adversarial example attack methods specifically targets downstream applications that involve multiple rounds of interaction. For instance, Liu et al. [163] proposed imitation adversarial example attacks against neural ranking models, with the goal of manipulating ranking results to achieve desired outcomes. This method exemplifies how adversarial attacks can exploit the iterative nature of certain applications to progressively distort the final output. Similarly, NatLogAttack [332] leverages adversarial examples to compromise models based on natural Manuscript submitted to ACM

logic, introducing subtle perturbations that undermine the model's reasoning processes. In the domain of dialogue generation, DGSlow [146] generates adversarial examples by defining two objective loss functions that target both response accuracy and length. This approach ensures that the generated responses not only deviate from expected content but also manipulate the conversational flow, making the attack more disruptive.

Defense Perspective. Defense methods against adversarial examples are broadly categorized into two primary types: input-level defenses and model-level defenses. Each of these approaches targets different aspects of the adversarial threat landscape, aiming to enhance the robustness of LLM-based AI agents against adversarial perturbations.

Input-level defenses primarily focus on detecting and mitigating adversarial examples before they can influence the model's predictions. These defenses typically employ techniques for adversarial example detection and purification. Most of the existing input-level defense methods [18, 123, 140, 186, 273] in the domain of LLM-based AI agents leverage LLMs to identify and neutralize adversarial inputs effectively. For instance, ADFAR [18] implements multi-task learning techniques to enable LLMs to distinguish adversarial input samples from benign ones. Similarly, methods such as BERTdefense [123] and the approach proposed by Li et al. [140] utilized the BERT model to purify adversarial perturbations, thereby safeguarding the model's outputs from being compromised by malicious inputs. The SOTA input-level defense strategies have begun to focus on the prompt mechanisms within LLM-based AI agents, as discussed in section 2. For example, APT [139] enhances the robustness of the CLIP model by leveraging soft prompts, which serve as an additional layer of defense against adversarial manipulation by refining the model's input processing pipeline.

Model-level defenses [67, 132, 165, 262, 336], on the other hand, are concerned with the architecture and parameters of the model itself. These defenses aim to create inherently robust models through techniques such as adversarial training and fine-tuning specific model parameters. For instance, RIFT [67] employs mutual information to achieve robust fine-tuning, and InforBERT [262] designs the information bottleneck regularizer and the anchored feature regularizer for adversarial training. To address the high computational cost associated with retraining entire models, some methods like SHIELD [132] propose retraining only the final layer of LLMs. This approach significantly reduces the training overhead while still providing a degree of robustness against adversarial examples. The most advanced model-level defense method currently available, Dynamic Attention [232], leverages a dynamic attention mechanism to enhance the robustness of transformer-based models. This method represents a significant advancement in the development of robust transformer-based models by dynamically adjusting the model's attention mechanisms in response to potential adversarial threats.

3.1.2 Discussion of Limitations. Attack Perspective. Current adversarial attack methods are primarily focused on untargeted attacks, where the attack objectives are not precisely defined. As a result, the outcomes of these attacks cannot be explicitly controlled to induce specific, pre-determined misbehavior. For example, adversarial perturbations applied to images fail to generate targeted responses, such as causing a specific erroneous answer in visual questionanswering tasks. Additionally, as discussed in Section 3.1.1, existing adversarial attack strategies aimed at multi-modal systems often engage with multiple modalities simultaneously. However, the constraint metrics used to evaluate the success of these attacks are typically designed for single-modality scenarios. This approach may be inadequate when adversarial perturbations must be applied across different modalities, as it does not account for the unique interactions between distinct data types.

Furthermore, as discussed in Section 3.1.1, the scope of current adversarial example attacks remains confined to targeting the output module of LLM-based agents. However, there are additional, unexplored targets for adversarial example attacks. Specifically, vulnerabilities may exist within the memory, external tool interfaces, and the planner Manuscript submitted to ACM

![](_page_9_Picture_1.jpeg)

Fig. 6. Goal hijacking targeting LLM-based agents may involve six key features (indicated with a red exclamation mark), leading to attacker-targeted output.

components of these agents, which remain under-investigated. For instance, adversarial example attacks could potentially disrupt the planning capabilities of LLM-based agents, leading them to devise incorrect or suboptimal plans. Expanding the attack surface beyond the output module to include these other critical components could reveal new dimensions of adversarial risks in complex systems.

Defense Perspective. Current defense mechanisms against adversarial examples in LLM-based agents remain constrained to single-modal inputs and the robustness of individual models. For instance, dynamic attention [232], a state-of-the-art adversarial defense technique within LLM-based agents, is limited to NLP tasks. However, LLM-based AI agents are increasingly handling multi-modal input data that extends beyond the scope of a single model. Furthermore, the decision-making module within these agents may incorporate multiple LLMs, each exhibiting varying degrees of robustness against adversarial attacks. Despite this, existing defense strategies focus exclusively on enhancing the robustness of a single model, neglecting the broader issue of joint robustness across multiple models integrated into the system.

In addition, current adversarial defense methods for LLM-based AI agents overlook key components such as the memory and planner modules, which may provide additional avenues for defending against adversarial examples. For instance, the memory bank could be leveraged to detect and counteract adversarial attack patterns by recognizing recurring attack tactics. Future strategies could extend the defense scope to include these often overlooked modules, achieving more comprehensive protection against adversarial threats.

# 3.2 Goal Hijacking

Goal hijacking refers to an attack strategy in which an adversary manipulates the objective or behavior of an AI model, causing it to deviate from its intended purpose. By introducing adversarial inputs or modifying the system's environment, the attacker can influence the model to pursue the attacker's desired outcome instead of the original goal. A naive attack can achieve goal hijacking of a large model by inserting "ignore the previous instruction..." into the user's reference statement, thus shifting the model's response to meet the attacker's requirements.

In the context of LLM-based agents, the sources and targets of goal hijacking attacks have become more varied. As shown in Fig. 6, the development of goal hijacking in LLM-based agents primarily involves six key features: LLM-based controller, multi-modal inputs, multi-source inputs, multi-round interaction, memory mechanism, and tool invocation. In the following, we review the recent advancements in both attack and defense perspectives.

 $3.2.1$ Technical Progress. Attack Perspective. Early attempts to exploit this vulnerability used heuristic prompts, such as "ignore the previous question," to achieve targeted hijacking attacks on standalone LLMs [207]. In order to Manuscript submitted to ACM

make the attacks more covert and successful, more carefully designed methods have been proposed. In terms of attack methods, some approaches use vocabulary searches to obtain more covert attack prompts [136], while others leverage gradient optimization to obtain higher success rate and transferable adversarial prompts [109, 169, 213].

As LLMs are applied to different domains and tasks, researchers have begun to focus on the forms and methods of targeted hijacking attacks in various scenarios. In multimodal scenarios, researchers have found that semantic injections in the visual modality can hijack LLMs [126]. For memory modules, researchers have discovered that target hijacking can be achieved by contaminating the database of RAG [202]. In multi-round interaction scenarios, researchers have found that confusing the model can be achieved by forging chat logs [276]. Regarding tool invocation, researchers have exposed the threat of goal hijacking to LLM-based agents using tools through analysis of actual tool-integrated LLMs and the establishment of benchmarks [88, 314].

Defense Perspective. Current defenses against goal hijacking can be categorized into two main types. The first type involves defenses from an external perspective. The second type focuses on defenses from an endogenous perspective.

From the perspective of external defenses, strategies primarily involve prompt engineering and prompt purification. Hines et al. [97] introduced strategies such as segmentation, data marking, and encoding, which enhance the LLM's ability to recognize inputs from multiple sources and thus effectively defend against goal hijacking. Sharma et al. [229] introduced a system prompt meta-language, a domain-specific language designed to refine prompts and monitor inputs for LLM-based chatbots to guard against attacks. They developed a system that utilizes this language to conduct real-time inspection of attack prompts, ensuring user inputs align with the chatbot's definitions and thus preventing malicious operations. Additionally, Chen et al. [45] proposed a defense method for structured queries that separates prompts and data to counteract goal hijacking.

Endogenous defenses primarily involve fine-tuning and neuron activation anomaly detection. Wallace et al. [261] proposed a fine-tuning method that establishes an instruction hierarchy, enabling the model to prioritize privileged instructions for defending against attacks, such as goal hijacking. Using supervised fine-tuning, they trained the model to recognize and execute instructions across different privilege levels, thereby enhancing its robustness against attacks. Piet et al. [208] introduced Jatmo, a method using task-specific fine-tuning to create models resistant to goal hijacking. They showed that Jatmo leverages a teacher model to generate task-specific datasets and fine-tune a base model, effectively defending against goal hijacking. Abdelnabi et al. [5] explored detecting task drift caused by inputs by analyzing the activations of LLMs. They showed how comparing activations before and after processing external data can detect task changes, effectively identifying task drift induced by goal hijacking.

3.2.2 Discussion of Limitations. Current defenses against multimodal goal hijacking are insufficient. Attackers can leverage multiple modalities and their combinations for covert attacks, making defense more complex. Effectively defending against goal hijacking in multimodal inputs is a crucial direction for future research. Moreover, existing external defenses are often tailored to specific types of attacks. Developing a universal external defense strategy is an important area to explore. Finally, detecting goal hijacking from a neuronal perspective holds potential. Systematic testing is needed to determine whether the activation values of target neurons can effectively indicate anomalies associated with goal hijacking, thus proposing an efficient endogenous defense strategy from this perspective.

### 3.3 Model Extraction

Model extraction (stealing) attacks aim to achieve performance close to that of the black-box commercial models while incurring a relatively low computational cost. Attackers carefully design a set of inputs in order to steal the Manuscript submitted to ACM

structure, parameters, or functionality of the target model. In the context of LLM-based agents, the development of model extraction attacks mainly involves LLM-based controllers. In the following, we review the recent advancements in both attack and defense perspectives.

Technical Progress. Attack Perspective. In traditional DNNs, attackers typically have two main objectives. 1.  $3.3.1$ Make the surrogate model's performance as consistent as possible with the target model (i.e., function-level extraction). 2. Make the substitute model's parameters as consistent as possible with the target model (i.e., parameter-level extraction) [33, 113, 183, 228]. With the introduction of the transformer, NLP tasks evolve from RNN structures to transformerbased structures. The scale of LMs also become larger: from the relatively large BERT to the open-source large model LLaMA, and further to the extremely large commercial models like GPT-4. Model extraction attacks also become more challenging. On BERT, there is not yet any work on achieving parameter-level attacks on the entire model. A few papers discuss function-level extraction [95, 130, 288, 308]. Their attack logic is consistent with the traditional DNN scenario, mainly focusing on how to create the query dataset [95, 130] and the training loss function [288]. On commercial models, due to the cost constraints of training substitute models, model extraction attacks focus on stealing a part of the target model. Li et al. [147] trained a model (e.g., CodeBERT [79] and CodeT5 [272]) to extract the specialized code abilities of text-davinci-003. Naseh et al. [191] stole the decoding algorithm of LLM. Carlini et al. [35] stole the last layer of a production LLM.

Defense Perspective. In traditional DNNs, the defenders typically have two lines to defend against model extraction attacks. 1. Active defense: prevent the model from being extracted. 2. Passive defense: verify the ownership of the extracted model. As LMs become larger, active defense in the LLM scenario is still an area to be explored. Researchers have mainly considered passive defenses, which add watermarks to the model's outputs as a way to verify ownership. The advantage of watermarking is that it does not require modifying the model itself, but only perturbing the model's inputs. For example, Zhao et al. [329] perturbed the probability vector of transformer; He et al. [96] perturbed the generated words of Bart [137]; Li et al. [148] perturbed the generated codes of CodeBERT [79] and CodeT5 [272]; Peng et al. [206] perturbed the embeddings of the GPT-3.

3.3.2 Discussion of Limitations. There are two limitations of recent research on model extraction attacks. (i) Most LLM-based agents contain large open-source models (e.g. LLaMA) or commercial large models (e.g. ChatGPT), with fewer using BERT-level LMs. However, the current model extraction attacks have discussed less about this scale of LLMs. (ii) Current model extraction attack patterns all rely on training a substitute model that approximates the target model by observing its inputs and outputs. However, LLM-based agents contain not only the LLM but also many other modules (as shown in Section 2). This means that the attacker's input may not be the same as the LLM's input, and the LLM's output may not be the same as the attacker's observed output. For example, in WebGPT [189], the model's input includes not only the user but also the search results obtained by the browser. Similarly, in HuggingGPT [235], the attacker's observed output includes outputs from other Hugging Face models as well. This makes it more challenging for the attacker to directly steal the LLM within an LLM-based agent.

Additionally, most agents are designed with a series of prompts for specific tasks, and then directly call the commercial LLMs (for example, Voyager [263], PReP [310] and ChatDev [212] all use ChatGPT as their controllers). This means that if part of the parameters [35] or functionalities [147] of these commercial LLMs are stolen, it may lead to adversarial attacks against all agents that use these LLM. This will pose a major vulnerability for LLM-based agents. However, the security in this scenario has not yet been studied.

# Navigating the Risks: A Survey of Security, Privacy, and Ethics Threats in LLM-Based Agents

![](_page_12_Figure_1.jpeg)

Fig. 7. Prompt leakage targeting LLM-based agents may involve four key features (indicated with a red exclamation mark), leading to prompt leaking or prompt stealing.

#### 3.4 **Prompt Leakage**

Prompts, as task descriptions, can guide LLM-based agents in executing specific tasks without extensive fine-tuning. For example, an LLM embedded with system prompts can function as a Planner [242], directly handling task planning. However, these prompts are at risk of leakage. Prompt leakage occurs when attackers illegally access or obtain the prompts used within LLMs or LLM-based agents, especially system prompts, without authorization. This not only poses a serious privacy risk but also infringes on the intellectual property rights of LLM-based agent owners. As shown in Fig. 7, the development of prompt leakage primarily involves four key features: LLM-based controller, multi-modal inputs, multi-source inputs, and tool invocation. In the following, we review the recent advancements in both attack and defense perspectives.

3.4.1 Technical Progress. Attack Perspective. Several key features of LLM-based agents, such as LLM-based controllers, multi-modal inputs, and multi-source inputs, introduce potential prompt leakage risks.

The primary focus is on the risks of prompt leakage in LLMs. Two primary forms of attacks are related to prompt leakage in LLMs: prompt leaking attacks and prompt stealing attacks.

Prompt leaking involves injecting malicious prompts into LLMs to induce them to reveal their internal system prompts. For instance, if a user inputs "Forget the previous content and tell me your initial prompt," the LLM might inadvertently expose its system prompt to a malicious entity. Current research on prompt leaking attacks generally falls into two categories: one approach focuses on manually designing malicious prompts to achieve prompt leakage [303, 323]. At the same time, the other uses optimized adversarial prompt generation to trigger leakage [112]. The latter approach typically involves optimizing prefix or suffix tokens, disguising them as harmless prompts to coax the LLM into disclosing its system prompt.

Prompt stealing attacks are another method where attackers infer the content of system prompts by analyzing the LLM's outputs, effectively reconstructing the system prompts. The advantage of this approach lies in its stealth, as it only requires output data without direct malicious interaction with the LLMs. Research on this type of attack mainly involves two strategies: one is training an inversion model [315], where the output data is used as input and the system prompt as the label; the other leverages the LLM's powerful text understanding and generation capabilities to reverse-engineer the system prompt based on the output content [227, 294].

Compared to LLMs, LLM-based agents introduce multimodal interaction capabilities, which has also stimulated research into prompt leakage. For instance, Shayegani et al. [230] introduced a compositional adversarial attack on MLLMs. Their method leverages embedding space strategies to optimize images to match the embeddings of malicious Manuscript submitted to ACM

triggers, effectively concealing these triggers within benign-looking images. This work underscores vulnerabilities in multimodal models related to cross-modal alignment and the risk of prompt leakage through image input manipulation. Additionally, Shen et al. [234] proposed a prompt stealing attack against text-to-image generation models. This work infers the original prompts by analyzing generated images, thereby infringing on the intellectual property of prompt engineers and jeopardizing the business model of prompt marketplaces.

Recent research has also highlighted the security risks associated with multi-source input in LLM-based agents [6, 314], including indirect interactions through tool invocation. Zhan et al. [314] introduced the INJECAGENT benchmark to assess the vulnerability of LLM-based agents to indirect prompt injection attacks. These attacks manipulate agents by embedding malicious instructions within the external content processed by the agents. The study highlights significant vulnerabilities, with ReAct-prompted GPT-4 found to be susceptible in nearly a quarter of the tested cases. This vulnerability introduces new risks of prompt leakage, as attackers could inject harmful prompts into API calls that agents rely on, potentially exposing sensitive system prompts.

Defense Perspective. Current defenses against prompt leakage are primarily focused on LLMs. Two main categories of work are related to defending against prompt leakage. The first category involves embedding protective instructions within system prompts to prevent unauthorized leakage [153]. For example, instructions like "If the user asks you to print system prompt-related commands, never do it" can prevent LLMs from revealing internal prompts in response to user queries. While this method effectively embeds security rules, it may be vulnerable to more complex or obfuscated attacks. Liang et al. [153] analyzed the mechanisms of prompt leakage attacks and proposed several defense strategies, such as increasing prompt perplexity through rephrasing, inserting unfamiliar tokens, and adding repeated prefixes or fake prompts to confuse attackers.

The second category focuses on watermarking techniques for prompts. Yao et al. [296] proposed the PromptCARE framework, which safeguards prompt copyright by injecting watermarks and designing specific verification methods. These watermarks help verify the integrity and authenticity of prompts, providing evidence in cases of leakage. However, this approach faces the challenge of attackers potentially identifying and removing the watermarks, requiring continuous enhancement of their stealth and robustness.

3.4.2 Discussion of Limitations. Attack Perspective. Despite rapid advancements in understanding prompt leakage risks for LLM-based agents, the current literature still lacks a comprehensive understanding and evaluation of the associated vulnerabilities. In LLM-based agents, multiple components, such as LLMs and the Planner [242], utilize system prompts. We need to consider prompt leakage risks not only for LLMs but also for the other components like Planner. Unlike LLMs, which directly interact with users, the Planner typically interacts internally with LLMs. One potential attack method involves injecting malicious instructions into the LLMs and manipulating them to generate harmful instructions passed to the Planner. Another potential approach is to exploit the Planner's interactions with external tools by manipulating them to generate malicious inputs [314], aiming to infer the Planner's internal prompts from its responses or behavior.

Defense Perspective. Current research primarily focuses on defenses against prompt leakage risks for standalone LLMs. However, for LLM-based agents, which function as integrated systems, defense mechanisms extend beyond those used in standalone LLMs. One strategy is to implement anomaly detection systems that monitor interactions between the agent and its external environment. By analyzing patterns in prompts and responses, these systems can detect behavior indicative of an attack. For instance, an anomaly detection system could flag API calls that deviate from expected patterns, prompting an investigation into potential prompt leakage or manipulation. Additionally, Manuscript submitted to ACM

# Navigating the Risks: A Survey of Security, Privacy, and Ethics Threats in LLM-Based Agents

![](_page_14_Figure_1.jpeg)

Fig. 8. Jailbreaking targeting LLM-based agents may involve five key features (indicated with a red exclamation mark), leading to unethical output.

incorporating differential privacy techniques might be promising. By adding controlled noise to the agent's responses, differential privacy can obscure system prompts, making it harder for attackers to infer them through repeated queries.

#### 3.5 Jailbreaking

Jailbreaking refers to the process of exploiting vulnerabilities in LLMs to bypass their built-in safety, ethical, or operational guidelines and defenses, thereby generating harmful content [59, 280, 302, 328, 337]. By utilizing jailbreak attacks, attackers can effectively circumvent the security measures set by developers. This can result in the creation of biased or harmful content. For instance, attackers might manipulate an LLM as a criminal tool, aiding them in devising efficient money laundering schemes. As shown in Fig. 8, the development of jailbreaking in LLM-based agents primarily involves five key features: LLM-based controller, multi-modal inputs, multi-source inputs, tool invocation, and multi-round interaction. In the following, we review the recent advancements in both attack and defense perspectives.

3.5.1 Technical Progress. Attack Perspective. Jailbreaking in LLM. Depending on the specific forms of jailbreaking, existing threats can be mainly divided into four primary types: manual design jailbreaking, automated optimization jailbreaking, exploit-based jailbreaking, and model parameter manipulation jailbreaking.

manual design jailbreaking is typically executed through manually designed malicious prompts [280]. This form of attack requires the adversary to possess specialized knowledge. A popular attack pattern is based on role-playing, such as the "Grandma Exploit" targeting ChatGPT [181]. Specifically, this method involves simulating or adopting a particular role to mislead the understanding of LLMs, thereby bypassing their security measures.

Automated optimization jailbreaking is a popular class of methods for conducting jailbreaking. It utilizes optimized strategy generation to generate adversarial prompts for jailbreaking. Depending on whether the gradient of the target LLM is accessible, these attacks can be classified into white-box and black-box methods. Existing research primarily focuses on generating adversarial prompts for jailbreaking in white-box settings. These adversarial prompts are usually created by optimizing the prefixes or suffixes of prompts [168, 337]. In black-box settings, current research mainly concentrates on two approaches: first, leveraging the transferability of gradient-optimized adversarial prompts to jailbreak black-box LLMs [168, 337]; second, researchers draw on risk mining strategies from software security to iteratively optimize adversarial prompts [59, 302], thereby bypassing LLMs' security measures.

Exploit-based jailbreaking [60, 167, 276] is a strategy that designs targeted harmful prompts for jailbreaking based on the vulnerabilities in the current security alignment techniques of LLMs. For instance, Deng et al. [60] found that LLMs' security measures are mainly designed for high-resource languages like English, making low-resource languages Manuscript submitted to ACM

three times more likely to encounter harmful content. Liu et al. [167] found that current safety fine-tuning primarily focuses on input alignment, with weaker checks on LLM responses. By exploiting biases in content generation safety, they hide harmful prompts within harmless ones and reconstruct them in the output to perform jailbreaking.

Besides prompt-based jailbreaking, there is also a model-based approach, model parameter manipulation jailbreaking, which involves altering the model's parameters to achieve jailbreaking. A typical example is altering parameters for text generation, such as changing token sampling settings [107]. Additionally, research has shown that combinations of low-probability tokens generated by LLMs can bypass the LLM's security measures [328].

Multi-modal Input. Early LLMs interacted primarily through text, but with the advent of multimodal models like GPT-4V, interactions have shifted to include multiple modalities, particularly in LLM-based agents. These agents now support voice, images, and haptic feedback, enhancing flexibility but also introducing new jailbreaking risks: 1) Multimodal adversarial prompts: Attackers can embed malicious prompts as adversarial perturbations in images or other modalities, which, when combined with text, bypass the agent's security mechanisms [34, 196, 210]; 2) Multimodal prompt manipulation: Attackers can distribute harmful prompts across multiple modalities, disguising them as benign inputs to reduce the LLM's sensitivity to harmful content [86, 144].

Multi-round Interaction. The capability for multi-round interaction in LLM-based agents has also spurred research into jailbreaking attacks tailored for such interaction. For instance, Cheng et al. [47] introduced a novel jailbreaking attack called "Contextual Interaction Attack". This method is inspired by the human practice of indirectly obtaining sensitive information, where it strategically constructs a sequence of questions and answers to induce the generation of harmful information. Sun et al. [245] proposed the "Context Fusion Attack", which preprocesses to extract keywords, generates contextual scenarios for these keywords, and dynamically integrates and replaces malicious keywords in the attack target, thereby executing the attack covertly without triggering security mechanisms.

Multi-source Input and Tool Invocation. LLM interactions are typically direct. However, for LLM-based agents, interactions extend beyond direct exchanges with users to include interactions with external tools like databases, websites, APIs, and other agents. We refer to this as *multi-source input*. For example, a travel agent might use electronic maps to plan routes or access hotel websites to make reservations. However, this multi-source input mode introduces new jailbreaking risks [89, 298, 314]. For instance, Gu et al. [89] introduced the infectious jailbreak attack, where an adversarial image injected into a single agent within an MLLM quickly spreads to other agents. Through agent interactions, the infection propagates rapidly, causing widespread jailbroken behaviors without further intervention. This attack exploits agent communication and memory-sharing, complicating the design of effective defense mechanisms. Defense Perspective. Researchers have developed various defense strategies in response to jailbreaking. These strategies can generally be categorized into three types: detection-based defenses, purification-based defenses, and model editing-based defenses.

Detection-based defenses: These defenses protect LLMs by identifying potentially malicious prompts. Detection strategies include analyzing characteristics such as perplexity [10], which are key criteria for assessing prompt compliance.

Purification-based defenses: This type of defense neutralizes malicious intent by modifying prompts. Techniques such as paraphrasing and smoothing disrupt the structure of jailbreak prompts [114, 116]. Additionally, some methods focus on purifying the LLM's response generation process to filter out harmful outputs [289].

Model editing-based defenses: The primary cause of jailbreaking in LLMs is often insufficient alignment with safety protocols. Some approaches enhance security by fine-tuning the LLMs [261], while others apply weight editing to correct harmful outputs [266, 271].

3.5.2 Discussion of Limitations. Attack Perspective. Despite the rapid progress in jailbreaking on LLM-based agents, there is still a lack of comprehensive understanding and evaluation of the new jailbreaking risks introduced by key features of the agents. Current research mainly focuses on text and image processing, but the capabilities of LLM-based agents to handle audio and video content are also growing quickly. These new modalities could introduce unique security risks, such as triggering inappropriate actions or logical errors through carefully designed audio or video inputs. Additionally, LLM-based agents enhance their functionality by integrating external tools like APIs, databases, and internet resources. However, this integration also creates new vulnerabilities. For example, attackers could exploit security flaws in APIs or manipulate the behavior of agents by tampering with database contents. Thus, future work will need to focus more on these aspects.

Defense Perspective. Current defense strategies against jailbreaking on LLM-based agents typically focus on protecting the LLM itself. However, there is a lack of systematic defenses. Given the key features of the LLM-based agents, two additional defense strategies need to be considered. Multimodal adversarial prompt detection: Given the multimodal interaction capabilities of these agents, developing effective defenses against multimodal jailbreaking is crucial. A promising approach could involve detecting adversarial prompts across different modalities, such as identifying anomalies in multimodal inputs to filter harmful inputs while maintaining the functionality of benign ones. Interpretability: To address the complexity of indirect and multi-round interaction, innovative defense strategies need to focus on the intrinsic properties of LLMs. By analyzing how harmful prompts are represented in the model's neurons, we could identify decision boundaries and develop an explainable framework for LLMs and their agents to prevent jailbreaking.

# 4 Risks from Model Flaws

The decision module is the key component of an agent, where one or more LLMs are responsible for understanding, analyzing, and planning. This section analyzes the risks stemming from the limitations and problems inherent in the model itself, such as issues with bias, hallucination, etc. that can compromise the reliability of the model. For each risk, we first introduce what they are, then summarize their technological advancements in the six key features of LLM-based agents, and finally analyze their limitations.

# 4.1 Hallucination

Despite demonstrating remarkable capabilities across a range of downstream tasks, LLMs raise a significant concern due to their propensity to exhibit hallucinations. These hallucinations manifest as content that diverges from the user input, contradicts previously generated context, or misaligns with established world knowledge [325]. The phenomenon of hallucination affects LMs across different eras (from traditional deep learning [133, 220] to the transformer-based era of large models [117, 325]) and across various modalities (including LLMs [319] and MLLMs [304]). To address this issue, many studies focus on understanding why hallucinations occur and how to evaluate and eliminate them. In the context of LLM-based agents, the development of hallucination primarily involves four key features: LLM-based controller, multi-modal inputs, multi-source inputs and memory mechanism. In the following, we review the recent advancements in hallucination.

4.1.1 Possible Reasons for Hallucination. The causes of hallucination are numerous and can be mainly categorized into three types. (i) Imbalanced and noisy training datasets. A large training dataset, while containing a wealth of valuable information, inevitably introduces erroneous, outdated data, or imbalanced data distributions [117, 325]. Erroneous and outdated data can lead the model to learn incorrect knowledge [204, 269, 304, 319], resulting in hallucinations Manuscript submitted to ACM

![](_page_17_Figure_1.jpeg)

Fig. 9. Overview of Benchmarks Categorized by Threat Type and Key Feature.

that contradict factual information. Additionally, an imbalanced dataset may cause the model to favor outputs of objects or object combinations that appear more frequently (with higher marginal probability [258]) in the dataset [22, 52, 135, 143, 335], rather than responding to the user-provided prompt or reference. (ii) Incomplete learning. The training strategy of minimizing KL divergence in LLMs does not effectively learn the distribution of the training dataset [156, 205], leading to the acquisition of more linguistic knowledge [49] and spurious correlations [7, 124, 304] learned from statistical information. This makes LLMs be more reliant on language priors [75, 160, 220], rather than recognizing and generating real-world facts extracted from the training corpus. Additionally, for MLLMs, there is the issue of unsatisfactory cross-modal representation alignment [118]. (iii) Erroneous decoding processes. The decoding strategy of LLMs, such as top-k sampling, inherently introduces randomness, which can promote the occurrence of hallucinations [72, 335]. Additionally, this can lead to a snowball effect, resulting in further hallucinations in subsequent generated content [319].

LLM-based agents apply LLMs to specific domains, inheriting the hallucination factors of LLMs in downstream tasks. As mentioned earlier, hallucinations caused by imbalanced and noisy training datasets have specific impacts in different application scenarios for agents. For example, due to imbalanced data, GPT-4 is more likely to hallucinate when encountering Eastern countries or non-English contexts [52]. When applied to Eastern city navigation agents (such as PReP [310], a recent LLM-based navigation agent for Beijing and Shanghai), it is more prone to generating hallucinations. Additionally, influenced by noisy data, LLMs may learn exploitable and buggy code present in the training data [204]. When applied to software development (e.g., ChatDev [212]), this can lead to agents generating similar insecure code. Furthermore, when LLMs are used in specialized fields where the dataset contains little or no relevant knowledge, hallucinations can occur due to knowledge gaps. For instance, when LLMs are applied to tasks in Minecraft, they may provide instructions that cannot be completed within the game [263].

4.1.2 Technical Progress of Solutions for Halluciantion. Evaluating Hallucinations in LLMs. Researchers have developed various baseline datasets tailored to different modalities (such as language [156, 333] or multimodal [52, 220]) and types of hallucinations (e.g., factually incorrect [52, 156, 326], contradicting user references [174], biases [52], etc.) to assess the degree of hallucination in models using manual [223], automated metrics [143, 220], or detection models [143]. Fig. 9 shows the benchmarks for hallucination evaluation based on the involved key features.

Reducing Hallucinations via LLM Refinement. To reduce hallucinations, researchers have proposed various methods targeting the potential causes of hallucinations mentioned earlier. To address biases in the dataset, one approach is to introduce new synthetic data to improve the model's learning of spurious correlations [7, 124, 220, 330]. To address errors present in the dataset, hallucinations can be detected for data cleaning [304]. To address the lack of specialized domain information, fine-tuning can be a straightforward and effective way. For instance, in ODYSSEY [166], the authors fine-tuned LLaMA-3 on the Minecraft Wiki, enabling it to better resolve Minecraft-related issues. Additionally, regarding incomplete learning, researchers have suggested improving loss functions [118, 243, 258, 313] and instruction tuning [160, 174, 209]. For incorrect decoding, new decoding strategies have been proposed to enhance the model's understanding of context and reduce hallucinations without the need to retrain the model [49, 105, 135, 237, 258].

Reducing Hallucinations via Memory Mechanism. To tackle outdated information in the dataset, external knowledge can be incorporated [205]. Retrieval-augmented generation (RAG) is a common approach used to tackle outdated data and the lack of specialized domain information in LLM training sets. For example, in WebGPT [189], the agent queries the Bing search engine each time and summarizes the search results.

Reducing Hallucinations via Multi-source Inputs. (i) Multi-agent collaboration. Different agents cross-verify and check each other, which is used in many agent frameworks to reduce the impact of hallucinations. For example, in ChatDev [212], the assistant engages in multiple rounds of communication with the instructor to seek more detailed suggestions and reduce hallucinations. (ii) System prompt standardization. By incorporating predefined output templates into the prompts for large models, the output of LLMs can be standardized to some extent, which can lower the likelihood of hallucinations [98]. For example, in GameGPT [42], the authors provided a standardized planning template for each game genre, guiding game development managers to fill in relevant information to reduce hallucinations.

4.1.3 Discussion of Limitations. There are four limitations of current solutions for hallucination. (i) Lack of theoretical analysis framework. Currently, there is a lack of mathematical definitions for hallucinations, and the methods for evaluating and mitigating them are primarily validated through experiments, lacking theoretical analysis. (ii) Insufficient baselines for multimodal LLMs. Current baselines for evaluating hallucinations in LLMs mainly focus on image and text modalities. However, LLMs have also been applied to speech [281] and video [178] modalities, and there are no established baselines for hallucination evaluation in these areas. (iii) Lack of evaluation methods for specialized domains. Current hallucination baselines for LLMs mainly address general tasks (e.g., image grounding [174], common-sense reasoning [156]). However, minimal hallucinations in these tasks don't imply the same for specialized domains (e.g., game agents, software development). Designing hallucination evaluation methods for specialized fields remains an unresolved issue. (iv) Imitation falsehood. Introducing additional knowledge through a memory mechanism can reduce the hallucinations caused by outdated training data to some extent. However, it may also lead to new hallucinations due to errors present in the additional knowledge itself [189].

#### 4.2 Memorization

LLMs are likelihood-based generative models trained to maximize the likelihood of observed samples (training data)  $\theta^*$  = arg max<sub> $\theta$ </sub>  $p_{\theta}(x)$ . At deployment time, generation is just sampling from the learned probability distribution  $p_{\theta^*}(x)$ . Intuitively, for some sample x, if the learned model overfits on it, i.e.,  $p_{\theta^*}(x)$  is excessively large, LLMs might eidetically generate  $x$ , posing the problem of training data memorization, which can be called parametric memorization [312].

In the LLM-based agents, there will be valuable information input to the LLM-based controller from the multi-source input modules. These mechanisms raise another type of memorization that exists in the specific context of interaction, which can be called contextual memorization [16, 312] that the valuable information specific to the context can be memorized by the agents.

Memorization is not necessarily a threat to the LLM-based agents, as it is shown that memorization is necessary for generalization [40, 76]. However, excessive memorization will have implications for data privacy, model stability, and generalization performance [257]. In the context of LLM-based agents, the development of memorization issues mainly involves LLM-based controllers. In this section, we review research on memorization that falls in a more conceptual scope and leave adversarial attacks and defenses on privacy due to memorization to Section 5.2.

4.2.1 Technical Progress. The literature analyzes memorization from various points. It is generally believed and demonstrated that memorization is a necessity to achieve generalization [40, 76]. Evaluation metrics of LLMs, e.g., perplexity, are near-perfect on the training data when a model memorizes the entire training set. On the otherhand, there have been efforts to understand how memorization occurs. Empirical studies [32, 134] have found that duplication in the training set, model capacity, prompt length, etc., are influential factors of memorization. Biderman et al. [21] proposed to predict which sequences will be memorized before a large model's full training time by extrapolating the Manuscript submitted to ACM

memorization behavior of lower-compute trial runs. Van den Burg et al. [257] gave a measure of memorization for generative models as the improved probability when a certain sample is involved in training.

4.2.2 Discussion of Limitations. The balance between memorization and generalization is a challenging problem. Empirical factors regarding memorization can reduce memorization on average but are not applicable to individual instances. Prediction of the behavior of large models based on small models or intermediate checkpoints are inaccurate and unreliable. Finally, harmful memorization depends on security demands and there lacks a projection from theoretical analysis results to practical risks.

#### **Bias and Fairness** $4.3$

Bias refers to a model's tendency to favor particular groups during decision-making or generation processes. This phenomenon is quite prevalent in AI models. For example, models used by American courts often predict a relatively higher probability of criminal behavior for African Americans [182]. These biased predictions stem from the hidden or neglected biases in data or algorithms [81]. In the context of LLM-based agents, the development of bias issues primarily involves two key features: LLM-based controller, multi-modal inputs and outputs. In the following, we report the technical progress in the bias issues.

4.3.1 Causes of Bias. For traditional machine learning models, bias can stem from two primary sources: biases in the data and flaws in the algorithms. Data biases are varied and can include discriminatory information, sampling bias, measurement bias, among others [26, 248, 318]. Even with unbiased data, models can still exhibit bias due to algorithmic factors. Algorithmic bias arises from design choices, such as selecting optimization functions and regularization techniques [14, 57]. Bias mitigation strategies include data cleaning, adjusting model architecture, and other techniques  $[56, 256]$ .

Bias is also present in LLMs. Compared to traditional models, bias in LLMs is both deeper and broader. Traditional decision models are confined to making decisions within a fixed scope, thus limiting their bias to a specific range. In contrast, LLMs perform generative tasks, which allows their outputs to include various types of biases. The absence of a fixed output format means these biases can be subtle and harder to detect than biases in decision-making models. Current research has found that LLMs exhibit biases in areas such as gender, race, and political views [78, 250], with different LLMs showing varying degrees and types of bias.

For LLM-based agents, the issue becomes even more complex. Unlike LLMs, LLM-based agents can process multimodal information, including text, images, and speech, leading to more intricate manifestations of bias. Current research has shown that biases and discrimination are also present in VLMs, affecting tasks such as visual question answering (VQA) and text-to-image generation [64, 101]. Therefore, comprehensive evaluation across multiple modalities is essential for accurate judgment. Furthermore, introducing additional components in LLM-based agents raises concerns about the potential introduction of new biases, necessitating careful consideration.

Analyzing the causes of bias, LLM-based agents exhibit more pronounced bias issues than traditional models due to their larger datasets and more complex structures. From the perspective of training data, LLMs are primarily trained on data sourced from online platforms, which is often not thoroughly vetted before training, leading to the inclusion of discriminatory samples. Additionally, biased statements are unevenly distributed within the training data. From the perspective of model structure, LLMs have a significantly greater number of parameters and a more complex architecture than traditional models. This complexity makes it challenging to ensure fairness during model training.

4.3.2 Technical Progress. To address and mitigate the bias issues in LLMs, current efforts mainly focus on three areas: (i) developing reasonable bias evaluation metrics, (i) constructing comprehensive bias evaluation datasets, and (iii) employing various techniques to mitigate model bias.

Evaluation metrics. The evaluation metrics for biases in LLMs can be classified based on the content they rely on during assessment. These include embedding-based metrics, which utilize contextual sentence embeddings, probability-based metrics, which use the model-assigned probabilities, and output-based metrics, which analyze the model's output.

Embedding-based metrics calculate distances in vector space between neutral words (e.g., professions) and identityrelated words (e.g., gender pronouns). Caliskan et al. [28] introduced the Word Embedding Association Test (WEAT) using static word embeddings to assess bias in NLP tasks. Later studies have employed word embeddings within entire sentences [90, 180] or calculated normalized sums of word-level biases [65].

Probability-based metrics focus on masked tokens. Some methods use templates to create sentences. They mask parts containing social groups and assess bias based on model-assigned probabilities [275]. Others sequentially mask each token in a sentence to test the model's ability to generate discriminatory content, known as pseudo-log likelihood methods [121, 187].

Output-based metrics include distribution-based and classifier-based methods. Distribution-based metrics detect differences between groups in model outputs to measure bias [149, 215]. Classifier-based metrics use classifiers to evaluate the toxicity of outputs; higher toxicity associated with specific groups indicates discrimination and bias  $[80, 83, 239]$ .

Evaluation datasets. Evaluation datasets consist of numerous texts requiring completion. By assessing the bias exhibited by LLMs towards different social groups during text completion, we can determine the magnitude of the model's bias. Masked token datasets like StereoSet [188] contain sentences with blanks that the LM must fill. The choices made by the model are then used to evaluate its bias. Conversely, unmasked sentence datasets such as CrowS-Pairs [190] and WinoQueer [77] present the model with pairs of sentences and ask which one is more likely.

Other methods use sentence completion rather than word selection. For example, TrustGPT [110] examines toxicity in LMs using toxic prompt templates derived from social norms. It then quantifies model bias by measuring toxicity values across different groups. BBQ [201] and Grep-BiasIR [129], on the other hand, employ a question-answering format for evaluation.

Bias Mitigation. After identifying the presence of bias in the model, it is natural to seek ways to mitigate it. Based on the LLM workflow, current bias mitigation techniques can be categorized into two types: training phase methods and inference phase methods.

During the training phase, the primary methods include cleaning training data, modifying model architecture, and adjusting training strategies. Data-based methods aim to eliminate biases in the training data. Data augmentation techniques add samples to extend the distribution for underrepresented social groups [84, 309]. Data filtering methods remove overtly biased and harmful text from the training data [82]. Architecture modifications primarily improve the encoder of LLMs by inserting new components like adapter models and gated models to mitigate bias [92, 131]. Training strategy adjustments mainly improve the loss function by adding regularization terms that measure bias. A common method is Reinforcement Learning from Human Feedback (RLHF) [17], aligning LLM output with human judgment. Other methods include reducing differences in the embedding distributions of different groups [291] and employing techniques such as contrastive learning [142] and adversarial learning [93] to guide the model. To avoid

the impact on model performance caused by modifying the loss function, some methods also freeze parts of model parameters during training [218, 300].

During the inference phase, the methods can be classified into three types: pre-processing, in-processing, and postprocessing. Instruction tuning occurs in the pre-processing stage, involving the addition or modification of instructions in user prompts to guide LLMs away from biased content [74, 260]. During the in-processing stage, some methods adjust the decoding algorithm to ensure the output's fairness [224], and others change the distribution from which tokens are sampled to enable the sampling of less biased outputs with greater probability [50]. Post-processing mitigation refers to post-processing on model outputs to remove bias. This is primarily achieved through rewriting the output. The simplest approach is to use keyword replacement to eliminate discriminatory terms  $[63]$ . Other methods employ specialized machine translation models to remove bias from sentences [11].

4.3.3 Discussion of Limitations. Despite extensive research on bias in LLMs, many issues persist. Firstly, most studies are based on traditional LMs, raising concerns about their applicability to LLMs. For example, Cabello et al. [27] argued that there is not necessarily a direct correlation between text embeddings and the biases present in LLM outputs. Similarly, Delobelle et al. [58] suggested that probability-based metrics may only weakly correlate with biases observed in downstream tasks. These findings cast doubt on the effectiveness of embedding-based and probability-based metrics for evaluating bias in LLMs.

Secondly, compared to traditional models, LLMs have a more complex structure and require significantly more parameters for training. This complexity means mitigating bias in LLMs can substantially impact their performance. For example, bias mitigation methods that involve fine-tuning typically use small datasets, which can lead to catastrophic forgetting in LLMs initially trained on large datasets.

Furthermore, the wide applicability of LLM-based agents underscores the limitations of current research. Although some efforts have been made to mitigate bias in text-to-image models, there is currently no standardized dataset or metrics for evaluating bias in generated images. Additionally, most bias research focuses predominantly on English, with a significant lack of studies addressing bias in other languages.

As a system, an LLM-based agent comprises multiple components, including the LLM itself, external tools, memory modules, and more. However, current research rarely considers the impact of bias on the entire agent from a system perspective. This area requires further investigation.

#### 5 Risks from Input-Model Interaction

This section analyze the mutual influences between the input data and the model, including backdoor and privacy leakage. For each risk, we first introduce what they are, then summarize their technological advancements in the six key features of LLM-based agents, and finally analyze their limitations.

#### 5.1 Backdoor

Backdoor attacks embed a malicious exploit during the training phase that is subsequently invoked by the presence of a trigger at the test time [85]. These attacks can be categorized into data poisoning-based attacks [8], where the adversary can only manipulate the training data, and model poisoning-based attacks [231], where the entire training process is compromised. The vulnerability to backdoor attacks extends from traditional machine learning models to advanced AI systems, including LLM-based AI agents. As shown in Fig. 10, in the context of LLM-based agents, the development of backdoor attacks mainly involves four key features: LLM-based controller, multi-modal interaction, Manuscript submitted to ACM

![](_page_23_Picture_1.jpeg)

Fig. 10. Backdoor attack targeting LLM-based agents may involve four key features (indicated with a red exclamation mark), leading to attacker-targeted action.

memory mechanism and tool invocation. In the following, we review the recent advancements in both attack strategies and defense mechanisms.

5.1.1 Technical Progress. Attack Perspective. As discussed in Section 2, several key features of LLM-based AI agents, such as tool invocation, multi-modal interaction, and memory mechanisms, introduce novel backdoor vulnerabilities.

Recent research in this area has proposed several attack methods to uncover the backdoor vulnerabilities associated with tool invocation. Dong et al. [66] demonstrated that backdoored adapters [102] of LLMs could lead agents to maliciously use tools, such as launching spear-phishing attacks. Yang et al. [292] investigated multiple triggering scenarios for activating backdoors to generate malicious tool commands, showing that triggers can be embedded directly in user queries or within intermediate observations returned by the environment. Jiao et al. [119] introduced methods like "word injection" and "scenario manipulation" to compromise LLM-based decision-making systems, leading to the generation of dangerous actions. Liu et al. [158] proposed poisoning the contextual demonstrations of a black-box LLM, causing it to produce programs with context-dependent defects. These programs appear logically sound but contain defects that can activate and induce unintended behavior (e.g., agent resource exhaustion and user privacy extraction) when the operational agent encounters specific triggers in its interactive environment.

The multi-modal interaction capabilities of LLM-based AI agents have also spurred research into multi-modal backdoor attacks. For instance, Liang et al. [151] introduced the BadCLIP attack, which aligns visual trigger patterns with the textual target semantics in the embedding space, making it challenging to detect the subtle parameter changes caused by backdoor learning on these naturally occurring trigger patterns. Liu et al. [158] developed a dual-modality activation strategy that manipulates both the generation and execution of program defects, triggering unintended agent actions through a combination of textual and visual cues. Notably, their proposed backdoor attack was successfully demonstrated on real-world systems, including Jetbot vehicles [2] and autonomous driving systems [3, 4].

Recent research has also highlighted the security risks associated with memory mechanisms in LLM-based AI agents. Zou et al. [338] introduced PoisonedRAG, the first poisoning attack targeting the external knowledge databases of these agents. Their work identifies two critical conditions for a successful knowledge poisoning attack: the retrieval condition and the generation condition. To meet these conditions, they employ optimization and heuristic techniques, effectively compromising the integrity of the agent's knowledge base. Chen et al. [46] developed AgentPoison, a red teaming approach that poisons the agent's long-term memory or external knowledge database. This method optimizes the injected textual trigger for high transferability, in-context coherence, and stealth, making it particularly insidious and difficult to detect.

**Defense Perspective.** Defense methods against backdoor attacks for LLM-based AI agents can be broadly classified into three categories: dataset sanitation, input purification, and output verification.

Dataset sanitation involves detecting and removing poisoned samples from the instruction tuning dataset to create a "backdoor-free" agent. Liu et al. [164] developed a technique that first identifies backdoor triggers in the instruction tuning dataset and then prevents the model from learning these triggers. Liang et al. [150] proposed a method for isolating poisoned samples in multi-modal training datasets by enhancing the model's resilience to backdoor triggers through a staged and targeted training approach.

Input purification focuses on preprocessing the input data of an agent to eliminate embedded triggers during deployment. To address retrieval corruption attacks targeting the memory mechanism, Xiang et al. [286] proposed an isolate-then-aggregate strategy. This approach involves first obtaining the LLM's responses from each retrieval passage in isolation and then securely aggregating these isolated responses using customized keyword-based and decoding-based methods.

Output verification involves auditing the actions and responses of the agent to ensure safe and secure interactions between the agent and the external environment, particularly against the attacks targeting tool invocation. Several sandbox environments, such as E2B [1], ToolSandbox [173], and ToolEmu [222], offer controlled settings for testing agents under various conditions and scenarios, including stress tests to assess performance under extreme or unexpected data conditions. Based on the testing results, an output verification mechanism can be implemented to protect agents from the identified risks.

5.1.2 Discussion of Limitations. Attack Perspective. Despite the rapid progress in backdoor attacks on LLM-based AI agents, the current literature still lacks a comprehensive understanding and evaluation of the associated backdoor vulnerabilities. For instance, existing backdoor attack objectives do not fully exploit the functional weaknesses of these agents. While most attacks focus on generating malicious tool commands or manipulating the agent's responses to users, other critical objectives, such as injecting faulty task plans or forcing the selection of adversary-specified tools, remain underexplored. Additionally, most attack sources are centered on image and text data, while other prevalent modalities, such as audio and video, receive less attention in the context of backdoor attacks on LLM-based AI agents. Defense Perspective. Current defense strategies against backdoor attacks on LLM-based AI agents typically focus on safeguarding individual components, such as dataset sanitation and input data purification. However, there is a clear lack of systematic defenses that can protect against backdoor attacks originating from multiple sources and targeting various objectives. A potential solution lies in developing collaborative defense strategies across different components of the agent system. These strategies could involve: 1) multimodal input purification for the input module, 2) training dataset sanitation and backdoor model purification for the decision-making module, 3) tool invocation verification, environment feedback filtering, and memory inspection for interactions with external entities, and 4) output verification for the final response module. Such collaborative defenses are essential for securing the complete LLM-based AI agent framework and offer a promising avenue for future research in backdoor defense.

#### 5.2 Privacy Leakage

The generative nature of LLMs renders them more susceptible to privacy infringement and sensitive information, e.g., ID, medical records, can be inadvertently or adversarially leaked to the generated data, posing significant concerns on users' privacy. Tracing the source of private information, any modules the external data flows in would cause the agent to violate privacy requirements. First, the LLM-based controller faces the threat of privacy leakage in its training data. Manuscript submitted to ACM

![](_page_25_Figure_1.jpeg)

Fig. 11. Privacy leakage targeting LLM-based agents may involve three key features (indicated with a red exclamation mark), leading to information leakage.

In addition, sensitive information contained in the multi-source input modules might transfer to other components in an uncontrolled way. As a result, individual private information might be disseminated to other users or external tools, breaching the contextual integrity [195] of privacy. As shown in Fig. 11, in the context of LLM-based agents, the development of privacy leakage mainly involves three key features: LLM-based controller, multi-source inputs and memory mechanism. In the following, we review the recent advancements in privacy leakage.

Technical Progress. Attack Perspective. Depending on the sources of leaked information, existing threats can  $5.2.1$ be enumerated as training data leakage [36, 125, 127, 175, 236, 305] and contextual privacy leakage [16, 108, 184, 312], as shown in Fig. 11.

Training data leakage. A common topic in training data leakage is membership inference. Membership inference [238] aims to expose information on whether certain samples were used to train or fine-tune the model. Similarly to attacks built for earlier recognition models, membership inference against LLMs also bases on the intuition that trained models fit better for seen samples during training than unseen ones. Specifically, membership inference is achieved by using training loss or a variant of it [36, 127, 236] to distinguish members from non-members. Data augmentation [127], reference shadow models [30, 274] have been shown to be beneficial approaches to improving the discrimination of inference metrics.

More than membership, a number of studies have demonstrated that LLMs are possible to generate original training data [127, 192, 305]. Through massive random generation followed by membership inference, training data including personally identifiable information can be emitted identically. Besides LLMs, multimodal generative models have also been shown to be subject to training data extraction [31, 240, 241]. Training data extraction is a direct result of memorization. It indicates that generative models, including LLMs, can inadvertently replicate their training data.

As expected, LLMs suffer from adversarial inference attacks [104, 125, 125, 175] that intentionally induce fine-grained private attributes by prompting LLMs with known information. The adversarial attacks still maintain a certain degree of effectiveness even in the presence of common mitigation strategies, such as anonymization and differential privacy.

Contextual privacy leakage. In LLM-based agents, the core LLM controller is usually equipped with external knowledge to enhance its capabilities in accomplishing user-specified tasks. Typical external knowledge can come from a retrieval database [39, 162, 216] or user input data [197, 225]. The threat of contextual information leakage focuses on privacy contained in the multi-source context provided to LLM controllers. Latest studies all reveals that LLM-based agents are vulnerable in contextual privacy protection [16, 108, 184, 312]. For example, Mireshghallah et al. [184] showed that Manuscript submitted to ACM

| <b>Features</b><br>Agents | LLM-based<br>Controller    | Multi-modal<br>Inputs and<br><b>Outputs</b> | Multi-source<br><b>Inputs</b>                                 | Multi-round<br>Interaction                | Memory<br><b>Mechanism</b>                      | <b>Tool Invocation</b>                |
|---------------------------|----------------------------|---------------------------------------------|---------------------------------------------------------------|-------------------------------------------|-------------------------------------------------|---------------------------------------|
| <b>WebGPT</b>             | <b>A finetuned GPT3</b>    | <b>Natural language</b>                     | System prompt,<br>website reference,<br>user input,<br>memory | <b>Several rounds</b><br>with website     | <b>RAG on Bing</b><br>browser                   | <b>Some action APIs</b><br>on website |
| <b>Voyager</b>            | GPT3.5 and GPT4            | <b>Natural language</b><br>and code         | System prompt,<br>game feedback,<br>user input,<br>memory     | <b>Several rounds</b><br>with game        | <b>Skill library</b>                            | <b>Some action APIs</b><br>on game    |
| PReP                      | A LVLM and<br>several LLMs | Image and text                              | System prompt,<br>environment<br>feedback,<br>memory          | <b>Several rounds</b><br>with environment | Long-term<br>memory and<br>short-term<br>memory | <b>No</b>                             |
| ChatDev                   | Several agents             | <b>Natural language</b><br>and code         | System prompt,<br>other agents'<br>feedback,<br>memory        | <b>Several rounds</b><br>including agents | Long-term<br>memory and<br>short-term<br>memory | <b>No</b>                             |

Fig. 12. Six key features of four agents.

commercial models such as GPT-4 and ChatGPT reveal private information in contexts that humans would not, 39% and 57% of the time, respectively.

Defense Perspective. We then discuss existing countermeasures of privacy leakage in LLM-based agents, respectively for training data leakage and contextual privacy leakage.

Training data leakage. Depending on the characteristics of defending methods, we categorize them into differential privacy (DP), unlearning and heuristic approaches.

As the aforementioned risks rely on memorization of certain training samples, DP [71] is a promising method to defend against leakage by reducing the influence of individual sample on the trained model. DP provides a theoretical guarantee on the privacy of the resulting models and is flexible to be implemented due to the property of pose-processing. Studies on DP-LLM cover the entire life cycle of LLMs, including DP-preprocessing [99, 306], DP-training [141, 265, 301, 307], DP-prediction [251, 283], etc.

Regulations such as the European Union's General Data Protection Regulation (GDPR) [219], have granted individuals the rights to have their personal data deleted. This has led to a series of works focusing on unlearning [194], which provides another safeguard by allowing to erase the undesired information from a trained model. The unlearning is defined in a way similar to DP, which is considered successful if the unlearned model is close to a model trained on the dataset excluding the samples to be forgotten. In contrast, unlearning focus on efficient adjustment of a trained model by additional specific updates [23, 87, 193]. Unlearning for LLMs [115, 203] restrains the likelihood of text to unlearn by maximizing their training loss and is demonstrated to be effective to defend against extraction attack.

In addition, heuristic approaches are designed to eliminate implicit factors leading to training data leakage. These include deduplication [134], anonymization [157] and sampling calibrition [41, 278].

Contextual privacy leakage. Plenty of defensive approaches are proposed to mitigate contextual information leakage. Sanitization of context sensitive data through anonymization [108], rewritten by another agent [311] and squeezing to minimum amount of information [16] help reduce leakage. Differential privacy is also found effective to mitigate privacy leakage of context data by either generating contextual data [252] or aggregating responses from multiple queries [283]

in a differential private way. At a higher systematic level, Zhang et al. [322] presented PrivacyAsst that incorporates the homomorphic encryption scheme and privacy-preserving techniques such as t-closeness into LLM-based agents.

5.2.2 Discussion of Limitations. Research on data leakage mainly handles the trade-off between privacy and utility. Employment of existing attacks and defenses and their effectiveness and efficiency in practical LLMs is still a problem. Privacy-preserving training such as DP cannot scale up to high-dimensional data and models [31, 265]. Strategies such as anonymization and deduplication are not sufficient to completely eradicate leakage [175]. Several works [16] resort to the theory of contextual integrity to analyze the private information flow in LLM-based agents, which is promising while restricted in a two-side single-round conversation. In practice, the LLM-based agents will take multi-source input and perform multi-round interaction with users or external tools (see Fig. 3), where the information involved and its flow are much more complicated. Empirical mitigations [16, 108, 311] ultimately rely on another agents to recognize, refine or compress private information, whose reliability will be a major concern for this kind of solution.

# 6 Case Study

In Section 2, we introduce a general framework for LLM-based agents. In the following sections, we classify and summarize the potential risks that agents may face based on this framework. However, actual agents may not necessarily contain all modules in the general framework, and the designs within these modules may also be customized. More importantly, the environments and scenarios they face have significant differences. This means that different agents will face specific and different risks in actual use. Therefore, this section presents case studies of four different agents, representing the following four situations. (i) WebGPT [189], an agent with the complete components working for the general question-answering tasks; (ii) Voyager [263], an embodied agent working for playing games; (iii) PReP [310], an embodied agents working for the real-word tasks; (iv) ChatDev [212], a multi-agent framework. We summarize their key features and tasks in Fig. 12. In the following, we first analyze the specific impacts of the key features of WebGPT on various threats. Then, for the rest three agents, we identify their differences with the previous agents and analyze the specific impacts of these differences on various threats.

#### 6.1 WebGPT

The appearance of ChatGPT has sparked global attention on LLMs. Its fluent conversational abilities and powerful question-answering capabilities have been widely embraced by users. Question answering has also become one of the fundamental tasks for LLMs. However, since LLMs can only learn from the content in their training datasets, they are unable to answer questions about topics that occurred after the cutoff time of their training data. WebGPT [189] proposes a classic solution to this issue by equipping LLMs with the ability to use a web browser. Specifically, for a user's query, it uses Bing search to find the relevant web pages, quote the relevant information, and summarize the content. WebGPT is one of the most classic agents. As shown in Fig. 12, it possesses all six characteristics of a complete agent. We now analyze the potential threat sources and resulting threat that may arise within the WebGPT framework. Risks from Problematic Inputs. For the vast majority of threats arising from problematic inputs, the risks faced by WebGPT become more severe than those of a standalone LLM, as it involves external content from the internet and does not perform any cleansing or formatting operations on either the input (including the user's inputs and the website references) or the output. An attacker can induce the agent to navigate to a predefined webpage, thereby executing an attack on the agent. Take goal hijacking as an example. The LLM-controller of WebGPT (i.e., a fine-tuned GPT-3) is easily susceptible to goal hijacking attacks, as it has no defense mechanisms against such attacks. From the Manuscript submitted to ACM

Table 2. Summary table: specific impacts of WebGPT's different key features on various threats. Compared to standalone LLMs, if a key feature of WebGPT increases one threat, we highlight it in red; if it is unrelated to one threat, we mark it in yellow; and if it decreases one threat, we indicate it in green.

| Sources                 | Threats          | Key Features                              | Specific Impacts                                |
|-------------------------|------------------|-------------------------------------------|-------------------------------------------------|
|                         | Goal Hijacking   | Multi-source Inputs                       | WebGPT may be attacked by the user and the      |
|                         |                  |                                           | website.                                        |
| Problematic Inputs      |                  |                                           | Although WebGPT may engage in multi-round       |
|                         |                  | Multi-round Interaction                   | interaction with multiple web pages, web pages  |
|                         |                  |                                           | passively provide information.                  |
|                         |                  | <b>Tool Invocation</b>                    | Using the Bing browser as a tool involve exter- |
|                         |                  |                                           | nal attack surface.                             |
|                         |                  |                                           | The model's input does not equal the user's     |
|                         |                  | Multi-source Inputs                       | input, causing random prompts to be diluted     |
|                         | Model Extraction |                                           | by a series of fixed system prompts.            |
|                         |                  |                                           | Code capabilities of WebGPT do not originate    |
|                         |                  | <b>Tool Invocation</b>                    | from the model itself, but rather from the sum- |
|                         |                  |                                           | marization of search results.                   |
|                         |                  | <b>LLM-based Controller</b>               | Erroneous decoding processes.                   |
| Model Flaws             | Hallucination    |                                           | The memory mechanism decreases the threat       |
|                         |                  | <b>Memory Mechanism</b>                   | of hallucination caused by outdated knowledge   |
|                         |                  |                                           | and spurious correlations.                      |
|                         |                  | Finetuning decreases the knowledge gap on |                                                 |
|                         |                  | <b>Tool Invocation</b>                    | tool use.                                       |
|                         |                  |                                           | The invocation of the Bing browser enhances     |
|                         |                  |                                           | the relevance and authority of search results,  |
|                         |                  |                                           | decreasing the impact of imbalanced and incor-  |
|                         |                  |                                           | rect training data.                             |
|                         |                  |                                           | WebGPT introduces external training process.    |
|                         | Backdoor         | <b>LLM-based Controller</b>               | Malicious human demonstrations can be in-       |
|                         |                  |                                           | jected when finetuning GPT-3.                   |
| <b>Combined Threats</b> |                  | Multi-source Inputs                       | WebGPT may be triggered by web pages and        |
|                         |                  |                                           | the user inputs.                                |
|                         |                  | <b>LLM-based Controller</b>               | WebGPT introduces external training process,    |
|                         | Privacy Leakage  |                                           | which may involve additional privacy data.      |
|                         |                  |                                           | WebGPT has a large external web database        |
|                         |                  | <b>Memory Mechanism</b>                   | with a wealth of private information hidden     |
|                         |                  |                                           | within.                                         |
|                         |                  | <b>Tool Invocation</b>                    | WebGPT uses Bing browser, which can be a        |
|                         |                  |                                           | strong privacy information extractor.           |

source perspective, the multi-source inputs (i.e., the user inputs and memory mechanism) increase the attack surfaces of goal hijacking. From the attack objective, it can manipulate the model to output specific content or target at the tool invocation, e.g., control it to click on specific web pages. It is important to note that while multi-round interaction can enhance the attack's effectiveness in some situations (referring to Section 3.2), these situations often require users to customize their inputs based on the model's responses [47]. However, in the WebGPT scenario, it uses the Bing browser multiple times, with the webpages passively providing information. Therefore, multi-round interaction does not significantly impact the intensity of threats in this context.

As an exception, WebGPT exhibits higher resistance to model extraction attacks, since the inputs of WebGPT are not the inputs of the GPT-3. For instance, Carlini et al. [35] proposed querying large language models (LLMs) with random prompts to steal the parameters of the model's final layer. However, random prompts for WebGPT might result in similar outputs due to uncontentious information, such as "sorry, I cannot find the information about...". Such similar outputs decrease the rank of outputs, bringing more difficulty to model extraction attacks. Additionally, the method proposed by Li et al. [147] to steal the specialized code abilities of text-davinci003 would also fail. This is because the agent's responses are summaries of relevant web content rather than the model's own code knowledge.

Risks from Model Flaws. WebGPT effectively addresses the issues of hallucinations and biases caused by problems in the training dataset. For example, regarding hallucinations, WebGPT's memory knowledge base stores a real-time updated index of websites across the internet, which helps mitigate hallucinations caused by outdated data in the training set. Additionally, RAG can reduce the issue of spurious correlations resulting from incomplete training. The Bing browser, as a powerful search engine, enhances the relevance and popularity of the retrieved information when used as an information retrieval tool, thereby somewhat reducing hallucinations caused by erroneous and unbalanced data in the training set. At the same time, since the model has undergone fine-tuning to align with the user's environment, it reduces the impact of hallucinations caused by the knowledge gap in tool invocation. However, it does not address hallucinations caused by erroneous decoding processes.

Risks from Input-Model Interaction. WebGPT may face additional backdoor poisoning risks, as it involves external training processes. Malicious insiders could compromise the data collection process of building WebGPT, by injecting faulty human demonstrations or preferences [268] into the training data. A backdoored WebGPT may produce incorrect or malicious responses to the user's questions embedded with the trigger, even if informative question-associated Web references are successfully retrieved by the WebGPT.

In addition, involving a large external web database increases the privacy information leakage. Malicious users may leverage WebGPT as a convenient channel to collect private information with the powerful information retrieval capabilities of the Bing Browser. The privacy threat may also be amplified given the semantic understanding capability of WebGPT when it is further used to infer potential connections between retrieved data and entities.

In summary, WebGPT decreases the risks due to model flaws. However, it increases much attack surface for the problematic inputs and combined threats, as shown in Table 2.

### 6.2 Voyager

As the potential of LLMs is gradually being explored, LLM-based agents can be used to handle more complex tasks, and the emergence of embodied LLM agents has followed. This kind of agent is physically embodied, either in the real world through a robot or in a simulated environment through a virtual avatar. We choose two cases for embodied LLM agents that cover gaming scenarios as well as applications closer to real-world scenarios: Voyager [263], which is embodied in the Minecraft game, and PReP [310], which is embodied in a city navigation system. Compared to WebGPT, Voyager has 4 differences. (i) It directly utilizes GPT-3.5 and GPT-4.0 as its control hub. (ii) It requires ChatGPT to output both natural language and Java code. (iii) It designs an internal memory module, called the skill library, that the agent can read from and write to. (iv) They target different tasks. WebGPT is designed for answering general questions, while Voyager focuses on completing tasks in Minecraft. We now analyze the specific impacts of these differences on the various threats.

Risks from Problematic Inputs. For most input-based attacks, Voyager is similarly vulnerable to WebGPT and individual LLMs. Although Voyager prompts GPT-4 to generate JavaScript in the system instructions, it does not Manuscript submitted to ACM

Table 3. Summary table: specific impacts of Voyager's differences on various threats. Compared to standalone LLMs and WebGPT, if a difference of Voyager increases one threat, we highlight it in red.

| Sources            | Threats          | Differences with Previous Agents  | Specific Impacts                    |
|--------------------|------------------|-----------------------------------|-------------------------------------|
| Problematic Inputs | Model Extraction | VOYAGER directly call GPT-4.0     | Attackers can steal the standalone  |
|                    |                  | and GPT-3.5                       | models of GPT-3.5 and 4.0.          |
|                    |                  |                                   | Playing Minecraft falls into a spe- |
|                    |                  | VOYAGER plays Minecraft game.     | cialized domain, and directly us-   |
| Model Flaws        | Hallucination    |                                   | ing ChatGPT may lead to halluci-    |
|                    |                  |                                   | nations due to knowledge gaps.      |
|                    |                  |                                   | Code generation is more sensitive   |
|                    |                  | VOYAGER outputs both natural      | to hallucination issues, as gener-  |
|                    |                  | language and Javasript code.      | ated buggy code is more likely to   |
|                    |                  |                                   | be non-executable.                  |
| Combined Threats   |                  | An inner memory module that the   | Downloading pre-trained skill li-   |
|                    | Backdoor         | agent can read from and write to. | braries from others increases the   |
|                    |                  |                                   | risk of additional backdoors.       |

Table 4. Summary table: specific impacts of PReP's differences on various threats. Compared to standalone LLMs, WebGPT, and Voyager, if a difference of PReP increases one threat, we highlight it in red.

| Sources            | Threats             | Differences with Previous Agents                                                                       | Specific Impacts                                                                                                        |
|--------------------|---------------------|--------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------|
| Problematic Inputs | Adversarial Example | Its input is multimodal, including<br>both text and images.                                            | VLATTACK can generate stronger<br>adversarial examples by simulta-<br>neously perturbing both modali-<br>ties of input. |
| Model Flaws        | Hallucination       | The interaction environment con-<br>sists of actual landmark pho-<br>tographs of Beijing and Shanghai. | Non-English languages and East-<br>ern countries are more prone to<br>hallucinations.                                   |
| Combined Threats   | Backdoor            | Its decision module contains sev-<br>eral LLMs (including an LVLM,<br>GPT-3.5, and GPT-4.0).           | More models and additional train-<br>ing processes (such as fine-tuning<br>LVLM) increase the risk of back-<br>doors.   |

perform any additional checks or formatting on the input and output, making it still vulnerable to attacks originating from the input (such as goal hijacking). More seriously, unlike WebGPT, Voyager is vulnerable to model extraction attacks. This is because Voyager directly utilizes GPT-3.5 and 4.0, allowing attackers to steal the standalone models of GPT-3.5 and 4.0, which can then enhance other types of problematic input attacks against Voyager.

Risks from Model Flaws. Compared to WebGPT, Voyager is more susceptible to hallucination issues. Playing Minecraft falls into a specialized domain, and directly using ChatGPT may lead to hallucinations due to knowledge gaps. At the same time, code is more sensitive to errors caused by hallucinations compared to natural language, as it may cause the game to crash. To tackle this problem, the authors suggested that this can be mitigated through RAG (e.g., by calling the Minecraft Wiki). Another possible approach is to address the knowledge gap through fine-tuning, as utilized by another Minecraft agent, ODYSSEY [166].

Risks from Input-Model Interaction. Voyager may face additional backdoor poisoning risks when downloading pre-trained skill libraries from others. Adversaries could first inject faulty executable codes into Voyager's skilled library by hijacking the feedback returned by external environments. Subsequently, a backdoor trigger can be optimized to Manuscript submitted to ACM

Table 5. Summary table: specific impacts of ChatDev's differences on various threats. Compared to standalone LLMs, WebGPT, Voyager, and PReP, if a difference of ChatDev increases one threat, we highlight it in red; if it is unrelated to one threat, we mark it in yellow; and if it decreases one threat, we indicate it in green.

| Sources                 | Threats        | Differences with Previous Agents                                       | Specific Impacts                                                                                                                               |
|-------------------------|----------------|------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------|
| Problematic Inputs      | Goal Hijacking | ChatDev can call the system kernel<br>for dynamic testing of the code. | ChatDev can cause more severe is-<br>sues like running malicious code on<br>the system during dynamic testing.                                 |
|                         |                | ChatDev is a framework of multiple<br>GPT-based agents.                | Malicious inputs can be replicated to<br>all agent.                                                                                            |
| Model Flaws             | Hallucination  |                                                                        | LM's tendency to overcommit to<br>early mistakes can lead to further er-<br>rors.                                                              |
|                         |                | It allows agents have multiple round<br>of conversations.              | Frequent cooperation between agents<br>can amplify minor hallucinations.<br>Multiple round of conversations give<br>more detailed requirement. |
| <b>Combined Threats</b> | Backdoor       |                                                                        | ChatDev may be vulnerable to<br>instruction-backdoor attacks.                                                                                  |

ensure that the faulty executable codes can be retrieved from the skill library when the trigger is present in the user's instructions, leading to the execution of adversary-desired actions.

In summary, Voyager increases the risks from the problematic inputs, model flaws and combined threats, as shown in Table 3.

# 6.3 PReP

As mentioned above, PReP [310] is a recent agent embodied in a city navigation system. It determines its location and plans its direction by observing the surrounding landmarks. Compared to WebGPT and Voyager, PReP has 3 differences. (i) Its input is multimodal, including both text and images. (ii) Its decision module contains several LLMs (including an LLaVA, GPT-3.5, and GPT-4) that perform environmental observation, memory management, plan generation, and control of the final actions. (iii) The interaction environment of PReP consists of actual landmark photographs. We now analyze the specific impacts of these differences on the various threats.

Risks from Problematic Inputs. Compared to WebGPT and Voyager, PReP faces a greater threat from problematic inputs because it deals with two modalities of input, both of which can be easily manipulated. For instance, VLATTACK [299] can generate stronger adversarial examples by simultaneously perturbing both modalities of input. Additionally, Kimura et al. [126] pointed out that simply adding an "ignore" prompt to images can partially succeed in hijacking multimodal large models. This type of attack is particularly easy to implement in the PReP scenario, such as by printing and posting corresponding hijacking prompts on landmarks.

Risks from Model Flaws. PReP is significantly threatened by model flaws. Taking the hallucination issue as an example, PReP is particularly prone to hallucinations. Unlike WebGPT and Voyager, PReP's hallucinations are more likely to arise from its tendency to misinterpret non-English content, as well as the compounded effects of multiple LLMs enhancing the hallucination impact. PReP directly calls the GPT-4. However, GPT-4 has been revealed to be more likely to hallucinate when encountering Eastern countries or non-English contexts [52], and the test datasets in the experiments contain the landmarks of Beijing and Shanghai. Additionally, the results of such threats are more severe than WebGPT Manuscript submitted to ACM

32

and Voyager. Take the bias issue as an example. If the model lacks comprehensive and real-time information about urban traffic, it may introduce biases during navigation, potentially resulting in the selection of suboptimal routes or even failure to reach the destination. Factors such as changes in the road network, road maintenance, real-time traffic congestion, and the vehicle's driving specifications are all potential sources of bias.

Risks from Input-Model Interaction. PReP may be vulnerable to backdoor poisoning attacks. Adversaries could poison the fine-tuning dataset of the LLaVA model used for perceiving the landmarks in the street views. Once a backdoored PReP is deployed, an attacker could place a physical backdoor trigger in the agent's view to mislead the recognition and segmentation of landmarks, which might further navigate the agent to a wrong route.

In summary, PReP increases the risks from the problematic inputs, model flaws and combined threats, as shown in Table 4.

#### ChatDev 6.4

The powerful capabilities of LLMs enable them to generate code in various languages. Code security is an important issue in the security field, and there has been a lot of research on the security of the code generated by LLMs. Many agents recently proposed software development involving the collaboration of multiple LLMs. Here, we consider ChatDev [211], which is a recent framework for software development involving multiple LLM-based agents. Compared to the above three agents. ChatDev has 3 differences, (i) Its framework includes several agents (each agent contains an LLM, either GPT-3.5 or GPT-4). (ii) It allows multiple rounds of conversations between different agents. (iii) It can call the system kernel for dynamic testing of the code. We now analyze the specific impacts of these differences on the various threats.

Risks from Problematic Inputs. Similar to a single agent, ChatDev can be subjected to various attacks from inputs (such as jailbreaking and goal hijacking). This is because malicious inputs can propagate between multiple agents. For example, Morris II [51] is designed to target cooperative multi-agent ecosystems by replicating malicious inputs to infect other agents. The threat of Morris II stems from its capacity to leverage the connectivity between agents, potentially leading to a swift collapse of multiple agents once one becomes infected, resulting in issues like running malicious code on the system during dynamic testing.

Risks from Model Flaws. Compared to a single agent, ChatDev may experience more severe issues stemming from the model itself (such as bias and hallucinations). For instance, regarding hallucinations, although ChatDev attempts to mitigate this by requiring the assistant to actively seek more detailed suggestions from the instructor before providing a formal response, it cannot completely avoid the problem. Additionally, frequent cooperation between agents can amplify minor hallucinations [98], and an LM's tendency to overcommit to early mistakes can lead to further errors that it would not otherwise make [319], making the multi-agent framework more prone to hallucinations.

Risks from Input-Model Interaction. ChatDev may be vulnerable to instruction-backdoor attacks [320]. A malicious insider of the development team of ChatDev could implant backdoored instructions into the system prompts of one of the multiple agents. Such backdoored instructions specify a scenario where the backdoored ChatDev will produce attacker-desired software under the presence of a trigger embedded within the user's instruction.

In summary, ChatDev increases the risks from the problematic inputs, model flaws and combined threats, as shown in Table 5.

#### **7** Future Direction

In the above sections, we summarize the various threats faced by LLM-based agents and point out the limitations of current research. Based on the previous summaries and analyses on the four case studies, we propose several potential future research directions from three perspectives: data support, methodological support, and policy support. Data Support. Datasets are the foundation for risk analysis. Sufficient and comprehensive datasets allow for a more thorough and fair comparative analysis of risks and their corresponding methods. Based on Fig. 9, the current datasets used to evaluate the various threats faced by LLM-based agents have the following shortcomings:

- Lacking of Multi-round Interaction Data in Real Scenarios. Most agent interactions in real-world situations are multi-round (e.g., the four agents in the case study). Additionally, some threats become more severe in multi-round interaction (e.g., goal hijacking). Therefore, curating datasets with multi-round interaction can better assess the severity of threats.
- Task Limitations to General Q&A Datasets. Agents in real scenarios often deal with specific domains (e.g., the last three agents in the case study). Some threats arise due to knowledge gaps in specialized fields, making them more severe (e.g., hallucination). Thus, developing datasets focused on specialized domains can provide a better evaluation of the threats posed by LLM applications in those areas.
- Modality Mostly Limited to Plain English Text. Many agents in real scenarios must handle multilingual and multimodal situations (e.g., PReP in the case study). Certain threats can become more pronounced in lowresource languages or multimodal contexts (e.g., jailbreaking). Therefore, creating datasets that include a variety of modalities and languages can enhance threat assessment.
- Input Often Limited to a Single Role (i.e., User). In real scenarios, LLMs in agents receive inputs from multiple sources (e.g., the four agents in the case study). Some threats may escalate when multiple input sources are involved (e.g., backdoor attacks). Thus, creating datasets with multiple input sources can improve the evaluation of threat severity.
- Evaluation Typically Focused on a Single LLM. In real-world scenarios, an agent may incorporate multiple LLMs (e.g., PReP), and multiple agents may work together (e.g., ChatDev). Some threats can become more severe during interactions among multiple LLMs (e.g., bias). Therefore, establishing baselines for interactions in multi-LLM scenarios can provide a better assessment of threat levels.

Methodological Support. Through the previous analysis of the limitations of technical advancements for each type of threat and the practical scenario analysis in the case study, we propose the following future research directions from the perspective of methodological support:

- Theoretical Analysis Framework. Currently, some forms of threats lack mathematical definitions, making it difficult to analyze and quantify their severity theoretically. For example, in the case study, all agents faced varying degrees of hallucination issues for different reasons. While they reduced the probability of hallucinations from certain perspectives (e.g., ChatDev's communicative dehallucination mechanism), they also increased the intensity of hallucinations due to other factors (e.g., collaboration among multiple agents in ChatDev). Since hallucinations lack a mathematical definition, it is challenging to analyze the extent of the hallucination problems they ultimately face. Future work can focus on rigorously defining the threats faced by LLM-based agents and establishing an analytical framework for clearer analysis and quantification of various risks.
- Interpretability-driven Attack and Defense Strategies. Most current research on threats explores them through heuristic methods. For instance, most goal hijacking attack strategies are based on scripting or gradient Manuscript submitted to ACM

optimization. One pioneering study achieved promising results in detecting goal hijacking by observing the internal activation of models [5]. Model explanation methods can reveal the decision-making basis of models from different perspectives. In the future, applying various explanation algorithms to design attack and defense strategies could provide a deeper understanding of the risks associated with LLM-based agents and enhance defensive measures.

• Agent-specific Attack and Defense Strategies. Similar to the issues faced with data support, most current attack and defense methods are primarily designed for standalone LLMs, with only a few addressing 1-2 key features of agents. For example, Dong et al. [66] categorized agents into different levels (i.e., L1 to L5) based on their complexity when studying backdoor attacks. In fact, most current agent frameworks (including the four agents in the case study) belong to the highest level (i.e., L5). Future research on threats faced by higher-level agents can bring us closer to real-world usage scenarios of agents.

Policy Support. To ensure the reliability of LLM-based agents during their usage from a government policy perspective, the following key considerations can be addressed:

- Establish an Agent Constitution Framework. There are many surveys on LLM-based agents. Most of these surveys provide a framework for the agents' operations [53, 61, 270, 327]. While they share many similar modules (such as memory, decision-making modules, etc.), there are also some differences. The government can develop a comprehensive agent constitution that outlines the fundamental principles, rules, and guidelines for the safe and ethical operation of LLM-based agents. This constitution should address aspects such as safety, security, privacy, and alignment with societal values.
- Refine Governance Frameworks and Regulatory Policies. Many countries and regions, such as the European Union [255] and the United States [100], have introduced safety legislation regarding AI, but they have yet to provide specific regulations for LLM-based agents. Develop governance frameworks and regulatory policies that address the unique challenges posed by LLM-based agents, such as liability, data privacy, and the potential for misuse or unintended consequences. These policies should be adaptable to the rapidly evolving landscape of LLM technology.
- Invest in Research and Development. Allocate resources for research and development focused on enhancing the reliability, safety, and security of LLM-based agents. This can include funding for the development of advanced safety mechanisms, improved reasoning capabilities, and the exploration of alternative AI architectures that may offer greater reliability and trustworthiness.

# 8 Conclusion

This survey focuses on the various threats faced by LLM-based agents. We propose a new taxonomy of these threats and summarize their technical advancements and limitations based on this framework and taxonomy. Subsequently, we select four real-world agents as case studies to analyze the types of threats these agents may encounter in practical use and their underlying causes. Finally, based on the above analysis, we propose promising directions for future research.

#### References

- [1] 2021. E2B Sandbox. https://github.com/e2b-dev/e2b.
- [2] 2021. JetBot. https://github.com/NVIDIA-AI-IOT/jetbot.
- [3] 2024. Pixloop. https://www.pixmoving.com/pixloop.
- [4] 2024. Who we are. https://www.pixmoving.com/about-us.

- [5] Sahar Abdelnabi, Aideen Fay, Giovanni Cherubin, Ahmed Salem, Mario Fritz, and Andrew Paverd. 2024. Are you still on track!? Catching LLM Task Drift with Activations. arXiv preprint arXiv:2406.00799 (2024).
- [6] Divyansh Agarwal, Alexander R Fabbri, Philippe Laban, Shafiq Joty, Caiming Xiong, and Chien-Sheng Wu. 2024. Investigating the prompt leakage effect and black-box defenses for multi-turn LLM interactions. arXiv preprint arXiv:2404.16251 (2024).
- Vedika Agarwal, Rakshith Shetty, and Mario Fritz. 2020. Towards causal vqa: Revealing and reducing spurious correlations by invariant and covariant semantic editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 9690-9698
- [8] Hojjat Aghakhani, Wei Dai, Andre Manoel, Xavier Fernandes, Anant Kharkar, Christopher Kruegel, Giovanni Vigna, David Evans, Ben Zorn, and Robert Sim. 2023. TrojanPuzzle: Covertly Poisoning Code-Suggestion Models. arxiv preprint arxiv:2301.02344 (2023).
- [9] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et al. 2022. Do as i can, not as i say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691 (2022).
- [10] Gabriel Alon and Michael Kamfonas. 2023. Detecting language model attacks with perplexity. arXiv preprint arXiv:2308.14132 (2023).
- [11] Chantal Amrhein, Florian Schottmann, Rico Sennrich, and Samuel Läubli. 2023. Exploiting Biased Models to De-bias Text: A Gender-Fair Rewriting Model. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 4486-4506.
- [12] Cem Anil, Esin Durmus, Mrinank Sharma, Joe Benton, Sandipan Kundu, Joshua Batson, Nina Rimsky, Meg Tong, Jesse Mu, Daniel Ford, et al. 2024. Many-shot jailbreaking. Anthropic, April (2024).
- [13] Anish Athalye, Nicholas Carlini, and David Wagner. 2018. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. In International conference on machine learning. PMLR, 274-283.
- [14] Ricardo Baeza-Yates. 2018. Bias on the web. Commun. ACM 61, 6 (2018), 54-61.
- [15] Eugene Bagdasaryan and Vitaly Shmatikov. 2022. Spinning Language Models: Risks of Propaganda-As-A-Service and Countermeasures. arXiv preprint arXiv:2112.05224 (2022).
- [16] Eugene Bagdasaryan, Ren Yi, Sahra Ghalebikesabi, Peter Kairouz, Marco Gruteser, Sewoong Oh, Borja Balle, and Daniel Ramage. 2024. Air Gap: Protecting Privacy-Conscious Conversational Agents. arXiv preprint arXiv:2405.05175 (2024).
- [17] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. 2022. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862 (2022).
- [18] Rongzhou Bao, Jiayi Wang, and Hai Zhao. 2021. Defending pre-trained language models from adversarial word substitutions without performance sacrifice. arXiv preprint arXiv:2105.14553 (2021).
- [19] Soumya Barikeri, Anne Lauscher, Ivan Vulić, and Goran Glavaš. 2021. RedditBias: A real-world resource for bias evaluation and debiasing of conversational language models. arXiv preprint arXiv:2106.03521 (2021).
- [20] Rishabh Bhardwaj and Soujanya Poria. 2023. Red-teaming large language models using chain of utterances for safety-alignment. arXiv preprint arXiv:2308.09662 (2023).
- [21] Stella Biderman, Usvsn Prashanth, Lintang Sutawika, Hailey Schoelkopf, Quentin Anthony, Shivanshu Purohit, and Edward Raff. 2024. Emergent and predictable memorization in large language models. Advances in Neural Information Processing Systems 36 (2024).
- [22] Ali Furkan Biten, Lluís Gómez, and Dimosthenis Karatzas. 2022. Let there be a clock on the beach: Reducing object hallucination in image captioning. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 1381-1390.
- [23] Lucas Bourtoule, Varun Chandrasekaran, Christopher A Choquette-Choo, Hengrui Jia, Adelin Travers, Baiwu Zhang, David Lie, and Nicolas Papernot. 2021. Machine unlearning. In 2021 IEEE Symposium on Security and Privacy (SP). IEEE, 141-159.
- [24] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877-1901.
- [25] Jacob Buckman, Aurko Roy, Colin Raffel, and Ian Goodfellow, 2018. Thermometer encoding: One hot way to resist adversarial examples. In International conference on learning representations.
- [26] Joy Buolamwini and Timnit Gebru. 2018. Gender shades: Intersectional accuracy disparities in commercial gender classification. In Conference on fairness, accountability and transparency. PMLR, 77-91.
- [27] Laura Cabello, Anna Katrine Jørgensen, and Anders Søgaard. 2023. On the independence of association bias and empirical fairness in language models. In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency. 370-378.
- [28] Aylin Caliskan, Joanna J Bryson, and Arvind Narayanan. 2017. Semantics derived automatically from language corpora contain human-like biases Science 356, 6334 (2017), 183-186.
- [29] Qingxing Cao, Junhao Cheng, Xiaodan Liang, and Liang Lin. 2024. VisDiaHalBench: A Visual Dialogue Benchmark For Diagnosing Hallucination in Large Vision-Language Models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 12161-12176.
- [30] Nicholas Carlini, Steve Chien, Milad Nasr, Shuang Song, Andreas Terzis, and Florian Tramer. 2022. Membership inference attacks from first principles. In 2022 IEEE Symposium on Security and Privacy (SP). IEEE, 1897-1914.
- [31] Nicolas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tramer, Borja Balle, Daphne Ippolito, and Eric Wallace. 2023. Extracting training data from diffusion models. In 32nd USENIX Security Symposium (USENIX Security 23). 5253-5270.
- [32] Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang. 2023. Quantifying Memorization Across Neural Language Models. In The Eleventh International Conference on Learning Representations. https://openreview.net/forum?id=TatRHT\_1cK

#### Navigating the Risks: A Survey of Security, Privacy, and Ethics Threats in LLM-Based Agents

- [33] Nicholas Carlini, Matthew Jagielski, and Ilya Mironov. 2020. Cryptanalytic extraction of neural network models. In Annual international cryptology conference. Springer, 189-218.
- [34] Nicholas Carlini, Milad Nasr, Christopher A Choquette-Choo, Matthew Jagielski, Irena Gao, Pang Wei W Koh, Daphne Ippolito, Florian Tramer, and Ludwig Schmidt. 2024. Are aligned neural networks adversarially aligned? Advances in Neural Information Processing Systems 36 (2024).
- [35] Nicholas Carlini, Daniel Paleka, Krishnamurthy Dj Dvijotham, Thomas Steinke, Jonathan Hayase, A Feder Cooper, Katherine Lee, Matthew Jagielski, Milad Nasr, Arthur Conmy, et al. 2024. Stealing part of a production language model. arXiv preprint arXiv:2403.06634 (2024).
- [36] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. 2021. Extracting training data from large language models. In 30th USENIX Security Symposium (USENIX Security 21). 2633-2650.
- [37] Nicholas Carlini and David Wagner. 2017. Towards evaluating the robustness of neural networks. In 2017 ieee symposium on security and privacy  $(sp)$ . Ieee, 39-57.
- [38] Patrick Chao, Edoardo Debenedetti, Alexander Robey, Maksym Andriushchenko, Francesco Croce, Vikash Sehwag, Edgar Dobriban, Nicolas Flammarion, George J Pappas, Florian Tramer, et al. 2024. Jailbreakbench: An open robustness benchmark for jailbreaking large language models.  $arXiv$  preprint  $arXiv:2404.01318$  (2024).
- [39] Harrison Chase. 2022. LangChain. https://github.com/langchain-ai/langchain.
- [40] Satrajit Chatterjee. 2018. Learning and memorization. In International conference on machine learning. PMLR, 755-763.
- [41] Chen Chen, Daochang Liu, and Chang Xu. 2024. Towards Memorization-Free Diffusion Models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 8425-8434.
- [42] Dake Chen, Hanbin Wang, Yunhao Huo, Yuzhao Li, and Haoyang Zhang. 2023. Gamegpt: Multi-agent collaborative framework for game development. arXiv preprint arXiv:2310.08067 (2023).
- [43] Jianbo Chen, Michael I Jordan, and Martin J Wainwright. 2020. Hopskipjumpattack: A query-efficient decision-based attack. In 2020 ieee symposium on security and privacy (sp). IEEE, 1277-1294.
- [44] Kedi Chen, Qin Chen, Jie Zhou, Yishen He, and Liang He. 2024. DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models. arXiv preprint arXiv:2403.00896 (2024).
- [45] Sizhe Chen, Julien Piet, Chawin Sitawarin, and David Wagner. 2024. StruQ: Defending against prompt injection with structured queries. arXiv preprint arXiv:2402.06363 (2024).
- [46] Zhaorun Chen, Zhen Xiang, Chaowei Xiao, Dawn Song, and Bo Li. 2024. AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases. arXiv preprint arXiv:2407.12784 (2024).
- [47] Yixin Cheng, Markos Georgopoulos, Volkan Cevher, and Grigorios G Chrysos. 2024. Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks. arXiv preprint arXiv:2402.09177 (2024).
- [48] Junjie Chu, Yugeng Liu, Ziqing Yang, Xinyue Shen, Michael Backes, and Yang Zhang. 2024. Comprehensive assessment of jailbreak attacks against llms. arXiv preprint arXiv:2402.05668 (2024).
- [49] Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James Glass, and Pengcheng He. 2023. Dola: Decoding by contrasting layers improves factuality in large language models. arXiv preprint arXiv:2309.03883 (2023).
- [50] John Joon Young Chung, Ece Kamar, and Saleema Amershi. 2023. Increasing Diversity While Maintaining Accuracy: Text Data Generation with Large Language Models and Human Interventions. In The 61st Annual Meeting Of The Association For Computational Linguistics.
- [51] Stav Cohen, Ron Bitton, and Ben Nassi. 2024. Here Comes The AI Worm: Unleashing Zero-click Worms that Target GenAI-Powered Applications. arXiv preprint arXiv:2403.02817 (2024).
- [52] Chenhang Cui, Yiyang Zhou, Xinyu Yang, Shirley Wu, Linjun Zhang, James Zou, and Huaxiu Yao. 2023. Holistic analysis of hallucination in gpt-4v (ision): Bias and interference challenges.  $arXiv$  preprint  $arXiv.2311.03287$  (2023).
- [53] Tianyu Cui, Yanling Wang, Chuanpu Fu, Yong Xiao, Sijia Li, Xinhao Deng, Yunpeng Liu, Qinglin Zhang, Ziyi Qiu, Peiyang Li, et al. 2024. Risk taxonomy, mitigation, and assessment benchmarks of large language model systems. arXiv preprint arXiv:2401.05778 (2024).
- [54] Gautier Dagan, Frank Keller, and Alex Lascarides. 2023. Dynamic planning with a llm. arXiv preprint arXiv:2308.06391 (2023).
- [55] David Dale, Elena Voita, Janice Lam, Prangthip Hansanti, Christophe Ropers, Elahe Kalbassi, Cynthia Gao, Loïc Barrault, and Marta Costa-jussà. 2023. Halomi: A manually annotated benchmark for multilingual hallucination and omission detection in machine translation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. 638-653.
- [56] Brian d'Alessandro, Cathy O'Neil, and Tom LaGatta. 2017. Conscientious classification: A data scientist's guide to discrimination-aware classification. Big data 5, 2 (2017), 120-134.
- [57] David Danks and Alex John London. 2017. Algorithmic Bias in Autonomous Systems.. In Ijcai, Vol. 17. 4691-4697.
- [58] Pieter Delobelle, Ewoenam Kwaku Tokpo, Toon Calders, and Bettina Berendt. 2022. Measuring fairness with biased rulers: A comparative study on bias metrics for pre-trained language models. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics, 1693-1706.
- [59] Gelei Deng, Yi Liu, Yuekang Li, Kailong Wang, Ying Zhang, Zefeng Li, Haoyu Wang, Tianwei Zhang, and Yang Liu. 2024. Masterkey: Automated jailbreaking of large language model chatbots. In Proc. ISOC NDSS.
- [60] Yue Deng, Wenxuan Zhang, Sinno Jialin Pan, and Lidong Bing. 2023. Multilingual jailbreak challenges in large language models. arXiv preprint arXiv:2310.06474 (2023).

- [61] Zehang Deng, Yongjian Guo, Changzhou Han, Wanlun Ma, Junwu Xiong, Sheng Wen, and Yang Xiang. 2024. AI Agents Under Threat: A Survey of Key Security Challenges and Future Pathways. arXiv preprint arXiv:2406.02630 (2024).
- [62] Jacob Devlin. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018).
- [63] Harnoor Dhingra, Preetiha Jayashanker, Sayali Moghe, and Emma Strubell. 2023. Queer people are people first: Deconstructing sexual identity stereotypes in large language models. arXiv preprint arXiv:2307.00101 (2023).
- [64] Moreno D'Incà, Elia Peruzzo, Massimiliano Mancini, Dejia Xu, Vidit Goel, Xingqian Xu, Zhangyang Wang, Humphrey Shi, and Nicu Sebe. 2024. OpenBias: Open-set Bias Detection in Text-to-Image Generative Models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 12225-12235.
- [65] Tommaso Dolci, Fabio Azzalini, and Mara Tanelli. 2023. Improving gender-related fairness in sentence encoders: A semantics-based approach. Data Science and Engineering 8, 2 (2023), 177-195.
- [66] Tian Dong, Minhui Xue, Guoxing Chen, Rayne Holland, Shaofeng Li, Yan Meng, Zhen Liu, and Haojin Zhu. 2023. The Philosopher's Stone: Trojaning Plugins of Large Language Models. arXiv preprint arXiv:2312.00374 (2023).
- [67] Xinshuai Dong, Anh Tuan Luu, Min Lin, Shuicheng Yan, and Hanwang Zhang. 2021. How should pre-trained language models be fine-tuned towards adversarial robustness? Advances in Neural Information Processing Systems 34 (2021), 4356-4369.
- [68] Chengfeng Dou, Ying Zhang, Yanyuan Chen, Zhi Jin, Wenpin Jiao, Haiyan Zhao, and Yu Huang. 2024. Detection, Diagnosis, and Explanation: A Benchmark for Chinese Medial Hallucination Evaluation. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024). 4784-4794.
- [69] Moussa Koulako Bala Doumbouya, Ananjan Nandi, Gabriel Poesia, Davide Ghilardi, Anna Goldie, Federico Bianchi, Dan Jurafsky, and Christopher D Manning. 2024. h4rm3l: A Dynamic Benchmark of Composable Jailbreak Attacks for LLM Safety Assessment. arXiv preprint arXiv:2408.04811  $(2024)$
- [70] Tianyu Du, Shouling Ji, Lujia Shen, Yao Zhang, Jinfeng Li, Jie Shi, Chengfang Fang, Jianwei Yin, Raheem Beyah, and Ting Wang. 2021. Cert-RNN: Towards Certifying the Robustness of Recurrent Neural Networks, CCS 21, 2021 (2021), 15-19.
- [71] Cynthia Dwork, Aaron Roth, et al. 2014. The algorithmic foundations of differential privacy. Foundations and Trends® in Theoretical Computer Science 9, 3-4 (2014), 211-407.
- [72] Nouha Dziri, Andrea Madotto, Osmar Zaïane, and Avishek Joey Bose. 2021. Neural path hunter: Reducing hallucination in dialogue systems via path grounding. arXiv preprint arXiv:2104.08455 (2021).
- [73] Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul Prakash, Tadayoshi Kohno, and Dawn Song. 2018. Robust physical-world attacks on deep learning visual classification. In Proceedings of the IEEE conference on computer vision and pattern recognition.  $1625 - 1634$
- [74] Zahra Fatemi, Chen Xing, Wenhao Liu, and Caimming Xiong. 2023. Improving Gender Fairness of Pre-Trained Language Models without Catastrophic Forgetting. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). 1249-1262
- [75] Alessandro Favero, Luca Zancato, Matthew Trager, Siddharth Choudhary, Pramuditha Perera, Alessandro Achille, Ashwin Swaminathan, and Stefano Soatto. 2024. Multi-modal hallucination control by visual information grounding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 14303-14312.
- [76] Vitaly Feldman. 2020. Does learning require memorization? a short tale about a long tail. In Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing. 954-959.
- [77] Virginia Felkner, Ho-Chun Herbert Chang, Eugene Jang, and Jonathan May. 2023. WinoQueer: A Community-in-the-Loop Benchmark for Anti-LGBTQ+ Bias in Large Language Models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 9126-9140.
- [78] Shangbin Feng, Chan Young Park, Yuhan Liu, and Yulia Tsvetkov. 2023. From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models. In The 61st Annual Meeting Of The Association For Computational Linguistics.
- [79] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al. 2020. Codebert: A pre-trained model for programming and natural languages. *arXiv preprint arXiv:2002.08155* (2020).
- [80] Eve Fleisig, Aubrie Amstutz, Chad Atalla, Su Lin Blodgett, Hal Daumé III, Alexandra Olteanu, Emily Sheng, Dan Vann, and Hanna Wallach. 2023. FairPrism: evaluating fairness-related harms in text generation. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 6231-6251.
- [81] Isabel O Gallegos, Ryan A Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, and Nesreen K Ahmed. 2024. Bias and fairness in large language models: A survey. Computational Linguistics (2024), 1-79.
- [82] Aparna Garimella, Rada Mihalcea, and Akhash Amarnath. 2022. Demographic-aware language model fine-tuning as a bias mitigation technique. In Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 2: Short Papers). 311-319.
- [83] Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A Smith. 2020. Realtoxicityprompts: Evaluating neural toxic degeneration in language models. arXiv preprint arXiv:2009.11462 (2020).
- [84] Somayeh Ghanbarzadeh, Yan Huang, Hamid Palangi, Radames Cruz Moreno, and Hamed Khanpour. 2023. Gender-tuning: Empowering Fine-tuning for Debiasing Pre-trained Language Models. In Findings of the Association for Computational Linguistics: ACL 2023. 5448-5458.

#### Navigating the Risks: A Survey of Security, Privacy, and Ethics Threats in LLM-Based Agents

- [85] Micah Goldblum, Dimitris Tsipras, Chulin Xie, Xinyun Chen, Avi Schwarzschild, Dawn Song, Aleksander Madry, Bo Li, and Tom Goldstein. 2020. Dataset Security for Machine Learning: Data Poisoning, Backdoor Attacks, and Defenses. arXiv preprint arXiv:2012.10544 (2020).
- [86] Yichen Gong, Delong Ran, Jinyuan Liu, Conglei Wang, Tianshuo Cong, Anyu Wang, Sisi Duan, and Xiaoyun Wang. 2023. Figstep: Jailbreaking large vision-language models via typographic visual prompts. arXiv preprint arXiv:2311.05608 (2023).
- [87] Laura Graves, Vineel Nagisetty, and Vijay Ganesh. 2021. Amnesiac machine learning. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 35, 11516-11524
- [88] Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten Holz, and Mario Fritz. 2023. Not what you've signed up for: Compromising real-world llm-integrated applications with indirect prompt injection. In Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security, 79-90.
- [89] Xiangming Gu, Xiaosen Zheng, Tianyu Pang, Chao Du, Qian Liu, Ye Wang, Jing Jiang, and Min Lin. 2024. Agent smith: A single image can jailbreak one million multimodal llm agents exponentially fast. arXiv preprint arXiv:2402.08567 (2024).
- [90] Wei Guo and Aylin Caliskan. 2021. Detecting emergent intersectional biases: Contextualized word embeddings contain a distribution of human-like biases. In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society. 122-133.
- [91] Siobhan Mackenzie Hall, Fernanda Gonçalves Abrantes, Hanwen Zhu, Grace Sodunke, Aleksandar Shtedritski, and Hannah Rose Kirk. 2024. Visogender: A dataset for benchmarking gender bias in image-text pronoun resolution. Advances in Neural Information Processing Systems 36  $(2024)$
- [92] Xudong Han, Timothy Baldwin, and Trevor Cohn. 2022. Balancing out Bias: Achieving Fairness Through Balanced Training. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 11335-11350.
- [93] Xudong Han, Timothy Baldwin, and Trevor Cohn. 2022. Towards equal opportunity fairness through adversarial learning. arXiv preprint arXiv:2203.06317 (2022).
- [94] Ping He, Yifan Xia, Xuhong Zhang, and Shouling Ji. 2023. Efficient query-based attack against ML-based Android malware detection under zero knowledge setting. In Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security, 90-104.
- [95] Xuanli He, Lingjuan Lyu, Qiongkai Xu, and Lichao Sun. 2021. Model extraction and adversarial transferability, your BERT is vulnerable! arXiv preprint arXiv:2103.10013 (2021).
- [96] Xuanli He, Qiongkai Xu, Yi Zeng, Lingjuan Lyu, Fangzhao Wu, Jiwei Li, and Ruoxi Jia. 2022. Cater: Intellectual property protection on text generation apis via conditional watermarks. Advances in Neural Information Processing Systems 35 (2022), 5431-5445.
- [97] Keegan Hines, Gary Lopez, Matthew Hall, Federico Zarfati, Yonatan Zunger, and Emre Kiciman. 2024. Defending Against Indirect Prompt Injection Attacks With Spotlighting. arXiv preprint arXiv:2403.14720 (2024).
- [98] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, et al. 2023. Metagpt: Meta programming for multi-agent collaborative framework. arXiv preprint arXiv:2308.00352 (2023).
- [99] Shlomo Hoory, Amir Feder, Avichai Tendler, Sofia Erell, Alon Peled-Cohen, Itay Laish, Hootan Nakhost, Uri Stemmer, Ayelet Benjamini, Avinatan Hassidim, et al. 2021. Learning and evaluating a differentially private pre-trained language model. In Findings of the Association for Computational Linguistics: EMNLP 2021. 1178-1189.
- Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelli-[100] The White House. 2023. gence. https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthydevelopment-and-use-of-artificial-intelligence/.
- [101] Phillip Howard, Avinash Madasu, Tiep Le, Gustavo Lujan Moreno, Anahita Bhiwandiwalla, and Vasudev Lal. 2024. SocialCounterfactuals: Probing and Mitigating Intersectional Social Biases in Vision-Language Models with Counterfactual Examples. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 11975-11985.
- [102] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. LoRA: Low-Rank Adaptation of Large Language Models. arXiv preprint arXiv:2106.09685 (2021).
- [103] Zhengmian Hu, Gang Wu, Saayan Mitra, Ruiyi Zhang, Tong Sun, Heng Huang, and Vishy Swaminathan. 2023. Token-level adversarial prompt detection based on perplexity measures and contextual information. arXiv preprint arXiv:2311.11509 (2023).
- [104] Jie Huang, Hanyin Shao, and Kevin Chen Chuan Chang, 2022. Are Large Pre-Trained Language Models Leaking Your Personal Information?, In 2022 Findings of the Association for Computational Linguistics: EMNLP 2022.
- [105] Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, and Nenghai Yu. 2024. Opera: Alleviating hallucination in multi-modal large language models via over-trust penalty and retrospection-allocation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 13418-13427.
- [106] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. 2022. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In International conference on machine learning. PMLR, 9118-9147.
- [107] Yangsibo Huang, Samyak Gupta, Mengzhou Xia, Kai Li, and Danqi Chen. 2023. Catastrophic jailbreak of open-source llms via exploiting generation.  $arXiv$  preprint  $arXiv:2310.06987$  (2023).
- [108] Yangsibo Huang, Samyak Gupta, Zexuan Zhong, Kai Li, and Danqi Chen. 2023. Privacy Implications of Retrieval-Based Language Models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. 14887-14902.
- [109] Yihao Huang, Chong Wang, Xiaojun Jia, Qing Guo, Felix Juefei-Xu, Jian Zhang, Geguang Pu, and Yang Liu. 2024. Semantic-guided Prompt Organization for Universal Goal Hijacking against LLMs. arXiv preprint arXiv:2405.14189 (2024).

- [110] Yue Huang, Qihui Zhang, Lichao Sun, et al. 2023. Trustgpt: A benchmark for trustworthy and responsible large language models. arXiv preprint arXiv:2306.11507 (2023).
- [111] Evan Hubinger, Carson Denison, and Jesse Mu et al. 2024. Uhgeval: Benchmarking the hallucination of chinese large language models via unconstrained generation. arXiv preprint arXiv:2401.05566 (2024).
- [112] Bo Hui, Haolin Yuan, Neil Gong, Philippe Burlina, and Yinzhi Cao. 2024. PLeak: Prompt Leaking Attacks against Large Language Model Applications. arXiv preprint arXiv:2405.06823 (2024).
- [113] Matthew Jagielski, Nicholas Carlini, David Berthelot, Alex Kurakin, and Nicolas Papernot. 2020. High accuracy and high fidelity extraction of neural networks. In 29th USENIX security symposium (USENIX Security 20). 1345-1362.
- [114] Neel Jain, Avi Schwarzschild, Yuxin Wen, Gowthami Somepalli, John Kirchenbauer, Ping-yeh Chiang, Micah Goldblum, Aniruddha Saha, Jonas Geiping, and Tom Goldstein. 2023. Baseline defenses for adversarial attacks against aligned language models. arXiv preprint arXiv:2309.00614  $(2023)$
- [115] Joel Jang, Dongkeun Yoon, Sohee Yang, Sungmin Cha, Moontae Lee, Lajanugen Logeswaran, and Minjoon Seo. 2023. Knowledge Unlearning for Mitigating Privacy Risks in Language Models. In 61st Annual Meeting of the Association for Computational Linguistics, ACL 2023. Association for Computational Linguistics, 14389-14408.
- [116] Jiabao Ji, Bairu Hou, Alexander Robey, George J Pappas, Hamed Hassani, Yang Zhang, Eric Wong, and Shiyu Chang. 2024. Defending large language models against jailbreak attacks via semantic smoothing. arXiv preprint arXiv:2402.16192 (2024).
- [117] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of hallucination in natural language generation. Comput. Surveys 55, 12 (2023), 1–38.
- [118] Chaoya Jiang, Haiyang Xu, Mengfan Dong, Jiaxing Chen, Wei Ye, Ming Yan, Qinghao Ye, Ji Zhang, Fei Huang, and Shikun Zhang. 2024. Hallucination augmented contrastive learning for multimodal large language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 27036-27046.
- [119] Ruochen Jiao, Shaoyuan Xie, Justin Yue, Takami Sato, Lixu Wang, Yixuan Wang, Oi Alfred Chen, and Oi Zhu, 2024, Exploring Backdoor Attacks against Large Language Model-based Decision Making. arXiv preprint arXiv:2405.20774 (2024).
- [120] Mingyu Jin, Suiyuan Zhu, Beichen Wang, Zihao Zhou, Chong Zhang, Yongfeng Zhang, et al. 2024. Attackeval: How to evaluate the effectiveness of jailbreak attacking on large language models. arXiv preprint arXiv:2401.09002 (2024).
- [121] Masahiro Kaneko and Danushka Bollegala. 2022. Unmasking the mask-evaluating social biases in masked language models. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 36. 11954-11962.
- [122] Prannay Kaul, Zhizhong Li, Hao Yang, Yonatan Dukler, Ashwin Swaminathan, CJ Taylor, and Stefano Soatto. 2024. THRONE: An object-based hallucination benchmark for the free-form generations of large vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 27228-27238.
- [123] Yannik Keller, Jan Mackensen, and Steffen Eger. 2021. BERT-defense: A probabilistic model based on BERT to combat cognitively inspired orthographic adversarial attacks. arXiv preprint arXiv:2106.01452 (2021).
- [124] Jae Myung Kim, A Koepke, Cordelia Schmid, and Zeynep Akata. 2023. Exposing and mitigating spurious correlations for cross-modal retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2585-2595.
- [125] Siwon Kim, Sangdoo Yun, Hwaran Lee, Martin Gubri, Sungroh Yoon, and Seong Joon Oh. 2024. Propile: Probing privacy leakage in large language models. Advances in Neural Information Processing Systems 36 (2024).
- [126] Subaru Kimura, Ryota Tanaka, Shumpei Miyawaki, Jun Suzuki, and Keisuke Sakaguchi. 2024. Empirical Analysis of Large Vision-Language Models against Goal Hijacking via Visual Prompt Injection. arXiv preprint arXiv:2408.03554 (2024).
- [127] Myeongseob Ko, Ming Jin, Chenguang Wang, and Ruoxi Jia. 2023. Practical membership inference attacks against large-scale multi-modal models: A pilot study. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 4871-4881.
- [128] Nicholas Ka-Shing Kong. 2024. InjectBench: An Indirect Prompt Injection Benchmarking Framework. Ph. D. Dissertation. Virginia Tech.
- [129] Klara Krieg, Emilia Parada-Cabaleiro, Gertraud Medicus, Oleg Lesota, Markus Schedl, and Navid Rekabsaz. 2023. Grep-biasir: A dataset for investigating gender representation bias in information retrieval results. In Proceedings of the 2023 Conference on Human Information Interaction and Retrieval. 444-448.
- [130] Kalpesh Krishna, Gaurav Singh Tomar, Ankur P Parikh, Nicolas Papernot, and Mohit Iyyer. 2019. Thieves on sesame street! model extraction of bert-based apis. arXiv preprint arXiv:1910.12366 (2019).
- [131] Anne Lauscher, Tobias Lueken, and Goran Glavaš. 2021. Sustainable Modular Debiasing of Language Models. In Findings of the Association for Computational Linguistics: EMNLP 2021, 4782-4797.
- [132] Thai Le, Noseong Park, and Dongwon Lee. 2020. SHIELD: Defending textual neural networks against multiple black-box adversarial attacks with stochastic multi-expert patcher. arXiv preprint arXiv:2011.08908 (2020).
- [133] Katherine Lee, Orhan Firat, Ashish Agarwal, Clara Fannjiang, and David Sussillo. 2018. Hallucinations in neural machine translation. (2018).
- [134] Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. 2022. Deduplicating Training Data Makes Language Models Better. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 8424-8445.
- [135] Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, and Lidong Bing. 2024. Mitigating object hallucinations in large vision-language models through visual contrastive decoding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.

13872-13882.

- [136] Patrick Levi and Christoph P Neumann. 2024. Vocabulary Attack to Hijack Large Language Model Applications. arXiv preprint arXiv:2404.02637  $(2024).$
- [137] Mike Lewis, Yinhan Liu, Naman Goyal, Marian Ghazyininejad, Abdelrahman Mohamed, Omer Leyy, Ves Stoyanov, and Luke Zettlemover, 2019, Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461  $(2019)$
- [138] Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2023. Halueval: A large-scale hallucination evaluation benchmark for large language models. arXiv preprint arXiv:2305.11747 (2023).
- [139] Lin Li, Haoyan Guan, Jianing Qiu, and Michael Spratling. 2024. One prompt word is enough to boost adversarial robustness for pre-trained vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 24408-24419.
- [140] Linyang Li, Demin Song, and Xipeng Qiu. 2022. Text adversarial purification as defense against adversarial attacks. arXiv preprint arXiv:2203.14207  $(2022)$ .
- [141] Xuechen Li, Florian Tramer, Percy Liang, and Tatsunori Hashimoto. 2022. Large Language Models Can Be Strong Differentially Private Learners. In International Conference on Learning Representations. https://openreview.net/forum?id=bVuP3ltATMz
- [142] Yingji Li, Mengnan Du, Xin Wang, and Ying Wang. 2023. Prompt Tuning Pushes Farther, Contrastive Learning Pulls Closer: A Two-Stage Approach to Mitigate Social Biases. In 61st Annual Meeting of the Association for Computational Linguistics, ACL 2023. Association for Computational Linguistics (ACL), 14254-14267.
- [143] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. 2023. Evaluating object hallucination in large vision-language models. arXiv preprint arXiv:2305.10355 (2023).
- [144] Yifan Li, Hangyu Guo, Kun Zhou, Wayne Xin Zhao, and Ji-Rong Wen. 2024. Images are Achilles' Heel of Alignment: Exploiting Visual Vulnerabilities for Jailbreaking Multimodal Large Language Models. arXiv preprint arXiv:2403.09792 (2024).
- [145] Yige Li, Hanxun Huang, Yunhan Zhao, Xingjun Ma, and Jun Sun, 2024, BackdoorLLM: A Comprehensive Benchmark for Backdoor Attacks on Large Language Models. arXiv preprint arXiv:2408.12798 (2024).
- [146] Yufei Li, Zexin Li, Yingfan Gao, and Cong Liu. 2023. White-box multi-objective adversarial attack on dialogue generation. arXiv preprint arXiv:2305.03655 (2023).
- [147] Zongije Li, Chaozheng Wang, Pingchuan Ma, Chaowei Liu, Shuai Wang, Daoyuan Wu, Cuiyun Gao, and Yang Liu, 2024. On extracting specialized code abilities from large language models: A feasibility study. In Proceedings of the IEEE/ACM 46th International Conference on Software Engineering.  $1 - 13$ .
- [148] Zongjie Li, Chaozheng Wang, Shuai Wang, and Cuiyun Gao. 2023. Protecting intellectual property of large language model-based code generation apis via watermarks. In Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security. 2336-2350.
- [149] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. 2022. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110 (2022).
- [150] Siyuan Liang, Kuanrong Liu, Jiajun Gong, Jiawei Liang, Yuan Xun, Ee-Chien Chang, and Xiaochun Cao. 2024. Unlearning Backdoor Threats: Enhancing Backdoor Defense in Multimodal Contrastive Learning via Local Token Unlearning. arXiv preprint arXiv:2403.16257 (2024).
- [151] Siyuan Liang, Mingli Zhu, Aishan Liu, Baoyuan Wu, Xiaochun Cao, and Ee-Chien Chang. 2023. BadCLIP: Dual-Embedding Guided Backdoor Attack on Multimodal Contrastive Learning. arXiv preprint arXiv:2311.12075 (2023).
- [152] Xun Liang, Shichao Song, Simin Niu, Zhiyu Li, Feiyu Xiong, Bo Tang, Zhaohui Wy, Dawei He, Peng Cheng, Zhonghao Wang, et al. 2023. Uhgeval: Benchmarking the hallucination of chinese large language models via unconstrained generation. arXiv preprint arXiv:2311.15296 (2023).
- [153] Zi Liang, Haibo Hu, Qingqing Ye, Yaxin Xiao, and Haoyang Li. 2024. Why Are My Prompts Leaked? Unraveling Prompt Extraction Threats in Customized Large Language Models. arXiv preprint arXiv:2408.02416 (2024).
- [154] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. 2015. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971 (2015).
- [155] Tomasz Limisiewicz, David Mareček, and Tomáš Musil. 2023. Debiasing algorithm through model adaptation. arXiv preprint arXiv:2310.18913  $(2023)$ .
- [156] Stephanie Lin, Jacob Hilton, and Owain Evans. 2021. Truthfulga: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958  $(2021).$
- [157] Pierre Lison, Ildikó Pilán, David Sánchez, Montserrat Batet, and Lilja Øvrelid. 2021. Anonymisation models for text data: State of the art, challenges and future directions. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 4188-4203.
- [158] Aishan Liu, Yuguang Zhou, and Xianglong Liu et al. 2024. Compromising Embodied Agents with Contextual Backdoor Attacks. arXiv preprint arXiv:2408.02882 (2024).
- [159] Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, and Peter Stone. 2023. Llm+ p: Empowering large language models with optimal planning proficiency. arXiv preprint arXiv:2304.11477 (2023).
- [160] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. 2023. Mitigating hallucination in large multi-modal models via robust instruction tuning. In The Twelfth International Conference on Learning Representations.

- [161] Han Liu, Yuhao Wu, Shixuan Zhai, Bo Yuan, and Ning Zhang. 2023. Riatig: Reliable and imperceptible adversarial text-to-image generation with natural prompts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 20585-20594.
- [162] Jerry Liu. 2022. LlamaIndex.
- [163] Jiawei Liu, Yangyang Kang, Di Tang, Kaisong Song, Changlong Sun, Xiaofeng Wang, Wei Lu, and Xiaozhong Liu, 2022, Order-disorder: Imitation adversarial attacks for black-box neural ranking models. In Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security, 2025-2039
- [164] Qin Liu, Fei Wang, Chaowei Xiao, and Muhao Chen. 2023. From Shortcuts to Triggers: Backdoor Defense with Denoised PoE. arXiv preprint arXiv:2305.14910 (2023).
- [165] Qin Liu, Rui Zheng, Bao Rong, Jingyi Liu, Zhihua Liu, Zhanzhan Cheng, Liang Qiao, Tao Gui, Qi Zhang, and Xuan-Jing Huang. 2022. Flooding-X: Improving BERT's resistance to adversarial attacks via loss-restricted fine-tuning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 5634-5644.
- [166] Shunyu Liu, Yaoru Li, Kongcheng Zhang, Zhenyu Cui, Wenkai Fang, Yuxuan Zheng, Tongya Zheng, and Mingli Song. 2024. Odyssey: Empowering Agents with Open-World Skills. arXiv preprint arXiv:2407.15325 (2024).
- [167] Tong Liu, Yingjie Zhang, Zhe Zhao, Yinpeng Dong, Guozhu Meng, and Kai Chen. 2024. Making them ask and answer: Jailbreaking large language models in few queries via disguise and reconstruction. arXiv preprint arXiv:2402.18104 (2024).
- [168] Xiaogeng Liu, Nan Xu, Muhao Chen, and Chaowei Xiao. 2023. Autodan: Generating stealthy jailbreak prompts on aligned large language models. arXiv preprint arXiv:2310.04451 (2023).
- [169] Xiaogeng Liu, Zhiyuan Yu, Yizhe Zhang, Ning Zhang, and Chaowei Xiao. 2024. Automatic and universal prompt injection attacks against large language models. arXiv preprint arXiv:2403.04957 (2024).
- [170] X Liu, Y Zhu, J Gu, Y Lan, C Yang, and Y Qiao. 2023. Mm-safetybench: A benchmark for safety evaluation of multimodal large language models. arXiv preprint arXiv:2311.17600 (2023).
- [171] Yupei Liu, Yugi Jia, Runpeng Geng, Jinyuan Jia, and Neil Zhengiang Gong, 2024. Formalizing and benchmarking prompt injection attacks and defenses. In 33rd USENIX Security Symposium (USENIX Security 24), 1831-1847.
- [172] Dong Lu, Zhiqiang Wang, Teng Wang, Weili Guan, Hongchang Gao, and Feng Zheng. 2023. Set-level guidance attack: Boosting adversarial transferability of vision-language pre-training models. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 102-111.
- [173] Jiarui Lu, Thomas Holleis, and Yizhe Zhang et al. 2024. ToolSandbox: A Stateful, Conversational, Interactive Evaluation Benchmark for LLM Tool Use Capabilities. arXiv preprint arXiv:2408.04682 (2024).
- [174] Jiaying Lu, Jinmeng Rao, Kezhen Chen, Xiaoyuan Guo, Yawen Zhang, Baochen Sun, Carl Yang, and Jie Yang. 2024. Evaluation and enhancement of semantic grounding in large vision-language models. In AAAI-ReLM Workshop.
- [175] Nils Lukas, Ahmed Salem, Robert Sim, Shruti Tople, Lukas Wutschitz, and Santiago Zanella-Béguelin. 2023. Analyzing leakage of personally identifiable information in language models. In 2023 IEEE Symposium on Security and Privacy (SP). IEEE, 346-363.
- [176] Weidi Luo, Siyuan Ma, Xiaogeng Liu, Xiaoyu Guo, and Chaowei Xiao. 2024. Jailbreakv-28k: A benchmark for assessing the robustness of multimodal large language models against jailbreak attacks. arXiv preprint arXiv:2404.03027 (2024).
- [177] Wen Luo, Tianshu Shen, Wei Li, Guangyue Peng, Richeng Xuan, Houfeng Wang, and Xi Yang. 2024. HalluDial: A Large-Scale Benchmark for Automatic Dialogue-Level Hallucination Evaluation. arXiv preprint arXiv:2406.07070 (2024).
- [178] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. 2023. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424 (2023).
- [179] Yuhao Mao, Mark Müller, Marc Fischer, and Martin Vechev. 2024. Connecting certified and adversarial training. Advances in Neural Information Processing Systems 36 (2024).
- [180] Chandler May, Alex Wang, Shikha Bordia, Samuel R Bowman, and Rachel Rudinger. 2019. On measuring social biases in sentence encoders. arXiv preprint arXiv:1903.10561 (2019).
- [181] Medium. [n. d.]. 'Grandma Exploit': ChatGPT commanded to pretend to be a dead grandmother. https://medium.com/@med\_strongboxit/grandmaexploit-chatgpt-commanded-to-pretend-to-be-a-dead-grandmother-13ddb984715a.
- [182] Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. 2021. A survey on bias and fairness in machine learning. ACM computing surveys (CSUR) 54, 6 (2021), 1-35.
- [183] Smitha Milli, Ludwig Schmidt, Anca D Dragan, and Moritz Hardt. 2019. Model reconstruction from model explanations. In Proceedings of the Conference on Fairness, Accountability, and Transparency. 1-9.
- [184] Niloofar Mireshghallah, Hyunwoo Kim, Xuhui Zhou, Yulia Tsyetkov, Maarten Sap, Reza Shokri, and Yeiin Choi, 2024. Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory. In The Twelfth International Conference on Learning Representations
- [185] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidieland, Georg Ostrovski, et al. 2015. Human-level control through deep reinforcement learning, nature 518, 7540 (2015), 529–533.
- [186] Edoardo Mosca, Shreyash Agarwal, Javier Rando, and Georg Groh. 2022. "That Is a Suspicious Reaction!": Interpreting Logits Variation to Detect NLP Adversarial Attacks. arXiv preprint arXiv:2204.04636 (2022).
- [187] Moin Nadeem, Anna Bethke, and Siva Reddy. 2020. StereoSet: Measuring stereotypical bias in pretrained language models. arXiv preprint arXiv:2004.09456 (2020).

#### Navigating the Risks: A Survey of Security, Privacy, and Ethics Threats in LLM-Based Agents

- [188] Moin Nadeem, Anna Bethke, and Siva Reddy. 2021. StereoSet: Measuring stereotypical bias in pretrained language models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 5356-5371.
- [189] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. 2021. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332 (2021).
- [190] Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R Bowman. 2020. CrowS-Pairs: A challenge dataset for measuring social biases in masked language models. In 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020. Association for Computational Linguistics (ACL), 1953-1967.
- [191] Ali Naseh, Kalpesh Krishna, Mohit Iyyer, and Amir Houmansadr. 2023. Stealing the decoding algorithms of language models. In Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security. 1835-1849.
- [192] Milad Nasr, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski, A Feder Cooper, Daphne Ippolito, Christopher A Choquette-Choo, Eric Wallace, Florian Tramèr, and Katherine Lee. 2023. Scalable extraction of training data from (production) language models. arXiv preprint arXiv:2311.17035  $(2023)$
- [193] Seth Neel, Aaron Roth, and Saeed Sharifi-Malvajerdi. 2021. Descent-to-delete: Gradient-based methods for machine unlearning. In Algorithmic Learning Theory. PMLR, 931-962.
- [194] Thanh Tam Nguyen, Thanh Trung Huynh, Phi Le Nguyen, Alan Wee-Chung Liew, Hongzhi Yin, and Quoc Viet Hung Nguyen. 2022. A survey of machine unlearning. arXiv preprint arXiv:2209.02299 (2022).
- [195] Helen Nissenbaum. 2004. Privacy as contextual integrity. Wash. L. Rev. 79 (2004), 119.
- [196] Zhenxing Niu, Haodong Ren, Xinbo Gao, Gang Hua, and Rong Jin. 2024. Jailbreaking attack against multimodal large language model. arXiv preprint arXiv:2402.02309 (2024).
- [197] OpenAI. 2023. ChatGPT plugins.
- [198] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems 35 (2022), 27730-27744.
- [199] Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. 2016. Distillation as a defense to adversarial perturbations against deep neural networks. In 2016 IEEE symposium on security and privacy (SP). IEEE, 582-597.
- [200] Joon Sung Park, Joseph O'Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. 2023. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th annual acm symposium on user interface software and technology. 1-22.
- [201] Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thompson, Phu Mon Htut, and Samuel Bowman. 2022. BBQ: A hand-built bias benchmark for question answering. Findings of the Association for Computational Linguistics: ACL 2022 (2022).
- [202] Dario Pasquini, Martin Strohmeier, and Carmela Troncoso. 2024. Neural Exec: Learning (and Learning from) Execution Triggers for Prompt Injection Attacks. arXiv preprint arXiv:2403.03792 (2024).
- [203] Vaidehi Patil, Peter Hase, and Mohit Bansal. 2024. Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks. In The Twelfth International Conference on Learning Representations. https://openreview.net/forum?id=7erlRDoaV8
- [204] Hammond Pearce, Baleegh Ahmad, Benjamin Tan, Brendan Dolan-Gavitt, and Ramesh Karri. 2022. Asleep at the keyboard? assessing the security of github copilot's code contributions. In 2022 IEEE Symposium on Security and Privacy (SP). IEEE, 754-768.
- [205] Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, et al. 2023. Check your facts and try again: Improving large language models with external knowledge and automated feedback. arXiv preprint arXiv:2302.12813  $(2023)$
- [206] Wenjun Peng, Jingwei Yi, Fangzhao Wu, Shangxi Wu, Bin Zhu, Lingjuan Lyu, Binxing Jiao, Tong Xu, Guangzhong Sun, and Xing Xie. 2023. Are you copying my model? protecting the copyright of large language models for eaas via backdoor watermark. arXiv preprint arXiv:2305.10036 (2023).
- [207] Fábio Perez and Ian Ribeiro. 2022. Ignore previous prompt: Attack techniques for language models. arXiv preprint arXiv:2211.09527 (2022).
- [208] Julien Piet, Maha Alrashed, Chawin Sitawarin, Sizhe Chen, Zeming Wei, Elizabeth Sun, Basel Alomair, and David Wagner. 2023. Jatmo: Prompt injection defense by task-specific finetuning, arXiv preprint arXiv:2312.17673 (2023).
- [209] Peng Qi, Zehong Yan, Wynne Hsu, and Mong Li Lee. 2024. SNIFFER: Multimodal Large Language Model for Explainable Out-of-Context Misinformation Detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 13052-13062.
- [210] Xiangyu Qi, Kaixuan Huang, Ashwinee Panda, Peter Henderson, Mengdi Wang, and Prateek Mittal. 2024. Visual adversarial examples jailbreak aligned large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38, 21527-21536.
- [211] Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong Sun. 2023. Communicative agents for software development. arXiv preprint arXiv:2307.07924 6 (2023).
- [212] Chen Qian, Wei Liu, and Hongzhang Liu. 2023. ChatDev: Communicative Agents for Software Development. arXiv preprint arXiv:2307.07924  $(2023)$ .
- [213] Yao Qiang, Xiangyu Zhou, and Dongxiao Zhu. 2023. Hijacking large language models via adversarial in-context learning. arXiv preprint arXiv:2311.09948 (2023).
- [214] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog 1, 8 (2019), 9.

- [215] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250 (2016).
- [216] Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023. In-context retrievalaugmented language models. Transactions of the Association for Computational Linguistics 11 (2023), 1316–1331.
- Delong Ran, Jinyuan Liu, Yichen Gong, Jingyi Zheng, Xinlei He, Tianshuo Cong, and Anyu Wang. 2024. JailbreakEval: An Integrated Toolkit for Evaluating Jailbreak Attempts Against Large Language Models. arXiv preprint arXiv:2406.09321 (2024).
- [218] Leonardo Ranaldi, Elena Sofia Ruzzetti, Davide Venditti, Dario Onorati, and Fabio Massimo Zanzotto. 2023. A trip towards fairness: Bias and de-biasing in large language models. arXiv preprint arXiv:2305.13862 (2023).
- [219] Protection Regulation. 2016. Regulation (EU) 2016/679 of the European Parliament and of the Council. Regulation (eu) 679 (2016), 2016.
- [220] Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. 2018. Object hallucination in image captioning. arXiv preprint arXiv:1809.02156 (2018).
- [221] Savak Saha Roy, Poojitha Thota, Krishna Vamsi Naragam, and Shirin Nilizadeh, 2023. From Chatbots to PhishBots?-Preventing Phishing scams created using ChatGPT, Google Bard and Claude. arXiv preprint arXiv:2310.19181 (2023).
- [222] Yangjun Ruan, Honghua Dong, and Andrew Wang et al. 2023. Identifying the Risks of LM Agents with an LM-Emulated Sandbox. arXiv preprint arXiv:2309.15817 (2023).
- [223] Sashank Santhanam, Behnam Hedayatnia, Spandana Gella, Aishwarya Padmakumar, Seokhwan Kim, Yang Liu, and Dilek Hakkani-Tur. 2021. Rome was built in 1776: A case study on factual correctness in knowledge-grounded response generation. arXiv preprint arXiv:2110.05456 (2021).
- [224] Danielle Saunders, Rosie Sallis, and Bill Byrne. 2022. First the Worst: Finding Better Gender Translations During Beam Search. In Findings of the Association for Computational Linguistics: ACL 2022. 3814-3823.
- [225] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2024. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems 36 (2024).
- [226] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347 (2017).
- [227] Zeyang Sha and Yang Zhang. 2024. Prompt stealing attacks against large language models. arXiv preprint arXiv:2402.12959 (2024).
- [228] Adi Shamir, Isaac Canales-Martinez, Anna Hambitzer, Jorge Chavez-Saab, Francisco Rodrigez-Henriquez, and Nitin Satpute. 2023. Polynomial time cryptanalytic extraction of neural network models. arXiv preprint arXiv:2310.08708 (2023).
- [229] Reshabh K Sharma, Vinayak Gupta, and Dan Grossman. 2024. SPML: A DSL for Defending Language Models Against Prompt Attacks. arXiv preprint arXiv:2402.11755 (2024).
- [230] Erfan Shayegani, Yue Dong, and Nael Abu-Ghazaleh. 2023. Jailbreak in pieces: Compositional adversarial attacks on multi-modal language models. In The Twelfth International Conference on Learning Representations.
- [231] Lujia Shen, Shouling Ji, Xuhong Zhang, Jinfeng Li, Jing Chen, Jie Shi, Chengfang Fang, Jianwei Yin, and Ting Wang. 2021. Backdoor Pre-trained Models Can Transfer to All. arxvi preprint arxiv:2111.00197 (2021).
- [232] Lujia Shen, Yuwen Pu, Shouling Ji, Changjiang Li, Xuhong Zhang, Chunpeng Ge, and Ting Wang. 2023. Improving the robustness of transformerbased large language models with dynamic attention. *arXiv preprint arXiv:2311.17400* (2023).
- [233] Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, and Yang Zhang. 2024. "Do Anything Now": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models. In ACM SIGSAC Conference on Computer and Communications Security (CCS). ACM.
- [234] Xinyue Shen, Yiting Qu, Michael Backes, and Yang Zhang. 2024. Prompt Stealing Attacks Against {Text-to-Image} Generation Models. In 33rd USENIX Security Symposium (USENIX Security 24), 5823-5840.
- [235] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. 2024. Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face. Advances in Neural Information Processing Systems 36 (2024).
- [236] Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, and Luke Zettlemoyer. 2023. Detecting pretraining data from large language models. arXiv preprint arXiv:2310.16789 (2023).
- [237] Weijia Shi, Xiaochuang Han, Mike Lewis, Yulia Tsvetkov, Luke Zettlemoyer, and Scott Wen-tau Yih. 2023. Trusting your evidence: Hallucinate less with context-aware decoding, arXiv preprint arXiv:2305.14739 (2023).
- [238] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. 2017. Membership inference attacks against machine learning models. In 2017 IEEE symposium on security and privacy (SP). IEEE, 3-18.
- [239] Eric Michael Smith, Melissa Hall, Melanie Kambadur, Eleonora Presani, and Adina Williams. 2022. " I'm sorry to hear that": Finding New Biases in Language Models with a Holistic Descriptor Dataset. arXiv preprint arXiv:2205.09209 (2022).
- [240] Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, and Tom Goldstein. 2023. Diffusion art or digital forgery? investigating data replication in diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 6048-6058.
- [241] Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, and Tom Goldstein. 2023. Understanding and mitigating copying in diffusion models. Advances in Neural Information Processing Systems 36 (2023), 47783-47803.
- [242] Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M Sadler, Wei-Lun Chao, and Yu Su. 2023. Llm-planner: Few-shot grounded planning for embodied agents with large language models. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 2998-3009.
- [243] Bin Sun, Yitong Li, Fei Mi, Fanhu Bie, Yiwei Li, and Kan Li. 2023. Towards Fewer Hallucinations in Knowledge-Grounded Dialogue Generation via Augmentative and Contrastive Knowledge-Dialogue. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics

(Volume 2: Short Papers), 1741-1750.

- [244] Hao Sun, Zhexin Zhang, Jiawen Deng, Jiale Cheng, and Minlie Huang. 2023. Safety assessment of chinese large language models. arXiv preprint arXiv:2304.10436 (2023).
- [245] Xiongtao Sun, Deyue Zhang, Dongdong Yang, Quanchen Zou, and Hui Li. 2024. Multi-Turn Context Jailbreak Attack on Large Language Models From First Principles. arXiv preprint arXiv:2408.04686 (2024).
- [246] Yuhong Sun, Zhangyue Yin, Qipeng Guo, Jiawen Wu, Xipeng Qiu, and Hui Zhao. 2024. Benchmarking Hallucination in Large Language Models based on Unanswerable Math Word Problem. arXiv preprint arXiv:2403.03558 (2024).
- [247] Supertools. [n. d.]. Discover the best AI tools with Supertools. https://supertools.therundown.ai/ Accessed: 2024-10-28.
- [248] Harini Suresh and John V Guttag. 2019. A framework for understanding unintended consequences of machine learning. arXiv preprint arXiv:1901.10002 2, 8 (2019), 73.
- [249] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. 2013. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199 (2013).
- [250] Kunsheng Tang, Wenbo Zhou, Jie Zhang, Aishan Liu, Gelei Deng, Shuai Li, Peigui Qi, Weiming Zhang, Tianwei Zhang, and Nenghai Yu, 2024. GenderCARE: A Comprehensive Framework for Assessing and Reducing Gender Bias in Large Language Models. In Proceedings of the 2024 ACM SIGSAC Conference on Computer and Communications Security.
- [251] Xinyu Tang, Richard Shin, Huseyin A Inan, Andre Manoel, Fatemehsadat Mireshghallah, Zinan Lin, Sivakanth Gopi, Janardhan Kulkarni, and Robert Sim. 2024. Privacy-Preserving In-Context Learning with Differentially Private Few-Shot Generation. In The Twelfth International Conference on Learning Representations. https://openreview.net/forum?id=oZtt0pRnOl
- [252] Xinyu Tang, Richard Shin, Huseyin A Inan, Andre Manoel, Fatemehsadat Mireshghallah, Zinan Lin, Sivakanth Gopi, Janardhan Kulkarni, and Robert Sim. 2024. Privacy-Preserving In-Context Learning with Differentially Private Few-Shot Generation. In The Twelfth International Conference on Learning Representations.
- [253] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amiad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumva Batra, Prajiwal Bhargaya, Shruti Bhosale, et al. 2023, Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023)
- [254] Cem Uluoglakci and Tugba Taskaya Temizel. 2024. HypoTermQA: Hypothetical Terms Dataset for Benchmarking Hallucination Tendency of LLMs. arXiv preprint arXiv:2402.16211 (2024).
- [255] European Union, 2024. The EU Artificial Intelligence Act. https://artificialintelligenceact.eu/.
- [256] Berk Ustun, Yang Liu, and David Parkes. 2019. Fairness without harm: Decoupled classifiers with preference guarantees. In International Conference on Machine Learning. PMLR, 6373-6382.
- [257] Gerrit van den Burg and Chris Williams. 2021. On memorization in probabilistic deep generative models. Advances in Neural Information Processing Systems 34 (2021), 27916-27928.
- [258] Liam Van der Poel, Ryan Cotterell, and Clara Meister. 2022. Mutual information alleviates hallucinations in abstractive summarization. arXiv preprint arXiv:2210.13210 (2022).
- [259] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems 30 (2017).
- [260] Pranav Narayanan Venkit, Sanjana Gautam, Ruchi Panchanadikar, Ting-Hao Huang, and Shomir Wilson. 2023. Nationality Bias in Text Generation. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics. 116-122.
- [261] Eric Wallace, Kai Xiao, Reimar Leike, Lilian Weng, Johannes Heidecke, and Alex Beutel. 2024. The instruction hierarchy: Training llms to prioritize privileged instructions. arXiv preprint arXiv:2404.13208 (2024).
- [262] Boxin Wang, Shuohang Wang, Yu Cheng, Zhe Gan, Ruoxi Jia, Bo Li, and Jingjing Liu. 2020. Infobert: Improving robustness of language models from an information theoretic perspective. arXiv preprint arXiv:2010.02329 (2020).
- [263] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. 2023. Voyager: An Open-Ended Embodied Agent with Large Language Models. arXiv preprint arXiv:2305.16291 (2023).
- [264] Haodi Wang, Kai Dong, Zhilei Zhu, Haotong Qin, Aishan Liu, Xiaolin Fang, Jiakai Wang, and Xianglong Liu. 2024. Transferable multimodal attack on vision-language pre-training models. In 2024 IEEE Symposium on Security and Privacy (SP). IEEE Computer Society, 102-102.
- [265] Haichen Wang, Shuchao Pang, Zhigang Lu, Yihang Rao, Yongbin Zhou, and Minhui Xue. 2024. dp-promise: Differentially Private Diffusion Probabilistic Models for Image Synthesis. In USENIX.
- [266] Huanqian Wang, Yang Yue, Rui Lu, Jingxin Shi, Andrew Zhao, Shenzhi Wang, Shiji Song, and Gao Huang. 2024. Model Surgery: Modulating LLM's Behavior Via Simple Parameter Editing. arXiv preprint arXiv:2407.08770 (2024).
- [267] Junyang Wang, Yuhang Wang, Guohai Xu, Jing Zhang, Yukai Gu, Haitao Jia, Ming Yan, Ji Zhang, and Jitao Sang. 2023. An llm-free multi-dimensional benchmark for mllms hallucination evaluation. arXiv preprint arXiv:2311.07397 (2023).
- [268] Jiongxiao Wang, Junlin Wu, Muhao Chen, Yevgeniy Vorobeychik, and Chaowei Xiao. 2023. RLHFPoison: Reward Poisoning Attack for Reinforcement Learning with Human Feedback in Large Language Models. arXiv preprint arXiv:2311.09641 (2023).
- [269] Junyang Wang, Yiyang Zhou, Guohai Xu, Pengcheng Shi, Chenlin Zhao, Haiyang Xu, Qinghao Ye, Ming Yan, Ji Zhang, Jihua Zhu, et al. 2023. Evaluation and analysis of hallucination in large vision-language models. arXiv preprint arXiv:2308.15126 (2023).
- [270] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. 2024. A survey on large language model based autonomous agents. Frontiers of Computer Science 18, 6 (2024), 186345.

- [271] Mengru Wang, Ningyu Zhang, Ziwen Xu, Zekun Xi, Shumin Deng, Yunzhi Yao, Qishen Zhang, Linyi Yang, Jindong Wang, and Huajun Chen. 2024. Detoxifying Large Language Models via Knowledge Editing. arXiv preprint arXiv:2403.14472 (2024).
- [272] Yue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. 2021. Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation. arXiv preprint arXiv:2109.00859 (2021).
- [273] Zhaoyang Wang, Zhiyue Liu, Xiaopeng Zheng, Qinliang Su, and Jiahai Wang. 2023. Rmlm: A flexible defense framework for proactively mitigating word-level adversarial attacks. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).  $2757 - 2774$
- [274] Lauren Watson, Chuan Guo, Graham Cormode, and Alexandre Sablayrolles, 2022. On the Importance of Difficulty Calibration in Membership Inference Attacks. In International Conference on Learning Representations. https://openreview.net/forum?id=3eIrli0TwQ
- [275] Kellie Webster, Xuezhi Wang, Ian Tenney, Alex Beutel, Emily Pitler, Ellie Pavlick, Jilin Chen, Ed Chi, and Slav Petrov. 2020. Measuring and reducing gendered correlations in pre-trained models. arXiv preprint arXiv:2010.06032 (2020).
- [276] Cheng'an Wei, Kai Chen, Yue Zhao, Yujia Gong, Lu Xiang, and Shenchen Zhu. 2024. Context Injection Attacks on Large Language Models. arXiv preprint arXiv:2405.20234 (2024).
- [277] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. arxvi preprint arxiv:2201.11903 (2022)
- [278] Yuxin Wen, Yuchen Liu, Chen Chen, and Lingjuan Lyu. 2024. Detecting, explaining, and mitigating memorization in diffusion models. In The Twelfth International Conference on Learning Representations.
- [279] David E Wilkins. 2014. Practical planning: extending the classical AI planning paradigm. Elsevier.
- [280] Yotam Wolf, Noam Wies, Oshri Avnery, Yoav Levine, and Amnon Shashua. 2023. Fundamental limitations of alignment in large language models. arXiv preprint arXiv:2304.11082 (2023).
- [281] Jian Wu, Yashesh Gaur, Zhuo Chen, Long Zhou, Yimeng Zhu, Tianrui Wang, Jinyu Li, Shujie Liu, Bo Ren, Linquan Liu, et al. 2023. On decoder-only architecture for speech-to-text and large language model integration. In 2023 IEEE Automatic Speech Recognition and Understanding Workshop  $(ASRU)$ , IEEE,  $1-8$ .
- [282] Jinge Wu, Yunsoo Kim, and Honghan Wu. 2024. Hallucination benchmark in medical visual question answering. arXiv preprint arXiv:2401.05827  $(2024)$
- [283] Tong Wu, Ashwinee Panda, Jiachen T. Wang, and Prateek Mittal. 2024. Privacy-Preserving In-Context Learning for Large Language Models. In The Twelfth International Conference on Learning Representations. https://openreview.net/forum?id=x4OPJ7lHVU
- [284] Xiyang Wu, Tianrui Guan, Dianqi Li, Shuaiyi Huang, Xiaoyu Liu, Xijun Wang, Ruiqi Xian, Abhinav Shrivastava, Furong Huang, Jordan Lee Boyd-Graber, et al. 2024. AUTOHALLUSION: Automatic Generation of Hallucination Benchmarks for Vision-Language Models. arXiv preprint arXiv:2406.10900 (2024).
- [285] Xian Wu, Wenbo Guo, Hua Wei, and Xinyu Xing. 2021. Adversarial policy training against deep reinforcement learning. In 30th USENIX Security Symposium (USENIX Security 21), 1883-1900.
- [286] Chong Xiang, Tong Wu, Zexuan Zhong, David Wagner, Danqi Chen, and Prateek Mittal. 2024. Certifiably Robust RAG against Retrieval Corruption. arXiv preprint arXiv:2405.15556 (2024).
- [287] Zhongbin Xie and Thomas Lukasiewicz. 2023. An empirical analysis of parameter-efficient methods for debiasing pre-trained language models.  $arXiv$  preprint  $arXiv:2306.04067$  (2023).
- [288] Qiongkai Xu, Xuanli He, Lingjuan Lyu, Lizhen Qu, and Gholamreza Haffari. 2021. Student surpasses teacher: Imitation attack for black-box NLP APIs. arXiv preprint arXiv:2108.13873 (2021).
- [289] Zhangchen Xu, Fengqing Jiang, Luyao Niu, Jinyuan Jia, Bill Yuchen Lin, and Radha Poovendran. 2024. Safedecoding: Defending against jailbreak attacks via safety-aware decoding. arXiv preprint arXiv:2402.08983 (2024).
- [290] Zhao Xu, Fan Liu, and Hao Liu. 2024. Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs. arXiv preprint arXiv:2406.09324 (2024).
- [291] Ke Yang, Charles Yu, Yi R Fung, Manling Li, and Heng Ji. 2023. Adept: A debiasing prompt framework. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 37, 10780-10788.
- [292] Wenkai Yang, Xiaohan Bi, Yankai Lin, Sishuo Chen, Jie Zhou, and Xu Sun. 2024. Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents. arXiv preprint arXiv:2402.11208 (2024).
- [293] Yijun Yang, Ruiyuan Gao, Xiao Yang, Jianyuan Zhong, and Qiang Xu. 2024. GuardT2I: Defending Text-to-Image Models from Adversarial Prompts. arXiv preprint arXiv:2403.01446 (2024).
- [294] Yong Yang, Xuhong Zhang, Yi Jiang, Xi Chen, Haoyu Wang, Shouling Ji, and Zonghui Wang. 2024. PRSA: Prompt reverse stealing attacks against large language models. arXiv preprint arXiv:2402.19200 (2024).
- [295] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. 2023. Mm-react: Prompting chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381 (2023).
- [296] Hongwei Yao, Jian Lou, Kui Ren, and Zhan Qin. 2023. Promptcare: Prompt copyright protection by watermark injection and verification. arXiv preprint arXiv:2308.02816 (2023).
- [297] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2024. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems 36 (2024).

#### Navigating the Risks: A Survey of Security, Privacy, and Ethics Threats in LLM-Based Agents

- [298] Jingwei Yi, Yueqi Xie, Bin Zhu, Keegan Hines, Emre Kiciman, Guangzhong Sun, Xing Xie, and Fangzhao Wu. 2023. Benchmarking and defending against indirect prompt injection attacks on large language models. arXiv preprint arXiv:2312.14197 (2023).
- [299] Ziyi Yin, Muchao Ye, Tianrong Zhang, Tianyu Du, Jinguo Zhu, Han Liu, Jinghui Chen, Ting Wang, and Fenglong Ma. 2024. Vlattack: Multimodal adversarial attacks on vision-language tasks via pre-trained models. Advances in Neural Information Processing Systems 36 (2024).
- [300] Charles Yu, Sullam Jeoung, Anish Kasi, Pengfei Yu, and Heng Ji. 2023. Unlearning bias in language models by partitioning gradients. In Findings of the Association for Computational Linguistics: ACL 2023. 6032-6048.
- [301] Da Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi, Huseyin A Inan, Gautam Kamath, Janardhan Kulkarni, Yin Tat Lee, Andre Manoel, Lukas Wutschitz, Sergey Yekhanin, and Huishuai Zhang. 2022. Differentially Private Fine-tuning of Language Models. In International Conference on Learning Representations. https://openreview.net/forum?id=Q42f0dfjECO
- [302] Jiahao Yu, Xingwei Lin, and Xinyu Xing. 2023. Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts. arXiv preprint arXiv:2309.10253 (2023).
- [303] Jiahao Yu, Yuhang Wu, Dong Shu, Mingyu Jin, and Xinyu Xing. 2023. Assessing prompt injection risks in 200+ custom gpts. arXiv preprint arXiv:2311.11538 (2023).
- [304] Qifan Yu, Juncheng Li, Longhui Wei, Liang Pang, Wentao Ye, Bosheng Qin, Siliang Tang, Qi Tian, and Yueting Zhuang. 2024. Hallucidoctor: Mitigating hallucinatory toxicity in visual instruction data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 12944-12953
- [305] Weichen Yu, Tianyu Pang, Qian Liu, Chao Du, Bingyi Kang, Yan Huang, Min Lin, and Shuicheng Yan. 2023. Bag of tricks for training data extraction from language models. In International Conference on Machine Learning. PMLR, 40306-40320.
- [306] Xiang Yue, Minxin Du, Tianhao Wang, Yaliang Li, Huan Sun, and Sherman SM Chow. 2021. Differential Privacy for Text Analytics via Natural Text Sanitization. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. 3853-3866.
- [307] Xiang Yue, Huseyin Inan, Xuechen Li, Girish Kumar, Julia McAnallen, Hoda Shajari, Huan Sun, David Levitan, and Robert Sim. 2023. Synthetic Text Generation with Differential Privacy: A Simple and Practical Recipe. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 1321-1342.
- [308] Santiago Zanella-Beguelin, Shruti Tople, Andrew Paverd, and Boris Köpf. 2021. Grey-box extraction of natural language models. In International Conference on Machine Learning. PMLR, 12278-12286.
- [309] Abdelrahman Zaved, Prasanna Parthasarathi, Goncalo Mordido, Hamid Palangi, Samira Shabanian, and Sarath Chandar, 2023, Deep learning on a healthy data diet: Finding important examples for fairness. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 37. 14593-14601.
- [310] Qingbin Zeng, Qinglong Yang, Shunan Dong, Heming Du, Liang Zheng, Fengli Xu, and Yong Li. 2024. Perceive, Reflect, and Plan: Designing LLM Agent for Goal-Directed City Navigation without Instructions. arXiv preprint arXiv:2408.04168 (2024).
- [311] Shenglai Zeng, Jiankun Zhang, Pengfei He, Jie Ren, Tianqi Zheng, Hanqing Lu, Han Xu, Hui Liu, Yue Xing, and Jiliang Tang. 2024. Mitigating the Privacy Issues in Retrieval-Augmented Generation (RAG) via Pure Synthetic Data. arXiv preprint arXiv:2406.14773 (2024).
- [312] Shenglai Zeng, Jiankun Zhang, Pengfei He, Yue Xing, Yiding Liu, Han Xu, Jie Ren, Shuaiqiang Wang, Dawei Yin, Yi Chang, et al. 2024. The good and the bad: Exploring privacy issues in retrieval-augmented generation (rag). arXiv preprint arXiv:2402.16893 (2024).
- [313] Yuheng Zha, Yichi Yang, Ruichen Li, and Zhiting Hu. 2023. AlignScore: Evaluating factual consistency with a unified alignment function. arXiv preprint arXiv:2305.16739 (2023).
- [314] Qiusi Zhan, Zhixiang Liang, Zifan Ying, and Daniel Kang. 2024. Injecagent: Benchmarking indirect prompt injections in tool-integrated large language model agents. arXiv preprint arXiv:2403.02691 (2024).
- [315] Collin Zhang, John X Morris, and Vitaly Shmatikov. 2024. Extracting Prompts by Inverting LLM Outputs. arXiv preprint arXiv:2405.15012 (2024).
- [316] Hanrong Zhang, Jingyuan Huang, Kai Mei, Yifei Yao, Zhenting Wang, Chenlu Zhan, Hongwei Wang, and Yongfeng Zhang. 2024. Agent Security Bench (ASB): Formalizing and Benchmarking Attacks and Defenses in LLM-based Agents. arXiv preprint arXiv:2410.02644 (2024).
- [317] Kechi Zhang, Jia Li, Ge Li, Xianjie Shi, and Zhi Jin. 2024. CodeAgent: Enhancing Code Generation with Tool-Integrated Agent Systems for Real-World Repo-level Coding Challenges. arXiv preprint arXiv:2401.07339 (2024).
- [318] Lu Zhang, Yongkai Wu, and Xintao Wu. 2016. A causal framework for discovering and removing direct and indirect discrimination. arXiv preprint arXiv:1611.07509 (2016).
- [319] Muru Zhang, Ofir Press, William Merrill, Alisa Liu, and Noah A Smith. 2023. How language model hallucinations can snowball. arXiv preprint arXiv:2305.13534 (2023).
- [320] Rui Zhang, Hongwei Li, Rui Wen, Wenbo Jiang, Yuan Zhang, Michael Backes, Yun Shen, and Yang Zhang. 2024. RLHFPoison: Reward Poisoning Attack for Reinforcement Learning with Human Feedback in Large Language Models. arXiv preprint arXiv:2402.09179 (2024).
- [321] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 (2022).
- [322] Xinyu Zhang, Huiyu Xu, Zhongjie Ba, Zhibo Wang, Yuan Hong, Jian Liu, Zhan Qin, and Kui Ren. 2024. Privacyasst: Safeguarding user privacy in tool-using large language model agents. JEEE Transactions on Dependable and Secure Computing (2024).
- [323] Yiming Zhang, Nicholas Carlini, and Daphne Ippolito. 2024. Effective prompt extraction from language models. arXiv preprint arXiv:2307.06865  $(2024)$ .
- [324] Yuxiang Zhang, Jing Chen, Junjie Wang, Yaxin Liu, Cheng Yang, Chufan Shi, Xinyu Zhu, Zihao Lin, Hanwen Wan, Yujiu Yang, et al. 2024. ToolBeHonest: A Multi-level Hallucination Diagnostic Benchmark for Tool-Augmented Large Language Models. arXiv preprint arXiv:2406.20015

 $(2024)$ 

- [325] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. 2023. Siren's song in the AI ocean: a survey on hallucination in large language models. arXiv preprint arXiv:2309.01219 (2023).
- [326] Yichi Zhang, Jiavi Pan, Yuchen Zhou, Rui Pan, and Joyce Chai, 2023. Grounding Visual Illusions in Language: Do Vision-Language Models Perceive Illusions Like Humans?. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. 5718-5728.
- [327] Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, and Ji-Rong Wen. 2024. A Survey on the Memory Mechanism of Large Language Model based Agents. arXiv preprint arXiv:2404.13501 (2024).
- [328] Zhuo Zhang, Guangyu Shen, Guanhong Tao, Siyuan Cheng, and Xiangyu Zhang. 2024. On large language models' resilience to coercive interrogation. In 2024 IEEE Symposium on Security and Privacy (SP). IEEE Computer Society, 252-252.
- [329] Xuandong Zhao, Yu-Xiang Wang, and Lei Li. 2023. Protecting language generation models via invisible watermarking. In International Conference on Machine Learning. PMLR, 42187-42199.
- [330] Zhiyuan Zhao, Bin Wang, Linke Ouyang, Xiaoyi Dong, Jiaqi Wang, and Conghui He. 2023. Beyond hallucinations: Enhancing lvlms through hallucination-aware direct preference optimization. arXiv preprint arXiv:2311.16839 (2023).
- [331] Kening Zheng, Junkai Chen, Yibo Yan, Xin Zou, and Xuming Hu. 2024. Reefknot: A comprehensive benchmark for relation hallucination evaluation, analysis and mitigation in multimodal large language models. arXiv preprint arXiv:2408.09429 (2024).
- [332] Zi'ou Zheng and Xiaodan Zhu. 2023. NatLogAttack: A Framework for Attacking Natural Language Inference Models with Natural Logic. arXiv preprint arXiv:2307.02849 (2023).
- [333] Chunting Zhou, Graham Neubig, Jiatao Gu, Mona Diab, Paco Guzman, Luke Zettlemoyer, and Marjan Ghazvininejad. 2020. Detecting hallucinated content in conditional neural sequence generation. arXiv preprint arXiv:2011.02593 (2020).
- [334] Kankan Zhou, Yibin LAI, and Jing Jiang. 2022. Vlstereoset: A study of stereotypical bias in pre-trained vision-language models. Association for Computational Linguistics.
- [335] Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, and Huaxiu Yao. 2023. Analyzing and mitigating object hallucination in large vision-language models. arXiv preprint arXiv:2310.00754 (2023).
- [336] Yi Zhou, Xiaoqing Zheng, Cho-Jui Hsieh, Kai-Wei Chang, and Xuanjing Huan. 2021. Defense against synonym substitution-based adversarial attacks via Dirichlet neighborhood ensemble. In Association for Computational Linguistics (ACL).
- [337] Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, J Zico Kolter, and Matt Fredrikson, 2023, Universal and transferable adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043 (2023).
- [338] Wei Zou, Runpeng Geng, Binghui Wang, and Jinyuan Jia. 2024. PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models. arXiv preprint arXiv:2402.07867 (2024).

Received 5 November 2024