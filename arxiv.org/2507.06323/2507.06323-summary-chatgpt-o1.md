Overview  
This paper examines how different large language model (LLM) agent deployment architectures—specifically Function Calling and the Model Context Protocol (MCP)—affect security vulnerabilities across both AI-specific (e.g., prompt injection) and traditional software domains (e.g., JSON injection, denial-of-service). As the authors state, “architectural choices fundamentally reshape threat landscapes” (p. 2). The study’s central goal is to unify AI-centric and software-centric security perspectives, offering a new threat classification framework applicable to both LLM-specific exploits and traditional software vulnerabilities.  

Novel Concepts & Deep Insights  
1. Cross-Domain Security Classification. The paper integrates a “unified threat classification framework” that captures nine categories of attacks, ranging from LLM-centric threats like “prompt injection” (T1, T2) to software-based vectors such as “unauthorized action execution” (T3, T4) (see Table 1, p. 11).  
2. Function Calling vs. MCP Paradigms. Two architectural paradigms are studied: (1) the centralized Function Calling approach, which embeds tool definitions in the LLM context; and (2) MCP, using a client-server model with explicit communication boundaries. The authors note, “While Function Calling simplifies integration, it concentrates risk within the core orchestration layer” (p. 17).  
3. Attack Progression Model. The paper formalizes how simple, composed, and chained attacks escalate vulnerability. “Chained attacks achieved 91–96% success rates across all configurations” (p. 25), underscoring how multi-step exploits bypass single-layer defenses.  

Methodology & Key Findings  
1. Methodology. A total of 3,250 attack scenarios were generated and executed, spanning seven language models across different providers. The testing framework employed a dual strategy: “CIA triad-centric testing” and “LLM-driven adversarial testing” (p. 14) to capture both conventional security weaknesses and sophisticated AI-targeted exploits.  
2. Key Security Outcomes.  
   • Function Calling showed higher overall attack success rate (“73.5% vs 62.59% for MCP,” p. 21). Specifically, it was more prone to “system-centric vulnerabilities” like JSON injection.  
   • MCP was less susceptible to direct function exploits but more vulnerable to “LLM-centric exposure” (68.28% ASR), linked to its “contextual communication protocol” (p. 22).  
   • Model intelligence paradox: “advanced reasoning models demonstrated higher exploitability despite better threat detection” (p. 23). In other words, while they often caught simple attempts, once compromised, they were more manipulable.  
   • Attack complexity drastically increases risk. Simple, single-vector attacks often failed or were detected about half of the time, but “chained attacks” nearly always succeeded, achieving over 90% success (Figure 13, p. 25).  

Future Predictions & Implications  
1. Growing Attack Complexity. The paper predicts “continued escalation in multi-stage and cross-domain attacks” (p. 30), urging that standard pen-testing must expand to account for chained AI/software exploits.  
2. Architectural Choices Drive Security Policy. The authors recommend “security-aware architectural design” (p. 31) during early development, highlighting that AI system integrators must weigh trade-offs of centralized vs. distributed orchestration when handling sensitive data.  
3. Potential Areas for Further Investigation. Possible future directions include “research into hybrid architectural approaches that combine the security benefits of different paradigms” (p. 36), and real-world deployment strategies for domain-specific agents (e.g., healthcare, finance).  

Critical Analysis (Optional but Recommended)  
Overall, this paper fills an important gap by formally connecting AI-specific threats with classical software vulnerabilities. Its strength lies in showing how “comprehensive assessment that spans both AI and software vectors is crucial” (p. 3). However, the authors acknowledge limitations in not examining emerging multi-agent protocols, such as “Google’s A2A Protocol” or “Microsoft’s multi-agent orchestration” (p. 37), which could introduce more complex real-world exploit paths. Despite these constraints, the paper’s core experimental results confirm that purely AI-centric or software-centric security solutions are insufficient; robust protections must address both layers in tandem.