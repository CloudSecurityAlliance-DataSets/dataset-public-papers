**Summary of “Beyond the Protocol: Unveiling Attack Vectors in the Model Context Protocol Ecosystem”**

- **Context & Motivation**  
  - The Model Context Protocol (MCP) [1] standardizes how Large Language Model (LLM)-based applications (hosts/clients) connect to external tools (servers).  
  - Its rapid adoption has led to thousands of MCP services [2], [7], [8] spanning essential functionalities (e.g., financial operations [4], software development [5]).  
  - However, the client-server structure introduces a new attack surface, where malicious MCP servers can exploit LLM agents by injecting harmful instructions, resources, or dependencies.

---

### 1. Threat Model & Attack Vectors

- **Threat Model**  
  - Adversary controls a malicious MCP server aiming to breach confidentiality, integrity, or availability of the LLM agent.  
  - LLMs inherently trust tool descriptions and outputs from MCP servers. This trust boundary creates risks when tool disinformation or hidden prompts appear legitimate.

- **Four Attack Categories**  
  1. **Tool Poisoning Attack**  
     - Malicious instructions are hidden in the server’s tool descriptions or responses.  
     - Users see benign, simplified descriptions, while LLM sees the full prompt injection.  
     - Example: A “search” tool quietly steals user API keys (Section III-B1, Fig. 2).
  2. **Puppet Attack**  
     - Multiple MCP servers are installed. A malicious server (Tool B) manipulates a legitimate server (Tool A).  
     - Through subtle prompt injections in Tool B’s description, the LLM is tricked into invoking harmful actions intended for Tool A (Section III-B2, Fig. 3).
  3. **Rug Pull Attack**  
     - The MCP server initially appears benign to gain trust.  
     - Later, the attacker updates the server’s tool description/code via package repositories (e.g., npm [12], uv [11]) to embed malicious instructions.  
     - Similar injection mechanics to Tool Poisoning once the malicious update is pulled (Section III-B3, Fig. 4).
  4. **Exploitation via Malicious External Resources**  
     - The MCP server’s descriptions/code are benign, but it relies on third-party resources that embed hidden instructions.  
     - On invocation, the LLM retrieves a poisoned external URL or API, triggering chained malicious actions (Section III-B4, Fig. 5).

---

### 2. Empirical Evaluation

1. **Malicious MCP Server Upload (RQ1)**  
   - A handcrafted “weather” MCP server containing explicit malicious code and documentation was successfully uploaded to three large MCP aggregators (Smithery.ai [2], MCP.so [7], Glama [8]) with no audits or warnings (Section IV-A).  
   - Indicates insufficient vetting/auditing by popular platforms despite security labels.

2. **User Study on Identifying Malicious Servers (RQ2)**  
   - Conducted a simulation with 20 participants (developers, security practitioners, researchers, students).  
   - Participants were asked to select servers from a simulated aggregator containing 13 servers, 4 of which were malicious.  
   - Key findings (Section IV-B):  
     - 75% unknowingly chose at least one malicious server.  
     - Even after being told some servers were malicious, only 1 participant detected all 4.  
     - Rug Pull attacks were the hardest to identify.  
     - Many participants were unfamiliar with new MCP-specific exploits.

3. **Real-World Attack Implementation (RQ3)**  
   - Tested three attack types (Tool Poisoning, Puppet, External Resource) on five mainstream LLMs: Claude 3.7 Sonnet, GPT-40, DeepSeek-V3 0324, LLaMA3.1-70B, and Gemini 2.5 Pro (Section IV-C).  
   - **Attack Success Rate (ASR)** of ~66% overall. Exploitation via Malicious External Resources reached >80% in some cases, the highest among the tested vectors.  
   - **Refusal Rate (RR)** of the LLMs was typically low (below 23% for most), indicating weak internal protections against malicious instructions.  
   - Models with stronger “tool-using” capabilities (e.g., DeepSeek-V3 0324, Gemini 2.5 Pro) showed higher ASRs.

---

### 3. Key Observations & Implications

- **User Unfamiliarity & Security Fatigue**  
  - Participants lacked awareness of nuanced prompt-based attacks.  
  - Frequent permission prompts in MCP clients lead to “alert fatigue” and uncritical tool installation.  
- **Aggregator Responsibility**  
  - Audits are minimal or absent on major platforms, creating a trust vacuum.  
  - Users often over-rely on aggregator “verified” labels without verification of malicious code.  
- **LLM Trust Paradox**  
  - LLMs must rely on external tools for advanced tasks, yet they are poor at detecting malicious or hidden prompts.  
  - Strengthening LLM reasoning (via RLHF or alignment [35]) could bolster resistance to covert instructions.

---

### 4. Proposed Mitigations

1. **Security Gateways**  
   - Place gateways at key interaction paths (e.g., path ② for capability registration, path ④ for prompt assembly).  
   - Filter potential hidden prompts or suspicious code/requests through integrated scanning in MCP clients (Section V-B).
2. **Integrity Mechanisms**  
   - For Rug Pull attacks, require cryptographic signatures of code changes and server descriptors.  
   - Enforce version checks on package repositories to detect unexpected malicious updates.
3. **LLM-Centric Hardening**  
   - Enhance refusal consistency with improved alignment or fine-tuning to spot malicious instructions.  
   - Provide robust red-teaming tests (e.g., BIPIA [46], WASP [47]) for both direct and indirect prompt injections.
4. **Stricter Aggregator Audits**  
   - Automated code static analysis for suspicious patterns.  
   - Clear accountability frameworks to manage malicious listing, possibly including platform liability.

---

**References**  
- [1] Anthropic, “Model Context Protocol,” 2025.  
- [2] Smithery.ai, “Smithery Model Context Protocol Registry,” 2025.  
- [4] base, “Base-mcp,” 2025.  
- [5] Microsoft, “Playwright-mcp,” 2025.  
- [7] mcp.so, “MCP Servers,” 2025.  
- [8] Glama, “Mcp servers glama,” 2025.  
- [11] uv, “Using tools | uv,” 2025.  
- [12] npm, “Npx | npm Docs,” 2025.  
- [35] J. Chen et al., “RMCBench: Benchmarking Large Language Models’ Resistance to Malicious Code,” ASE ‘24.