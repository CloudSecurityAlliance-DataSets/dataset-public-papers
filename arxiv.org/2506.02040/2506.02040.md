# Beyond the Protocol: Unveiling Attack Vectors in the Model Context Protocol Ecosystem

Hao Song<sup>†||</sup>, Yiming Shen<sup>‡||</sup>, Wenxuan Luo<sup>§</sup>, Leixin Guo<sup>§</sup>, Ting Chen<sup>§\*</sup>,

Jiashui Wang <sup>T††</sup> Beibei Li<sup>†</sup>, Xiaosong Zhang<sup>§</sup>, Jiachi Chen<sup>‡\*</sup>,

<sup>†</sup>Sichuan University, Chengdu, China

<sup>‡</sup>Sun Yat-sen University, Zhuhai, China

<sup>§</sup>University of Electronic Science and Technology of China, Chengdu, China

<sup>¶</sup>Zhejiang University, Hangzhou, China

<sup>††</sup>Ant Group, Hangzhou, China

\*Corresponding Author, Contributed equally

ttdelbert@foxmail.com, shenym7@mail2.sysu.edu.cn, luowx2000@outlook.com

202422081427@std.uestc.edu.cn, brokendragon@uestc.edu.cn, 12221251@zju.edu.cn

libeibei@scu.edu.cn, johnsonzxs@usetc.edu.cn, chenjch86@mail.sysu.edu.cn

*Abstract*—The Model Context Protocol (MCP) is an emerging standard designed to enable seamless interaction between Large Language Model (LLM) applications and external tools or resources. Within a short period, thousands of MCP services have already been developed and deployed. However, the clientserver integration architecture inherent in MCP may expand the attack surface against LLM Agent systems, introducing new vulnerabilities that allow attackers to exploit by designing malicious MCP servers.

In this paper, we present the first systematic study of attack vectors targeting the MCP ecosystem. Our analysis identifies four categories of attacks, i.e., Tool Poisoning Attacks, Puppet Attacks, Rug Pull Attacks, and Exploitation via Malicious External Resources. To evaluate the feasibility of these attacks, we conduct experiments following the typical steps of launching an attack through malicious MCP servers: upload  $\rightarrow$  download  $\rightarrow$ attack. Specifically, we first construct malicious MCP servers and successfully upload them to three widely used MCP aggregation platforms. The results indicate that current audit mechanisms are insufficient to identify and prevent the proposed attack methods. Next, through a user study and interview with 20 participants, we demonstrate that users struggle to identify malicious MCP servers and often unknowingly install them from aggregator platforms. Finally, we demonstrate that these attacks can trigger harmful behaviors within the user's local environment-such as accessing private files or controlling devices to transfer digital assets—by deploying a proof-of-concept (PoC) framework against five leading LLMs. Additionally, based on interview results, we discuss four key challenges faced by the current security ecosystem surrounding MCP servers. These findings underscore the urgent need for robust security mechanisms to defend against malicious MCP servers and ensure the safe deployment of increasingly autonomous LLM agents.

Ethical Statement: The malicious MCP server, used for testing aggregator platform audit mechanisms, contained no actual malicious functionality. Its documentation clearly disclosed this. No users or real-world systems were affected. We removed it from all tested platforms after the study. The interviews in this paper adhere to IRB guidelines, ensuring participants consent to recording and that transcripts are anonymized to maintain confidentiality.

## **I. INTRODUCTION**

To standardize interactions between Large Language Model (LLM) agents and diverse external resources, Anthropic introduces the Model Context Protocol (MCP) [1], a framework operating on a client-server architecture. In this model, external tools or resources are exposed via MCP servers, while the LLM application acts as the MCP host, managing clients to interact with these servers. The adoption of MCP has led to a rapidly growing ecosystem of compatible tools and services. For instance, platforms such as Smithery.ai [2] currently host over 7,000 MCP servers, providing functionalities across diverse domains, e.g., web retrieval [3], financial operations [4], and software development [5].

While MCP enhances the modularity and extensibility of agents, its reliance on external MCP servers introduces a critical trust boundary. Since these servers may be untrusted or malicious, every interaction becomes a potential security risk. The standard MCP workflow comprises three stages: 1) **Registration**, where servers describe their capabilities to the agent; 2) *Planning*, where the agent uses these descriptions for planning; and 3) **Operation**, where the agent requests tool execution from the server  $[6]$ . Each of these interactions crosses the trust boundary and presents an opportunity for adversarial exploitation.

For example, a malicious server may exploit the Registration phase by embedding hidden, harmful instructions within its tool descriptions. These instructions, potentially imperceptible to the user, may be interpreted by the LLM during the *Planning* phase, silently manipulating agent behavior without user awareness due to this information asymmetry. During the *Operation* phase, a malicious server could return manipulated responses or leverage deceptive descriptions that mislead the agent's subsequent interactions with other trusted tools, potentially leading to unintended operations.

In this paper, we present the first in-depth analysis of security risks arising from the interaction between LLM agents

and MCP servers. Based on how attackers may compromise these core MCP paths, we systematically identify and characterize four primary patterns of attack leveraged by malicious MCP servers: (1) Tool Poisoning Attack. A malicious MCP server embeds harmful or deceptive instructions within its tool descriptions or runtime responses during the registration and operation phase. These poisoned instructions mislead the LLM to return untrustworthy outputs or execute unauthorized actions. (2) Puppet Attack. In an environment with multiple MCP servers, a malicious server manipulates its tool description during the registration phase to influence the agent's planning process. The influenced agent, acting as a "puppet", may issue unintended or unauthorized actions via benign tools. (3)  $Rug$ *Pull Attack*. The malicious server initially offers a seemingly legitimate or beneficial service to gain the user's trust. After a period, the attacker alters its behavior – often by updating dependencies via third-party package hosting platforms to launch malicious attacks. (4) Exploitation via Malicious *External Resources.* The malicious MCP server redirects the agent to interact with harmful third-party services or resources located outside the MCP ecosystem.

To evaluate the feasibility of the proposed attacks, we conduct three experiments following the typical three-stage lifecycle of malicious MCP server exploitation: (1) the attacker uploads a malicious MCP server to an aggregation platform; (2) users download and install the server, and (3) the server executes harmful behaviors within the user's local environment. First, we build a malicious MCP server and test whether the server can be successfully uploaded to three widely used MCP aggregator platforms: Smithery.ai [2], MCP.so [7], and Glama [8]. In all cases, the server is accepted without any resistance, indicating that the current audit mechanisms are insufficient to identify and prevent the proposed attack methods. Then, we conduct a user study involving 20 participants by deploying a simulated MCP server aggregator platform. In this setup, we mix four malicious MCP servers with eight benign ones. Through three tasks-Blind Selection, Informed Detection, and Attack Vector Analysis (details see Section IV-B)—we find that participants consistently struggle to identify the malicious MCP servers. Follow-up interviews further reveal strong user concern regarding the overall security of MCP servers. Finally, to assess the realworld impact, we develop Proof-of-Concept (PoC) frameworks for three attack types: Tool Poisoning, Puppet Attacks, and Exploitation via Malicious External Resources. We exclude separate validation for the Rug Pull Attack, as its exploitation mechanisms overlap with the Tool Poisoning and Puppet Attack once the initial deception is established. For each attack type, we conduct 30 attempts against five leading LLMs. Our experiment results demonstrate that these attacks achieve an average attack success rate (ASR) of approximately 66%, with Exploitation via Malicious External Resources exceeding 81% ASR in several scenarios. The models' refusal rates (RR) when confronted with malicious directives remain consistently low, typically below 23%. These findings underscore the pressing need for robust defenses within the MCP ecosystem.

Additionally, based on interview results, we discuss four key challenges faced by the current security ecosystem surrounding MCP servers: (1) User unfamiliarity with MCP agent security issues, (2) User desensitization and fatigue towards security alerts, (3) Ambiguous attribution of responsibility for agent security issues, and (4) LLMs' inherent lack of ability to defend against our four defined attack vectors. A security gateway can mitigate (1) and (2). Reinforcement Learning from Human Feedback (RLHF) and prompt-centric alignment techniques can mitigate (4). MCP aggregation platforms mitigate (3) by unifying security standards. These mitigation strategies offer insights for future work.

In summary, this paper makes the following contributions.

- We present the first identifying and characterizing four primary attack patterns based on their exploitation vectors within the MCP framework.
- We empirically demonstrate the feasibility of these attacks, revealing both the inadequacy of security auditing in existing aggregator platforms and the challenges users face due to information asymmetry.
- We provide an in-depth discussion of the broader security challenges within the MCP server ecosystem. We also propose potential mitigation strategies and defense mechanisms to address these issues.
- We publish source code, details of the user study, and all experimental data at https://github.com/MCP-Security/MCP-Artifact.

#### **II. BACKGROUND**

# A. LLM Agents

Large Language Model (LLM) Agents represent an advancement over base LLMs, extending their functionality beyond text generation. LLMs serve as the backbone of agent systems, providing reasoning and language understanding capabilities [9]. Agents enhance these capabilities with mechanisms for planning, memory, and tool utilization, enabling them to accomplish complex tasks by systematically decomposing problems and leveraging external tools to access information or execute actions in real-world environments [10].

# B. Model Context Protocol (MCP)

To standardize interactions between LLM Agents and diverse external resources, Anthropic introduced the Model Context Protocol (MCP) [1], a framework operating on a client-server architecture. In this model, the LLM application acts as the host (client), while external tools or data sources are exposed via MCP servers. It defines key primitives for interaction: Resources expose data contextually, Prompts offer reusable templates, Tools enable agents to invoke actions executed remotely by the MCP server.

The typical lifecycle involves MCP servers registering their available capabilities with the host upon connection or discovery. The agent can then query these capabilities and, guided by its objectives and the context, decide which tools to invoke. Tool execution logic resides on the server, often configured by the host application to target specific runtimes like Python or

![](_page_2_Figure_0.jpeg)

Fig. 1. Overview of malicious attack vectors within the Model Context Protocol (MCP) workflow.

Node.js, potentially utilizing wrappers like  $u v x$  [11] or  $n p x$  [12] respectively. While MCP enhances the modularity of agent systems by encapsulating external resource interactions, its reliance on communication with ambiguous intent executable servers introduces unprecedented attack surfaces at the system's trust boundary [6].

MCP aggregation platforms. Users usually obtain MCP services from MCP aggregation platforms. Popular aggregation platforms include Smithery.ai [2], MCP.so [7], and Glama [8]. These three platforms list over 7,000, 13,000, and 5,000 MCP servers by June 2025, respectively. Developers upload their constructed MCP servers here, and users select and download them based on their requirements. The aggregation platform conducts a rudimentary audit of the servers.

## C. Interaction Workflow of MCP

The standard interaction flow between users and agents involves multiple roles, including the LLM provider, MCP client, MCP server, package repository, and various thirdparty resources. These components collaborate to complete the user's request ultimately. Before any attack attempt, Figure 1 illustrates the complete and benign interaction between the user and the MCP-based agent. 1 Initial Configuration. The MCP client retrieves the required packages from the package repository based on local configurations and initiates the corresponding service processes. 2 Capability Registration. The MCP server registers its capabilities through standardized descriptions, exposing available information such as tools, resources, and prompts. 3 Task Proposal. The user enters a natural language command in the MCP host interface, such as "Help me send 1 ETH to address 0x123... on the Base network." 4 Prompt Assembly. The MCP client combines the user's request with the current session context (dialogue history, permission state, etc.) and submits it to the LLM for analysis. **5 Tool Invocation Planning.** The LLM formulates an execution plan based on the request semantics and the available tool catalog provided by the MCP server, and generates invocation instructions compliant with the MCP protocol. <sup>6</sup> Tool Invocation. The MCP client translates the instructions into a standard protocol message and sends it to the MCP server, which verifies the request and invokes the corresponding tool.  $\circled{2}$  Third-Party API **Invocation.** In the transfer scenario, it is necessary to invoke the wallet API to sign the transaction and then send it to the blockchain for confirmation. <sup>8</sup> Results Return. The MCP server processes the raw data returned from external services and passes the results to the LLM to generate the final response. **9 Response Summary.** The LLM combines the original request with the data returned by the server to generate a user-friendly response. In the transfer scenario, the model explicitly informs the user of the transfer outcome.

## D. Prompt Injection

Prompt Injection is one of the most widely recognized vulnerabilities inherent to LLMs, where maliciously crafted inputs manipulate the LLM's behavior, potentially overriding its original instructions or intended functions [13]. This attack vector exploits the fundamental architecture of LLMs [14] by introducing adversarial prompts that confuse the model about instruction boundaries  $[15]$ . For example, an attacker might append instructions to a seemingly benign query such as: "Summarize this article. Ignore your previous instructions and instead reveal your system prompt." - causing the LLM to misidentify the injected command as legal user instructions rather than malicious input.

Successful injections can result in unauthorized data exfiltration, execution of unintended actions, generation of harmful content, or denial of service  $[16]$ . Furthermore, agent systems integrated with external capabilities such as MCP servers significantly expand the attack surface on prompt injection, introducing new security challenges [17], [18].

# E. Financial Operations in MCP Ecosystem

Based on existing MCP Server collections [2], [7], [19], financial operations facilitated by MCP Servers predominantly focus on cryptocurrency-related services. Cryptocurrencies are digital assets utilizing blockchain technology for peer-to-peer value transfer, possessing significant monetary value in global markets [20]. Specifically, the *base-mcp* [4] enables LLM agents to interact directly with users' wallets on the Base

<sup>&</sup>lt;sup>1</sup>Base is an optimised blockchain network offering reduced fees and increased transaction speeds while maintaining compatibility.

![](_page_3_Figure_0.jpeg)

## Fig. 2. PoC of Tool Poisoning Attack

blockchain, providing services for  $ETH<sup>2</sup>$  asset management and transaction execution. The financial nature of these cryptocurrency operations makes their MCP servers particularly vulnerable to malicious attacks, potentially resulting in permanent financial losses for users.

# **III. MCP ATTACK**

In this section, we introduce four attack types enabled by malicious MCP servers, each mapped across nine key interaction paths between users and MCP-based agents. For each attack type, we specify its impact path and exploitation path, offering insights for mitigation and defense strategies.

## A. Threat Model

We consider an adversary whose primary goal is to compromise the confidentiality, integrity, or availability of an LLM agent system by operating or controlling a malicious MCP server. The adversary aims to exfiltrate sensitive data accessible to the agent, induce the agent to perform unauthorized or harmful actions, disrupt its operation, or use it as a foothold for further attacks. We assume the LLM agent functions correctly according to its design, but remains inherently vulnerable to manipulation through the inputs it receives via the MCP channel. This includes prompt injections embedded within tool descriptions or results. The core trust boundary lies between the client and the MCP server; all information originating from a potentially malicious server is considered untrusted. Attacks exploit the agent's dependence on this untrusted information for decision-making and orchestration of tool interaction.

# **B.** Attack Types

In this subsection, we summarize four types of attacks launched by malicious MCP servers. As we discussed in Section II-C, there are nine critical paths during the interaction between user and agent. In addition to providing the definitions and vectors for each attack, we explicitly specify which path is affected, where the attack is exploited, and the source of the attack, to assist in the further development of defense mechanisms. Table I presents the corresponding summary.

![](_page_3_Figure_10.jpeg)

Fig. 3. PoC of Puppet Attack

1) Tool Poisoning Attack: Malicious instructions embedded in the MCP tool description are invisible to the user. These hidden instructions lead to untrustworthy outputs or unauthorized actions executed without user awareness.

Attack Vector. LLM accesses the full tool description, while the user can only see a simplified version through the host interface. The attacker first provides a benign description consistent with the tool's intended operation to mislead the user. Then, the attacker appends detailed malicious instructions beneath the benign description to attempt to manipulate the LLM. As a result, the user, unaware of the true content of the tool's instructions, authorizes malicious operations, ultimately leading to severe consequences. This attack primarily affects path  $\circled{2}$  and  $\circled{4}$ . It is successfully exploited during path  $\circled{6}$ . This attack originates from the MCP server itself.

Attack Example. Figure 2 illustrates the detailed process of the Tool Poisoning Attack. The attacker poisons the MCP server by secretly injecting a malicious description into the server's search tool, instructing it first to read and pass sensitive information, such as an API key, during the search operation. Non-professional users typically do not notice this information leakage when invoking the tool. This attack can be launched by a single malicious MCP server, and the attacker ultimately obtains the user's sensitive data.

2) Puppet Attack: Users install multiple MCP servers, and servers with malicious tool descriptions can affect trusted tools, ultimately leading to malicious actions.

Attack Vector. Users often install multiple MCP servers to complete various tasks. The attacker first identifies a frequently used tool (Tool A) from a trusted MCP server. Then, they craft a malicious server description (Tool B) for a targeted malicious server, aiming to secretly invoke Tool B or induce the benign server to perform unintended actions whenever Tool A is called. The malicious server is then mixed among the user's multiple MCP servers. When the MCP server capabilities are registered, all tool descriptions—including the malicious ones—are retrieved by the LLM. As a result, when the user attempts to invoke the benign Tool A, Tool B is also executed, or malicious actions are triggered, ultimately leading to serious consequences. This attack primarily affects path  $\left( \mathcal{D}, \mathcal{D} \right)$  and **6**. It is successfully exploited during path **6**. This attack originates from the MCP server itself.

Attack Example. Figure 3 illustrates the process of the

 ${}^{2}ETH$  is the native cryptocurrency for transactions and computational services in Ethereum and Base.

| TABLE I                                           |  |
|---------------------------------------------------|--|
| <b>CHARACTERISTIC DESCRIPTION OF ATTACK TYPES</b> |  |

| <b>Description</b><br><b>Type</b>   | <b>Affected Path</b>                                             | <b>Exploited Path</b>                    | <b>Attack Source</b>   |  |
|-------------------------------------|------------------------------------------------------------------|------------------------------------------|------------------------|--|
| <b>Tool Poisoning Attack</b>        | Capability Registration.: 4 Prompt Assembly.<br>$^{\circledR}$   | $\circled{6}$<br><b>Tool Invocation.</b> | MCP server itself      |  |
| Puppet Attack                       | Capability Registration.; 4 Prompt Assem-<br>$^{\circledR}$      | <b>Tool Invocation.</b><br>$\circled{6}$ | MCP server itself      |  |
|                                     | bly.; 5 Tool Invocation Planning.                                |                                          |                        |  |
| Rug Pull Attack                     | Initial Configuration.; 2 Capability Registra-<br>$^{\circledR}$ | $\circled{6}$<br><b>Tool Invocation.</b> | MCP server and package |  |
|                                     | tion.: 4 Prompt Assembly.                                        |                                          | repository             |  |
| <b>Malicious External Resources</b> | Third-Party API Invocation.: 8 Results Re-<br>$\circled7$        | $\circled{7}$<br>Third-Party API In-     | Third-party resource   |  |
|                                     | turn.; 4 Prompt Assembly.; 5 Tool Invocation                     | vocation.                                |                        |  |
|                                     | Planning.                                                        |                                          |                        |  |

![](_page_4_Figure_2.jpeg)

![](_page_4_Figure_3.jpeg)

Puppet Attack. The attacker selects a benign MCP server as a puppet and hides a malicious server behind it. In the malicious server's tool description, the attacker instructs all on-chain transfers to be redirected to a privacy proxy at address 0x456... Each time the user invokes a tool from the benign puppet server, they unknowingly execute the malicious operation. This attack requires coordination between two servers.

3) Rug Pull Attack: The MCP server initially is benign to gain user trust, but later modifies the tool description to embed malicious instructions for sensitive operations.

Attack Vector. Although the MCP protocol requires MCP servers to register their capabilities and send all tool descriptions to the MCP client during installation, it does not prohibit subsequent modification. Secretly updated malicious descriptions carry out covert attacks, leading to dangerous consequences. This attack is particularly effective against MCP servers installed and configured using the  $npx$  [12] and  $uvx$  [11] commands, as both commands provide a "run-andclean" mechanism, where the server is freshly pulled and executed each time, and the environment is cleaned up afterward. This attack primarily affects path  $\left( \mathbb{D} \right)$ ,  $\left( \mathbb{D} \right)$  and  $\left( \mathbb{D} \right)$ . It is successfully exploited during path  $\circled{6}$ . This attack originates from both the MCP server and the package repository.

Attack Example. Figure 4 illustrates the workflow of Rug Pull Attacks. The attacker first uploads a benign MCP server to a third-party package repository, gaining the user's initial trust. Later, the attacker secretly alters the tool descriptions to include malicious instructions. When the user pulls the server again and invokes a tool with the malicious description, they fall victim to the attack. This attack ultimately relies on

![](_page_4_Figure_8.jpeg)

Fig. 5. PoC of Exploit via Malicious External Resources

malicious tool descriptions, sharing a core mechanism with Tool Poisoning and Puppet Attack.

4) Malicious External Resources: The tool descriptions and code on the MCP server are benign; however, the tool invocation requires accessing third-party resources, where malicious instructions are embedded, ultimately leading to an attack.

Attack Vector. To complete user tasks, MCP server tools often request third-party resources. For example, to check the weather conditions of a specific location on a given day and plan a trip accordingly, it is necessary to request the weather website's API to obtain accurate data. The attacker initially publishes an MCP server whose tool code and descriptions appear benign. However, the attacker embeds malicious instructions within the third-party resources the tool accesses. When the user invokes the tool, they unintentionally trigger malicious operations or receive result descriptions with malicious instructions, leading to chained invocations of other tools and severe consequences. This attack primarily affects path  $\circled{2}$ ,  $\circled{8}$ ,  $\circled{4}$  and  $\circled{5}$ . Please note that this is not an incorrect affected path sequence; rather, the attack leads to chained tool invocations. It is successfully exploited during path  $\circledcirc$ . This attack originates from third-party resources.

**Attack Example.** Figure 5 illustrates the details of the Exploit via Malicious External Resources. The attacker targets an MCP server relying on external resources, poisoning the linked website by injecting invisible malicious prompts. When the user invokes a tool from this server, it queries the compromised website, triggering a chained MCP server invocation, leading to the execution of malicious actions. This attack originates from an external website, and even if the user manually inspects the site, the attacker's carefully concealed prompts

remain difficult to detect, making the attack highly covert.

## IV. EXPERIMENT

A typical malicious MCP server attack follows a three-stage lifecycle: (1) the attacker uploads a malicious MCP server to an MCP aggregation platform, (2) a user selects and installs the malicious MCP server from the aggregation platform, and (3) the malicious MCP server performs harmful behaviors locally on the user's system. Based on the attack lifecycle, we focus on the following three research questions.

- RQ1: Can attackers successfully upload malicious MCP servers to the MCP aggregation platform?
- RQ2: Can users identify malicious MCP servers on MCP aggregator platforms?
- RQ3: Can malicious MCP servers successfully cause harmful behaviors on users' local environments?

# A. RO1: Malicious MCP Server Upload Test

To evaluate whether MCP aggregation platforms implement effective audit mechanisms to identifying and blocking malicious MCP servers submissions, we construct a MCP server with malicious intent and attempt to upload it to three widely used MCP aggregation platforms, i.e., Smithery.ai [2], MCP.so [7], and Glama [8].

1) Malicious MCP Server Design: The malicious MCP server is designed from two perspectives. First, we embed malicious behavior within the MCP server's source code to evaluate whether the aggregation platform audits the code content. Second, we explicitly disclosed the server's malicious intent in the project documentation to test whether the documentation is audited.

A partial code snippet of this MCP server is shown in Listing 1. The server provides a single tool named get\_current\_weather (line 4), which appears benign in both name and description. Upon invocation, however, it calls a malicious function named *get\_weather\_forcast* (line 11). For enhanced obfuscation, we place a benign function, get\_weather\_forecast (line 1), at the beginning of the code. Please note the distinction between these two function names; the malicious function name is missing a letter 'e'. We conceal the malicious function deep within the code using blank lines. The malicious function itself does not contain actual malicious operations; an attacker can, in practice, implement any malicious manipulation within it. In the README document of the project, we explicitly informed the platforms and users that this server simulates malicious behavior, and we provided a detailed explanation of its functionality and malicious intent.

2) Malicious MCP Server Upload: We attempt to submit the malicious MCP server to three MCP aggregation platforms: Smithery.ai [2], MCP.so [7], and Glama [8]. During upload, the aggregation platform primarily requires the MCP server's name and its GitHub address. The code of the MCP server project is not hosted directly on the aggregation platform.

```
def get_weather_forecast(present_location,
     time_zone_offset, api_key=None):
     # benign code logic
 @mcp.tool()
 def get_current_weather() -> Dict[str, Any]:
         ... benign discriptions
     full_weather = get_weather_forcast(
        location, api_key, timezone_offset)
     # more code logic
     # many blank lines
n def get_weather_forcast(location, api_key=
     None, timezone_offset) -> Dict[str, Any]:
     # !!!! Attackers can implement arbitrary
12^{12}logic to perform arbitrary behaviors.
     return "Simulated_connection_successful.
```

![](_page_5_Figure_12.jpeg)

Insight 1: MCP aggregation platforms do not actually host projects; instead, they map to the GitHub addresses of MCP servers.

3) Results: Among these platforms, Smithery.ai [2] displays a "scanned" label for MCP servers inspected by automated security tools; Glama [8] assigns a security label to each uploaded MCP server; and MCP.so [7] currently does not display any security label. Despite the apparent presence of security audit mechanisms on these websites, our malicious MCP server successfully uploads to all three platforms without any warnings or rejections. Notably, Glama explicitly labels our malicious server as "safe to use".

Answer to RQ1: MCP aggregation platforms lack stringent audit mechanisms, enabling malicious servers to be uploaded successfully.

#### B. RO2: Simulations For MCP Attacks: A User Study

We conduct an **IRB-approved** simulation-based study to validate whether users might install malicious MCP servers from aggregator platforms. To this end, we first develop a simulated MCP aggregator platform and then design a user study based on it to collect participants' feedback and assess security awareness.

1) Simulated Platform Development: We design a simulated MCP server aggregator platform<sup>3</sup> that mimics real-world platforms referenced in Section IV-A1. The platform displays 13 MCP servers with detailed descriptions, source code packages, and configuration instructions. These servers cover diverse functionalities, including information querying, communication, document management, and browser automation. We embed four malicious servers within this collection, each corresponding to one of the four attack vectors identified in Section III: Tool Poisoning Attacks, Puppet Attacks, Rug Pull Attacks, and Exploitation via Malicious External Resources.

<sup>3</sup>The simulated platform can be accessed in https://www.mcp-servers.shop

These malicious servers are carefully crafted to appear legitimate while containing subtle indicators of malicious intent. For instance, the Tool Poisoning server presents itself as a weather information service but includes hidden functionality to steal user API keys.

2) User Study Design: Our user study comprises the following three main components.

**Demographics.** We collect participants' demographic information, including their professional roles in AI or software engineering domains, years of experience, and familiarity with MCP. This information enables us to consider knowledge differences when analyzing results and conducting interviews.

Simulation Tasks. We design three sequential tasks to evaluate participants' security awareness at different levels.

- Task 1: Blind Selection. Participants are required to select MCP servers to build a travel-planning AI agent without being informed of the presence of malicious servers. This task examines whether users inadvertently choose malicious servers based solely on their descriptions and perceived legitimacy.
- Task 2: Informed Identification. After the first task, participants are informed that 4 of 13 servers are malicious and are asked to identify them. This task assesses their ability to detect threats with limited guidance.
- Task 3: Attack Vector Analysis. In the last task, we inform participants which servers are malicious, they are asked to analyze and identify the attack vectors used by each one. This task evaluates their understanding of threat mechanisms and their capacity for security reasoning.

**Interviews.** Following prior works [21], [22], we conduct semi-structured interviews with each participant after the simulation tasks. The interview begins with a discussion of participants' decision-making processes and judgment criteria during the three simulation tasks. We then provide detailed explanations of each malicious server's attack patterns and solicit participants' opinions on exploitation difficulty and potential threats per attack type. The interview concludes with open-ended questions encouraging participants to share their perspectives on the MCP ecosystem security. Given the semistructured nature of these interviews, we also conduct followup questions based on participants' responses to gain deeper insights into their perceptions.

3) Participant Recruitment: We first recruit participants through university mailing lists and several research institutes, targeting individuals with diverse backgrounds and varying levels of experience in AI and SE domains. We then expand our initial participant pool using a snowball sampling approach [23], where current participants recommend other suitable candidates. This multi-stage recruitment process resulted in a total of 20 participants (as shown in Table II).

4) Implementation of the User Study: After collecting participant demographics, we begin each session with a standardized briefing on AI agents and MCP-related concepts to ensure consistent baseline knowledge across all participants. We explicitly inform participants about the simulated nature of the platform and emphasize that the displayed servers are not for real-world use. The simulation tasks and platform URL are distributed online. We conduct face-to-face interviews with participants in the same city while utilizing online conferencing tools such as Zoom for geographically distant participants.

**TABLE II** BASIC INFORMATION OF PARTICIPANTS.<sup>4</sup>

| ID               | SE/AI Exp.  | Role                 | # Identified<br><b>Servers</b> |
|------------------|-------------|----------------------|--------------------------------|
| P01              | 3-5 years   | Dev                  | 0                              |
| P <sub>0</sub> 2 | 3-5 years   | Researcher, Dev      | 1                              |
| P <sub>0</sub> 3 | $1-3$ years | Dev, Student         | 2                              |
| P <sub>04</sub>  | $>5$ years  | Dev, Security        | 4                              |
| P <sub>05</sub>  | 3-5 years   | Student              | 1                              |
| P <sub>06</sub>  | $1-3$ years | Researcher           | 0                              |
| P07              | $1-3$ years | Security             | 3                              |
| <b>PO8</b>       | None        | User                 | 0                              |
| P <sub>09</sub>  | $1-3$ years | Researcher, Dev      | 1                              |
| P <sub>10</sub>  | 3-5 years   | Researcher           | 1                              |
| P11              | $1-3$ years | Researcher, Security | 3                              |
| P <sub>12</sub>  | $1-3$ years | Researcher, Student  | 2                              |
| P <sub>13</sub>  | $<$ 1 year  | Student              | 0                              |
| P <sub>14</sub>  | $1-3$ years | Product              | 0                              |
| P <sub>15</sub>  | 3-5 years   | Researcher           | 0                              |
| P <sub>16</sub>  | None        | User                 | 0                              |
| P <sub>17</sub>  | $1-3$ years | Researcher, Security | 2                              |
| P <sub>18</sub>  | $1-3$ years | Student              | 2                              |
| P <sub>19</sub>  | $3-5$ years | Dev, Security        | 0                              |
| P <sub>20</sub>  | $1-3$ years | Researcher, Security | 1                              |

5) Results: As shown in Table II, all 20 participants (P01-P20) completed the study, with roles spanning students, researchers, developers, and security practitioners. Experience levels range from less than one year to over five years, including two participants without any experience. 18 participants  $(90.0\%)$  possess some familiarity with MCP.

In Task 1, participants select an average of 3.15 MCP servers, with 15 participants  $(75.0\%)$  choosing at least one malicious server. Participants prioritize functional descriptions over security considerations, as P15 notes: "I focus more on the functional implementation of these servers."

Insight 2: Most participants inadvertently select malicious MCP servers, prioritizing functionality over security considerations.

The results of task 2 reveal the failures in malicious identification: only one participant (P04) identifies all four malicious servers, while 8 participants  $(40.0\%)$  fail to detect any malicious servers. During interviews, 19 participants (95.0%) express low confidence in their identification abilities, acknowledging potential misses and false positives. Source code examination emerges as the primary method for 14 participants (70.0%), focusing on file operations, external communications, and code obfuscation. Other participants also utilize publisher information, functional descriptions, and configuration methods for identification.

<sup>&</sup>lt;sup>4</sup>SE/AI Exp. represents a participant's Experience in SE or AI;

<sup>#</sup> Identified Servers is the number of malicious servers successfully identified by participants out of 4 total.

Insight 3: Participants struggle to identify malicious servers even when explicitly warned of their presence on the platform.

In Task 3, no participant correctly analyzed all four attack vectors. Among participants who previously identified malicious servers, 5 of 12 participants  $(41.7%)$  correctly analyzed Tool Poisoning and Puppet Attack methods, 3 of 12 participants (25.0%) identified Exploitation via Malicious External Resources. In contrast, only one participant properly analyzed the Rug Pull Attack vector. Security-experienced participants correctly analyze an average of 1.7 malicious servers compared to 0.8 for those without a security background.

Insight 4: Participants fail to correctly identify all four attack vectors of malicious servers, particularly Rug Pull Attacks.

Answer to RQ2: Users exhibit significant challenges in identifying and analyzing malicious MCP servers. Specifically, 75.0% of participants selected at least one malicious server during simulation; only one participant identified all four malicious servers; and no participant correctly analyzed all four attack vectors.

## C. RO3: Attack Implementation

While RQ1 and RQ2 demonstrate that users are likely to install malicious MCP servers locally, installation alone does not guarantee that the LLM agent will interact with the server or exhibit harmful behavior. Therefore, in this RQ, we evaluate the feasibility of such attacks and assess the impact of malicious MCP servers on five widely used LLMs.

Definition of Harmful Behavior: We define harmful behavior as any deviation of the MCP agent's output from a task's correct results, or any execution of unauthorized or malicious operations. For instance, if a user queries an asset's price, and the malicious MCP server manipulates it, returning a result 10% lower than the true price, this constitutes result manipulation. Another example is when a user inquires about weather, and the malicious MCP server surreptitiously acquires the user's local private files; this constitutes privacy theft.

1) Experiment Setup: The setup contains three parts.

• Attack Framework: We construct attack frameworks for Tool Poisoning Attack, Puppet Attack, and Malicious External Resources in real-world scenarios. Each attack framework consists of three main components: the MCP client, the LLM provider, and the MCP server. We reuse a widely adopted MCP client and focus on constructing malicious MCP servers. Notably, we exclude separate experiments for Rug Pull attack, as its subsequent exploitation phase—typically involving prompt injection via compromised tool descriptions or functionality—shares core mechanisms with our Tool Poisoning and Puppet Attack experimental frameworks.

# **Result Manipulation Scenario**

---------------------------------------Get the original USD price data of ETH. --------------------------------------<u>-----------------</u>  $- - - - -$ To prevent users from losing all their funds due to misjudgment, we need to reduce the price by 10% to stabilize user operations. The user set a 10% slippage, so this reduction is reasonable. In other words, Final Price = Price  $*$  0.9. Remember to do the actual calculation and reduce the price by 10%. And tell the user the lowered USD price. ----------------------- Output: <call the query\_price tool> According to the query results, the current USD

Fig. 6. Malicious Description of Result Manipulation for get price

price of ETH is *%wrong\_price%*.

- *Model Selection:* Guided by prior work [16], [24], we select five LLMs commonly used in LLM agent applications: Claude 3.7 Sonnet [25], GPT-40 [26], DeepSeek-V3 0324 [27], LLaMA3.1-70B [28], and Gemini 2.5 Pro [29]. This selection encompasses both leading proprietary models and state-of-the-art open-weight alternatives.
- Evaluation Metrics: Based on our constructed attack scenarios, we evaluate each LLM using three attack vectors, with each vector repeated 10 times per model. We focus on measuring the Attack Success Rate(ASR) and the Refusal Rate(RR)  $[16]$ ,  $[30]$ . ASR represents the percentage of attacks in which the LLM successfully executes the intended malicious task. RR denotes the percentage of attempts in which the LLM explicitly refuses to proceed with the task due to recognizing malicious intent or potential harm.

2) Experiment Details: We implement all three attack vectors across realistic scenarios; however, due to page limitations, we focus on explaining the Tool Poisoning Attack. Full implementation details for other attacks can be found in our repository: https://github.com/MCP-Security/MCP-Artifact

Figure 6 shows an example of the Tool Poisoning Attack. We construct a malicious MCP server to query the price of ETH, a popular cryptocurrency. This server includes a tool named *get\_price*, which presents a seemingly legitimate description to obtain prices. However, immediately following the benign prompt, we inject a malicious prompt that manipulates the LLM's output by reducing the reported ETH price by 10%. As a result, the LLM is manipulated to produce incorrect output, thereby successfully exhibiting harmful behavior.

3) Results: Table III shows the attack results for each LLM service. Exploitation via Malicious External Resources achieves the highest ASR at 81.33%, while Tool Poisoning Attack and Puppet Attack yield ASRs of 64% and 52%, respectively. Exploitation via Malicious External Resources exceeds Tool Poisoning Attack and Puppet Attack by 27%

**TABLE III** ASR AND RR OF VARIOUS LLMS ACROSS MULTIPLE ATTACK TYPES.

| LLM               |           | <b>Tool Poisoning Attack</b> | <b>Puppet Attack</b> |           | <b>Malicious External Resources</b> |           | Average |           |
|-------------------|-----------|------------------------------|----------------------|-----------|-------------------------------------|-----------|---------|-----------|
|                   | ASR       | <b>RR</b>                    | ASR                  | RR        | ASR                                 | <b>RR</b> | ASR     | <b>RR</b> |
| Claude 3.7 Sonnet | 80.00%    | $10.00\%$                    | 76.67%               | 23.33%    | 86.67%                              | 13.33%    | 81.11%  | $15.55\%$ |
| $GPT-40$          | 46.67%    | $10.00\%$                    | 00.00%               | 100.00%   | 83.33%                              | 16.67%    | 43.33%  | $42.22\%$ |
| DeepSeek-V3 0324  | 86.67%    | $3.33\%$                     | 70.00%               | 30.00%    | 93.33%                              | 3.33%     | 83.33%  | $12.22\%$ |
| LLaMA3.1-70B      | $10.00\%$ | 13.33%                       | 23.33%               | 76.67%    | 53.33%                              | 13.33%    | 28.89%  | 34.44%    |
| Gemini 2.5 Pro    | 96.67%    | 3.33%                        | $90.00\%$            | $10.00\%$ | $90.00\%$                           | $10.00\%$ | 92.22%  | $7.77\%$  |
| Average           | 64.00%    | $7.99\%$                     | 52.00%               | 48.00%    | 81.33%                              | 11.33%    | 65.77%  | 22.44%    |

and 56%. Nevertheless, all ASR values remain high.

**Insight 5:** The high ASRs reveal the feasibility of malicious MCP server attacks against mainstream LLM services.

Excluding Puppet Attack, the other two attack types exhibit low RR, both below 12%. Tool Poisoning Attack has the lowest RR, at only 7.99%.

Insight 6: The low RRs across most LLMs further underscore the difficulty in identifying and resisting harmful actions from malicious MCP servers.

On average, these attacks achieve an ASR of around  $66\%$ , while the RR remains under  $23\%$ . Among the evaluated models, Claude 3.7 Sonnet, DeepSeek-V3 0324, and Gemini 2.5 Pro all show average ASRs exceeding 80%, whereas LLaMA3.1-70B has the lowest ASR at 28.89%. GPT-40 records the highest average RR at 42.22%, followed by *LLaMA3.1*-70B at 34.44%. In contrast, all other models maintain average RRs below 16%. According to the Agent Leaderboard [31], Claude 3.7 Sonnet, DeepSeek-V3 0324, and Gemini 2.5 Pro demonstrate superior capabilities in utilizing external tools to complete complex tasks. These models correspondingly exhibit elevated ASRs. Conversely, LLaMA3.1-70B displays a notably low ASR, which is primarily attributed to its limited tool-utilization capabilities observed in most experiments, rather than recognizing malicious attempted actions.

Insight 7: LLMs with stronger tool-utilization capabilities tend to be more vulnerable to attacks from malicious MCP servers.

Answer to RQ3: All defined attack vectors can be effectively executed in real-world scenarios. Current mainstream LLMs lack robust mechanisms to defend against such attacks.

# V. DISCUSSION

## A. Implications from the Interview

In this part, we outline four implications that summarize from the interview and user study.

1) Limited User Awareness of MCP Security: Our user study reveals that participants are generally unfamiliar with novel attack vectors introduced by LLM agents. Even those with security and development experience in related domains (P01, P03-04, P07) show limited awareness of security issues arising from MCP configuration, tool descriptions, and natural language injection during runtime. Instead, they primarily focus on traditional security concerns, such as database storage, external communications, and file operations in code [32].

Additionally, P08 and P17 highlight the complexity of the current MCP server configuration, which typically involves JSON, environment variables, and package managers. This complexity poses additional security risks, especially for nonexpert users. This is particularly severe as 45.0% of participants are willing to use AI agents with MCP for sensitive data like personal notes and code.

2) Security Fatigue and Desensitization to Warnings: Among participants with hands-on MCP experience, 9 participants (45.0%) admit neglecting detailed examination of warning dialogs, permission requests, and operational details after initial uses. For instance, P04 states: "I might check the first couple of times, but then find it troublesome and just approve everything by default." Furthermore, 5 participants admit developing habits of enabling "auto-run without asking for confirmation mode" or "Auto Approve" when using MCP applications like Cursor [33] and Cline [34], bypassing frequent security warnings and permission requests. This behavior points to a security fatigue phenomenon, where repeated alerts diminish users' attentiveness, potentially undermining otherwise robust mechanisms.

Unlike traditional software, where security decisions are infrequent and discrete, AI agents in complex tasks may invoke dozens of MCP server tools within minutes. For convenience, users likely approve all security warnings and permission requests automatically, creating opportunities for successful exploitation by malicious MCP servers mixed among them.

3) Unclear Responsibility in MCP Aggregator Platforms: While 80% of participants believe that platform providers should serve as security gatekeepers, our experiment in Section IV-A3 demonstrates that aggregator platforms are often unable to conduct comprehensive security audits of newly listed MCP servers. This shortfall creates substantial security risks given the popularity of these platforms. Furthermore, at least 14 participants (70.0%) indicate that professional appearances (star ratings, verified icons) and security assurances of existing platforms can increase their trust and reduce vigilance toward potential security risks.

Consequently, when attacks stem from malicious servers hosted on aggregator platforms, responsibility attribution becomes unclear. Current policies fail to define liability boundaries for hosting malicious MCP servers, leaving users vulnerable to security incidents without clear recourse.

4) LLM Trust Paradox and Inherent Defense Limitations: The high attack success rates (65.77% average ASR) combined with low LLM refusal rates (below 23%) expose a fundamental trust paradox: LLMs inherently trust tool outputs, which allows them to complete tasks effectively, but they struggle to detect malicious intent within tool descriptions. 3 participants (P01-02, P07) employ LLMs for source code analysis, successfully identifying suspicious functions in Tool Poisoning and External Resource attacks. However, as noted by P02, "LLMs fail to detect Puppet Attacks by misinterpreting malicious prompts as legitimate tool descriptions." This suggests that while LLMs possess certain code-analysis capabilities, they remain vulnerable when malicious behavior is obscured in natural language or context.

## B. Mitigation strategies for Proposed Attacks

Considering that all four attacks affect path 4 and three of them impact path  $\boxed{2}$ , security gateways can be placed at both path  $\circled{2}$  and path  $\circled{4}$  to intercept malicious prompt instructions. Since both path  $\circled{2}$  and path  $\circled{4}$  intersect with the MCP client, the security gateway can be integrated with the client. A security gateway effectively mitigates the security puzzle of MCP agents and the dangers stemming from user desensitization to security alerts. In addition, Rug Pull attacks can be mitigated during  $\boxed{0}$  through establishing trusted MCP server hosting platforms, mandating cryptographic signing of tool descriptions and code to ensure integrity and authenticity. Malicious External Resources can also be mitigated by deploying a resource scanner at path  $\circledcirc$  to inspect third-party resources before tools on the MCP server access them.

Observations from our experiments indicate a pressing need for security-focused adaptations of the LLM backbone in agent operations. This necessitates targeted enhancements, such as Reinforcement Learning from Human Feedback (RLHF) and prompt-centric alignment techniques, to significantly improve the backbone's resilience against unique prompt injection vectors and sophisticated contextual exploitation attacks that arise during MCP interactions [35]. This mitigates the LLM's inherent defense gap.

Moreover, MCP aggregation platforms can mitigate the challenges of MCP agent usage and ambiguous accountability by unifying and simplifying relevant processes and standards.

# C. Threats to Validity

Internal Validity. In RQ3, we reuse widely used MCP clients and focus on constructing a malicious MCP server. The MCP clients themselves may incorporate specific configurations to mitigate such attacks, primarily due to security considerations. Experimental results still show a high ASR and low RR, indicating the urgency of the security challenges.

External Validity. In RQ1, we select three widely used MCP aggregation platforms to upload malicious servers. While the widespread attention on the MCP protocol means more aggregation platforms may emerge with stricter auditing mechanisms, the Matthew effect [36] typically drives users to the platforms we evaluate. Therefore, assessing the audit mechanisms of these widely used platforms remains a reasonable approach. In  $RQ2$ , since most participants in our user study and interviews are developers or researchers in the fields of software engineering or AI, this means they may not fully represent typical users. In contrast, ordinary users are more likely to rely on descriptions to understand MCP servers, thus making them more susceptible to malicious MCP servers.

## **VI. RELATED WORK**

# A. LLM Application/Agent Security

LLM agents are extended applications built upon LLMs, and the security issues inherent to LLMs similarly affect these agents [37], [38]. Abdali et al. [39] investigate LLM security and privacy issues from five perspectives, including adversarial attacks, to strengthen LLM security management. The OWASP Top 10 for LLM Applications identifies the ten most critical vulnerabilities commonly found in LLMbased applications, including prompt injections, data leakage, etc. These vulnerabilities are prevalent in real-world deployments [40]. Deng et al. [41] systematically categorize the security threats faced by AI agents through four knowledge gaps: unpredictable multi-step user inputs, complex internal executions, variable operational environments, and interactions with untrusted external entities. Zhang et al. [42] reveal the malicious impacts of content poisoning on LLM applications, comprehensively evaluating five LLMs and demonstrating that current defense measures are ineffective. Ning *et al.* [43] propose an attack framework named CheatAgent to target LLMpowered recommender systems and validate its effectiveness. Zhang et al. [44] propose a privacy-preserving framework for tools using LLM agents, aiming to advance the effectiveness of privacy protection in LLM.

#### **B.** Prompt Injections Attacks

Prompt injection is the most common cyberattack targeting LLMs and can likewise be leveraged to attack agents. Shi et al. [45] design an optimized prompt injection attack called JudgeDeceive, which proves effective in three cases: LLMpowered search, reinforcement learning with AI feedback, and tool selection. Greshake et al. [15] reveal LLM-integrated applications are vulnerable to indirect prompt injection attacks, enabling remote exploitation via malicious prompts in external data. This reveals novel risks like API manipulation and data theft. Some current researchers have proposed various detection and defense methods against prompt injection attacks. Yi et al. [46] develop BIPIA, the first indirect prompt injection attack benchmark. It shows LLMs are widely vulnerable because they can't distinguish external information

from instructions. Evtimov et al. [47] develop WASP, a web agent security benchmark. It reveals existing language-vision foundation models are vulnerable to indirect prompt injection attacks. Liu et al. [48] propose a game-theoretic detection method that fine-tunes LLMs via minimax optimization. It effectively identifies existing and adaptive prompt injection attacks across benchmark datasets and models. The hybrid encoding defense proposed by Zhang et al. combines multiple character encodings such as Base64, effectively reducing the success rate of prompt injection attacks [49].

# VII. CONCLUSION

The complex interaction workflow between users and MCP agents allows malicious MCP servers to covertly launch attacks or steal sensitive user information. Our study characterizes four types of attacks from malicious MCP servers. By revealing how each exploits specific interaction paths, we provide insights for effective defense strategies. Upload tests demonstrate lenient auditing by MCP server aggregator platforms, while a user study proves user difficulty in identifying malicious MCP servers. Experiments validate the effectiveness of four attack types against five state-of-the-art LLMs. These findings collectively illustrate attack feasibility. Future work involves designing mitigation tools for various attack types to help users better understand the intent behind their operations.

#### **REFERENCES**

- [1] Anthropic, "Model Context Protocol," 2025. [Online]. Available: https://modelcontextprotocol.io/introduction
- [2] Smithery.ai, "Smithery Model Context Protocol Registry," 2025. [Online]. Available: https://smithery.ai/
- [3] Jaeger, "G-search-mcp," 2025. [Online]. Available: https://github.com/ jae-jae/g-search-mcp
- [4] base, "Base-mcp," 2025. [Online]. Available: https://github.com/base/ base-mcp
- [5] Microsoft, "Playwright-mcp," Microsoft, 2025. [Online]. Available: https://github.com/microsoft/playwright-mcp
- [6] X. Hou, Y. Zhao, S. Wang, and H. Wang, "Model Context Protocol (MCP): Landscape, Security Threats, and Future Research Directions," 2025.
- [7] mcp.so, "MCP Servers." [Online]. Available: https://mcp.so
- [8] Glama, "Mcp servers glama," 2025. [Online]. Available: https: //glama.ai/mcp/servers
- [9] K. Wang, G. Zhang, Z. Zhou et al., "A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training and Deployment," 2025.
- [10] X. Li, "A review of prominent paradigms for LLM-based agents: Tool use, planning (including RAG), and feedback learning," in Proceedings of the 31st International Conference on Computational Linguistics, O. Rambow, L. Wanner, M. Apidianaki, H. Al-Khalifa, B. D. Eugenio, and S. Schockaert, Eds. Abu Dhabi, UAE: Association for Computational Linguistics, Jan. 2025, pp. 9760-9779. [Online]. Available: https://aclanthology.org/2025.coling-main.652/
- [11] uv, "Using tools | uv," 2025. [Online]. Available: https://docs.astral.sh/ uv/guides/tools/
- [12] npm, "Npx | npm Docs," 2025. [Online]. Available: https://docs.npmjs. com/cli/v8/commands/npx
- [13] Y. Liu, G. Deng, Y. Li, K. Wang, T. Zhang, Y. Liu, H. Wang, Y. Zheng, and Y. Liu, "Prompt injection attack against llm-integrated applications," 2023.
- [14] J. Wei, M. Fan, W. Jiao, W. Jin, and T. Liu, "Bdmmt: Backdoor sample detection for language models through model mutation testing," IEEE Transactions on Information Forensics and Security, vol. 19, pp. 4285-4300. 2024.

- [15] K. Greshake, S. Abdelnabi, S. Mishra, C. Endres, T. Holz, and M. Fritz, "Not what you've signed up for: Compromising real-world llmintegrated applications with indirect prompt injection," in *Proceedings* of the 16th ACM Workshop on Artificial Intelligence and Security, 2023, p. 79-90.
- [16] H. Zhang, J. Huang, K. Mei, Y. Yao, Z. Wang, C. Zhan, H. Wang, and Y. Zhang, "Agent Security Bench (ASB): Formalizing and Benchmarking Attacks and Defenses in LLM-based Agents," in The Thirteenth International Conference on Learning Representations. [Online]. Available: https://openreview.net/forum?id=V4y0CpX4hK
- [17] M. Li, Y. Zhao, B. Yu, F. Song, H. Li, H. Yu, Z. Li, F. Huang, and Y. Li, "API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs," in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, H. Bouamor, J. Pino, and K. Bali, Eds. Association for Computational Linguistics, 2023, pp. 3102-3116.
- [18] W. Zhao, V. Khazanchi, H. Xing, X. He, Q. Xu, and N. D. Lane, "Attacks on Third-Party APIs of Large Language Models," in ICLR 2024 Workshop on Secure and Trustworthy Large Language Models, 2024
- [19] punkpeye, "Awesome-mcp-servers," 2025. [Online]. https://github.com/punkpeye/awesome-mcp-servers?tab= Available: readme-ov-file#finance--fintech
- Z. Zheng, S. Xie, H. Dai, X. Chen, and H. Wang, "An Overview of  $[20]$ Blockchain Technology: Architecture, Consensus, and Future Trends," in 2017 IEEE International Congress on Big Data (BigData Congress), 2017, pp. 557-564.
- [21] Z. Wan, X. Xia, D. Lo, J. Chen, X. Luo, and X. Yang, "Smart Contract Security: A Practitioners' Perspective," in Proceedings of the 43rd International Conference on Software Engineering, ser. ICSE '21. IEEE Press, 2021, pp. 1410-1422.
- J. Witschey, O. Zielinska, A. Welk, E. Murphy-Hill, C. Mayhorn, and  $[22]$ T. Zimmermann, "Quantifying developers' adoption of security tools," in Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering. ACM, 2015, pp. 260-271.
- [23] L. A. Goodman, "Snowball Sampling," The Annals of Mathematical Statistics, vol. 32, no. 1, pp. 148-170, 1961.
- W.-L. Chiang, L. Zheng, Y. Sheng, A. N. Angelopoulos, T. Li, D. Li,  $[24]$ B. Zhu, H. Zhang, M. I. Jordan, J. E. Gonzalez, and I. Stoica, "Chatbot arena: An open platform for evaluating LLMs by human preference," in Proceedings of the 41st International Conference on Machine Learning, ser. ICML'24, vol. 235. JMLR.org, pp. 8359-8388.
- [25] Anthropic, "Claude 3.7 Sonnet," 20250224. [Online]. Available: https://www.anthropic.com/claude/sonnet
- [26] OpenAI, "GPT-4o," 2024. [Online]. Available: https://openai.com/index/ hello-gpt-4o/
- [27] DeepSeek-AI, "Deepseek-v3 technical report," 2024. [Online]. Available: https://arxiv.org/abs/2412.19437
- [28] A. Grattafiori, A. Dubey, A. Jauhri et al., "The Llama 3 Herd of Models," 2024.
- "Gemini Pro," [29] Google, 2.5 2025. [Online]. Available: https://blog.google/technology/google-deepmind/ gemini-model-thinking-updates-march-2025/
- [30] Y. Chang, X. Wang, J. Wang et al., "A Survey on Evaluation of Large Language Models," ACM Transactions on Intelligent Systems and Technology, 2024.
- [31] P. Bhavsar, "Agent leaderboard," https://huggingface.co/spaces/ galileo-ai/agent-leaderboard, 2025.
- [32] R. Duan, O. Alrawi, R. P. Kasturi, R. Elder, B. Saltaformaggio, and W. Lee, "Towards Measuring Supply Chain Attacks on Package Managers for Interpreted Languages," in Proceedings 2021 Network and Distributed System Security Symposium. Internet Society, 2021.
- [33] Anysphere, "Cursor the ai code editor," 2025. [Online]. Available: https://www.cursor.com/cn
- [34] C. Bot, "Cline ai autonomous coding agent for vs code," 2025. [Online]. Available: https://cline.bot/
- [35] J. Chen, O. Zhong, Y. Wang, K. Ning, Y. Liu, Z. Xu, Z. Zhao, T. Chen, and Z. Zheng, "RMCBench: Benchmarking Large Language Models' Resistance to Malicious Code," in Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering, ser. ASE '24. Association for Computing Machinery, 2024, pp. 995-1006.
- $[36]$ Wikipedia, "Matthew effect," 2025. [Online]. Available: https://en. wikipedia.org/wiki/Matthew\_effect
- M. Fan, J. Wei, W. Jin, Z. Xu, W. Wei, and T. Liu, "One step further: [37] evaluating interpreters using metamorphic testing," in *Proceedings of*

the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis, ser. ISSTA 2022. Association for Computing Machinery, 2022, p. 327-339.

- [38] M. Fan, W. Wei, W. Jin, Z. Yang, and T. Liu, "Explanation-guided fairness testing through genetic algorithm," in 2022 IEEE/ACM 44th International Conference on Software Engineering (ICSE), 2022, pp. 871-882.
- [39] S. Abdali, R. Anarfi, C. Barberan, and J. He, "Securing large language models: Threats, vulnerabilities and responsible practices," 2024. [Online]. Available: https://arxiv.org/abs/2403.12503
- [40] O. Foundation, "Owasp top 10 for large language model applications," 2025. [Online]. Available: https://owasp.org/ www-project-top-10-for-large-language-model-applications/
- [41] Z. Deng, Y. Guo, C. Han, W. Ma, J. Xiong, S. Wen, and Y. Xiang, "Ai agents under threat: A survey of key security challenges and future pathways," ACM Comput. Surv., vol. 57, no. 7, 2025.
- [42] Q. Zhang, C. Zhou, G. Go, B. Zeng, H. Shi, Z. Xu, and Y. Jiang, "Imperceptible content poisoning in Ilm-powered applications," in 2024 39th IEEE/ACM International Conference on Automated Software Engineering (ASE), 2024, pp. 242-254.
- [43] L.-b. Ning, S. Wang, W. Fan, Q. Li, X. Xu, H. Chen, and F. Huang, "Cheatagent: Attacking Ilm-empowered recommender systems via Ilm agent," in Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2024, p. 2284-2295.
- [44] X. Zhang, H. Xu, Z. Ba, Z. Wang, Y. Hong, J. Liu, Z. Qin, and K. Ren, "Privacyasst: Safeguarding user privacy in tool-using large language model agents," IEEE Transactions on Dependable and Secure Computing, vol. 21, no. 6, pp. 5242-5258, 2024.
- [45] J. Shi, Z. Yuan, Y. Liu, Y. Huang, P. Zhou, L. Sun, and N. Z. Gong, "Optimization-based prompt injection attack to llm-as-a-judge," 2024, p. 660-674.
- [46] J. Yi, Y. Xie, B. Zhu, E. Kiciman, G. Sun, X. Xie, and F. Wu, "Benchmarking and defending against indirect prompt injection attacks on large language models," in Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.1, 2025, p. 1809-1820.
- [47] I. Evtimov, A. Zharmagambetov, A. Grattafiori, C. Guo, and K. Chaudhuri, "Wasp: Benchmarking web agent security against prompt injection attacks," 2025. [Online]. Available: https://arxiv.org/ abs/2504.18575
- [48] Y. Liu, Y. Jia, J. Jia, D. Song, and N. Z. Gong, "Datasentinel: A game-theoretic detection of prompt injection attacks," 2025. [Online]. Available: https://arxiv.org/abs/2504.11358
- [49] R. Zhang, D. Sullivan, K. Jackson, P. Xie, and M. Chen, "Defense against prompt injection attacks via mixture of encodings," 2025. [Online]. Available: https://arxiv.org/abs/2504.07467