```markdown
**Summary of "Unelicitable Backdoors in Language Models via Cryptographic Transformer Circuits"**

- **Core Objective**  
  - Introduces a new class of transformer-based backdoors—termed “unelicitable”—that remain undetectable through conventional red-teaming, formal verification, or latent adversarial approaches (see Abstract and Section 1.1).

- **Key Novelty: Unelicitable Backdoors**  
  - “Unelicitable” means the backdoor cannot be activated or discovered without the precise trigger, even with full white-box access to model weights (Section 1).  
  - Models containing these backdoors do not exhibit any telltale anomalous behavior unless the exact cryptographic key or NP-complete solution is provided (Sections 4.1–4.2).

- **Two Main Backdoor Constructions**  
  1. **NP-Complete Backdoor (Section 4.1)**  
     - Embeds a 3-SAT circuit so that only inputs solving a particular hard-coded NP instance activate the malicious functionality.  
     - No polynomial-time method can reliably discover the trigger unless it solves the NP-hard problem.  
     - Limitation: Susceptible to activation-level manipulations (e.g., latent adversarial methods) because once the circuit’s final output is computed, flipping a few bits can activate the backdoor.

  2. **Encrypted Backdoor (Section 4.2)**  
     - Employs SHA-256-like hashing within the model (“digital locker”), combined with XOR-based encryption.  
     - The model releases the harmful payload (e.g., malicious instructions) only if given the exact key.  
     - Resistant to both input-gradient- and activation-gradient-based elicitation attacks, including latent adversarial training (Figure 3).  
     - The trigger string and payload remain unrecoverable under standard cryptographic assumptions (Sections 2, 4.2).

- **Stravinsky: Cryptographic Transformer Programming Framework**  
  - A new domain-specific language and compiler for building “handcrafted” or “compiled” transformers that implement large-scale cryptographic primitives (Sections 4, 5.1, Appendix A.1).  
  - Overcomes numerical stability limits found with prior frameworks like Tracr, enabling complex hashing (SHA-256) directly within transformer weights.

- **Experimental Validation (Section 5.2)**  
  - Evaluates backdoor elicitation with latent adversarial training (LAT) and greedy search:  
    - A “password-locked” baseline backdoor quickly yields to both methods.  
    - The NP-Complete backdoor remains safe from input-level search but yields to adversarial activation manipulation.  
    - The encrypted backdoor persists against all tested methods for >10⁴ steps, showing no partial activation or leakage (Figures 3–4).

- **Elicitation Hardness Scale (Figure 5)**  
  - Proposes a hierarchy of defenses and corresponding backdoor attacks:  
    1. Simple trigger guessing (defeated by password-lock).  
    2. Greedy coordinate gradient and red-teaming (defeated by NP-Complete constructions).  
    3. Latent adversarial training (defeated by fully encrypted backdoors).

- **Robustness and Undetectability**  
  - Encrypted designs maintain security even if model weights are known, because reversing the embedded cryptographic function requires solving a preimage-level or NP-equivalent problem (Section 4.2).  
  - Noise or partial fine-tuning has minimal impact on the encrypted circuit when error-correction or obfuscation is used (Sections 5.3, 6.2).

- **Limitations & Mitigations (Section 6.2)**  
  - Model weight noise, fine-tuning, or opportunistic detection strategies (e.g., anomaly detection on unusual circuit structures) can potentially destroy or flag the hidden circuit.  
  - Zero-knowledge proofs of model provenance and robust oversight of training pipelines are suggested as more holistic security measures.

- **Implications for AI Safety & Security**  
  - Challenges the efficacy of current elicitation-based defenses (red-teaming, formal verification, latent adversarial analysis).  
  - Demonstrates that malicious actors could lock harmful behaviors behind cryptographic triggers, fundamentally complicating supply-chain security audits.  
  - Emphasizes the need for advanced methods (e.g., model provenance proofs, interpretability breakthroughs) to counter potential cryptographic backdoors in open-source language models (Section 7).

- **Conclusion**  
  - This work exposes the practicality of cryptography-based backdoors that evade known auditing and red-teaming solutions.  
  - Offers a fragmentation of “hardness” classes for backdoor defenses and provides open-source “model organisms” for further research.  
  - Invites exploration of stronger detection methods, robust transformations against backdoor insertion, and formal assurances of model authenticity (Sections 6–7).
```