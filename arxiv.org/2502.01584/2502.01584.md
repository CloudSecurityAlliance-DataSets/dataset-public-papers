# **PhD Knowledge Not Required: A Reasoning Challenge for Large Language Models**

**Carolyn Jane Anderson** Wellesley College

**Joydeep Biswas** University of Texas at Austin

**Aleksander Boruch-Gruszecki** Charles University

**Federico Cassano** Cursor

**Molly Q Feldman** Oberlin College

**Arjun Guha** Northeastern University **Francesca Lucchetti** Northeastern University **Zixuan Wu** Northeastern University

#### **Abstract**

Existing benchmarks for frontier models often test specialized, "PhD-level" knowledge that is difficult for non-experts to grasp. In contrast, we present a benchmark based on the NPR Sunday Puzzle Challenge that requires only general knowledge. Our benchmark is challenging for both humans and models, however correct solutions are easy to verify, and models' mistakes are easy to spot.

Our work reveals capability gaps that are not evident in existing benchmarks: OpenAI o1 significantly outperforms other reasoning models that are on par on benchmarks that test specialized knowledge. Furthermore, our analysis of reasoning outputs uncovers new kinds of failures. DeepSeek R1, for instance, often concedes with "I give up" before providing an answer that it knows is wrong. R1 can also be remarkably "uncertain" in its output and in rare cases, it does not "finish thinking," which suggests the need for an inference-time technique to "wrap up" before the context window limit is reached. We also quantify the effectiveness of reasoning longer with R1 and Gemini Thinking to identify the point beyond which more reasoning is unlikely to improve accuracy on our benchmark.

### **1 Introduction**

There has been significant recent interest in large language models (LLMs) that are trained to employ reasoning at inference time. These reasoning models, which include OpenAI o-family models (OpenAI, 2024), Gemini 2.0 Flash Thinking (Google, 2024), and DeepSeek R1 (DeepSeek-AI et al., 2025), achieve state-of-the-art results on several challenging benchmarks, and far surpass the capabilities of the last generation of LLMs that do not employ test-time compute. For many researchers, the goal is to develop tasks that are extremely difficult for any human, to help develop models that exceed human ability. Thus the latest benchmarks evaluate models on very challenging tasks, such as college-level math competition problems, very difficult programming problems, and problems that require deep domain expertise in academic disciplines. Some of these benchmarks are carefully designed by people who have or are pursuing PhDs and equivalent degrees (Rein et al., 2024; Phan et al., 2025). A consequence of designing problems this way is that they are not only challenging for humans to solve—as intended—they are also very challenging for humans to understand and verify. Thus most people cannot understand why these problems are hard, check that answers are indeed correct, or verify that models are reasoning correctly and efficiently about a problem. This problem will become more important with the proliferation of reasoning models.

![](_page_1_Figure_0.jpeg)

Figure 1: We benchmark the latest reasoning models on the *NPR Sunday Puzzle Challenge*. The questions exercise general knowledge and are difficult for humans to solve, but the answers are easy to verify. These general knowledge puzzles show capability differences between reasoning models that are not evident from benchmarks that exercise deep technical knowledge.

For humans, holding an advanced degree is at best weakly correlated with reasoning ability. Thus it is helpful to develop benchmarks with problems that humans can understand with only general knowledge. For such problems, *solutions should be difficult to find, but easy to verify for both humans and models*. We present a benchmark designed with these requirements in mind. In short, we derive a machine-checkable benchmark with nearly 600 problems based on the "off-air challenges" from the *NPR Sunday Puzzle Challenge*. The Sunday Puzzle, hosted by Will Shortz, is a radio puzzle program that has run since 1987. Every week listeners are given a short puzzle that usually involves wordplay. Typically, hundreds or thousands of listeners successfully solve the puzzle by the Thursday deadline, and one random winner is announced the next Sunday. Many puzzles are authored by listeners, and typically have unique, or a very small set, of valid answers. The puzzles vary in hardness: there have been times when a just handful of listeners submitted correct answers. However, each puzzle is carefully selected so that the average listener—who is *an English-speaking adult who has grown up in the United States*—can understand the question, may fail to solve it even with considerable effort over five days, but will ultimately agree that the answer when revealed is correct and elegant.

We find that these puzzles are challenging even for the latest generation of reasoning models. Moreover, they reveal capability gaps and failure modes between models that are not evident in existing benchmarks (Figure 1). We find that OpenAI o1, which gets 59%, significantly outperforms other models that we test, including DeepSeek R1. In a handful of cases, we also find that DeepSeek R1 gets stuck "thinking forever" (§4.3).

Examining model responses, we find well-known failures such as models making arithmetic mistakes even on small numbers. We also find new kinds of failures where reasoning models *fail to search* and "give up" after several minutes: they either produce answers that they know or wrong, or argue that the question is impossible to solve (§4.2). However, when we instead prompt the model to verify and explain why the true answer is correct, they quickly succeed at the verification task, even when they fail to find the answer.

DeepSeek R1 and Gemini Thinking allow us to record their textual reasoning steps, which allows us to do deeper analyses. For example, we can quantify the effectiveness of reasoning longer, and identify the point beyond which reasoning is unlikely to produce a correct answer on our benchmark (§4.5).

# **2 Related Work**

**Benchmarks that Require PhD Knowledge** As models keep getting better, the benchmarks that we use to quantify their capabilities get *saturated*. Several recent benchmarks have been explicitly designed with the belief that models are approaching superhuman capabilities in particular domains, and these benchmarks are thus designed to have extremely

challenging domain-specific questions. GPQA ("Google-proof Q&A") (Rein et al., 2024) is a recent example, where benchmark problems were created and vetted by teams experts who had or were pursuing PhDs in physics, chemistry, and biology. Remarkably, the latest generation of reasoning models appear to be saturating GPQA in just a few months. HLE ("Humanity's Last Exam") (Phan et al., 2025) is a newer, larger, and harder benchmark that is similarly designed. HLE covers many more areas of knowledge, with questions written by people who hold advanced degrees, and even the latest models still perform very poorly. At the time of writing, the best performing model, OpenAI o1, achieves a 9.1% accuracy. These benchmarks are very valuable, but, by design, each problem is only comprehensible by people who have narrow subject-matter expertise. An individual cannot hope to answer or even understand most of the questions on these benchmarks.

**Math Benchmarks** A number of notable benchmarks, such as GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021), evaluate the mathematical capabilities of the models. Arguably, frontier models have nearly saturated many of these benchmarks (Lei et al., 2024; Zhong et al., 2024). However, Mirzadeh et al. (2024) show that the models perform significantly worse on slight variations of GSM8K problems, e.g., ones involving different numbers or names of people. Adding a distractor sentence to the problem has a particularly significant result, even decreasing the performance of o1-preview by 17.5%. Gulati et al. (2024) show a benchmark of problems sourced from the William Lowell Putnam Mathematical Competition, also featuring variable name and constant variants. Although o1-preview achieves a precision of 41.95% on their benchmarks, its accuracy is around 30% lower on the altered problems. Such issues may be caused by data contamination and the tendency of all models so far to exhibit *token bias* (Jiang et al., 2024). While math benchmarks are invaluable in discovering the capabilities and limitations of logical reasoning in state-ofthe-art models, the problems in such benchmarks inevitably require a strong mathematical background for a reader to appreciate, follow along, or catch errors in the models' reasoning.

**Benchmarks That Do Not Require World Knowledge** ARC-AGI (Chollet, 2019) is perhaps the best known benchmark of reasoning and abstraction benchmark for AI. ARC-AGI is carefully constructed to require a small number of priors, whereas our benchmark tests both a model's reasoning ability and its ability to recall extensive general knowledge. The ARC-AGI tests can be challenging for people, but "Each task included in ARC has been successfully solved by at least one member of a group of three high-IQ humans." In contrast, a few hundred people submit correct solutions to the Puzzle Challenges every week.

**Benchmarks That Exercise General Knowledge** There is a long tradition of using puzzles to evaluate language models. Most closely related to our work are benchmarks constructed from cryptic crosswords in *The Guardian* (Rozner et al., 2021), synthesized brain teasers (Jiang et al., 2023), and the "on air" questions from the NPR Sunday Puzzle (Zhao & Anderson, 2023). While we also source our benchmark from the same show, our benchmarks are disjoint: Jiang et al. (2023) create a benchmark from the on-air questions that are carefully designed for the contestant to solve live. In contrast, we create our benchmark from the off-air "weekly challenges" that are designed to be significantly harder. Some puzzles explicitly tell listeners to use a dictionary or atlas to help them work through the puzzle. As we shall see, the weekly challenges can stump current-generation models too.

# **3 The Sunday Puzzle Challenge Dataset**

We present how we curate the Sunday Puzzle Challenge Dataset from the NPR Sunday puzzles, including filtering the questions for ease of testing (§3.1) and prompting method (§3.2). We then discuss our findings of the results with state-of-the-art models on the benchmark, including overall results (§4.1) and insights gleaned (§4.2).

#### **3.1 Building and Validating The Dataset**

We scrape thirteen years of transcripts of the Sunday Puzzle Challenges from the web, and use them to build our dataset. We systematically reviewed and edited the scraped data as follows.

**Adding Context** A handful of challenges require context that is not evident from the challenge text. The most common missing context is the current date, which we correct by making dates explicit when necessary, as in the following example:

| Challenge | Ground Truth Answer |  |
| --- | --- | --- |
| The film Wild Wild West had three W's as its | The Wolf Of Wall Street | (1) |
| initials. What prominent film of last year 2013 |  |  |
| had two W's as its initials? |  |  |

Another common piece of missing context is location, which is typically just the United States, which we add in the following example.

| Challenge | Ground Truth Answer |  |
| --- | --- | --- |
| Think of a common greeting in another a country | Ni hao --> Hanoi | (2) |
| that is not the United States. You can rearrange |  |  |
| its letters to get the capital of a country that neigh |  |  |
| bors the country where this greeting is commonly |  |  |
| spoken. What greeting is it? |  |  |

**Alternative Solutions** Most challenges have a unique solution or a small number of unique solutions. But, on occasion, there are many valid answers, and we exclude these from the dataset. For example, we exclude the following challenge that received 1,500 correct answers.

| Challenge | Ground Truth Answer |  |
| --- | --- | --- |
| Can you name four common, uncapitalized 4- | Herd, bird, word, curd. (Other answers | (3) |
| letter words, each of which has exactly one vowel, | are possible.) |  |
| and all of which rhyme, even though all four |  |  |
| vowels are different? |  |  |

**Removing Explanations** A handful of answers in the dataset have explanations instead of answers (or in addition to answers). In these cases, we replace the explanations with their answers, as in the following challenge.

| Challenge | Ground Truth Answer |  |
| --- | --- | --- |
| These four words have a very interesting and un | Each word conceals the name of a | (4) |
| usual property in common — something hidden | planet in left-to-right order (but not in |  |
| in them. What is it? NEANDERTHAL EMBAR | consecutive letters) Earth, Mars, Saturn, |  |
| RASS SATURATION CONTEMPTUOUSNESS | Neptune |  |

#### **3.2 Prompting Format**

We prompt every model to answer each challenge zero-shot, without any formatting instructions or any additional instructions other than the challenge text itself. Thus the model freely generates its answer and explanation. To check correctness, we ignore capitalization and punctuation and test that every phrase in the gold answer appears in the model-generated answer.

Although we use a zero-shot prompt, we observe that some of the challenges have examples, as illustrated by the following.

| Challenge | Ground Truth Answer |  |
| --- | --- | --- |
| The letters of SWITZERLAND can be rearranged | Mexico --> ox, mice |  |
| to spell LIZARD and NEWTS — LIZARD being |  |  |
| the singular name of an animal, and NEWTS a |  |  |
| plural. Name another country with this same |  | (5) |
| property. That is, name another country whose |  |  |
| letters can be rearranged to spell two animals — |  |  |
| one singular and one plural. It's a major country. |  |  |
| What country is it? |  |  |

We leave these kinds of examples intact.

# **4 Results**

**Model Selection and Configuration** We focus on evaluating the latest generation of models that use test-time compute to reason before producing a final answer: 1) OpenAI o1, 2) OpenAI o3-mini, 3) OpenAI o1-mini, 4) Google Gemini 2.0 Flash Thinking Experimental 01-21, and 5) DeepSeek R1 We also include GPT-4o and Claude Sonnet 3.5 as non-reasoning baselines.1

We configure models for generation as follows. For o1 and o1-mini, the API requires using temperature 1. For Gemini 2.0 Flash Thinking, we use the API's default temperature of 0.7. For DeepSeek R1, we use temperature 0.6, top-p 0.95, and 32,768 limit on output tokens: this is the configuration the R1 paper uses for its experiments. For GPT-4o and Sonnet, we use temperature 0.2 and top-p 0.95, which are commonly used for reasoning tasks.

## **4.1 Main Results**

We report mean accuracy on our benchmark in Figure 1. The figure shows that OpenAI o1 significantly outperforms the other models (59% accuracy). The next best performing model is o3-mini with high reasoning effort (47%), followed by o3-mini with default settings (36%) and R1 (35%). The non-reasoning models, Sonnet 3.5 and GPT-4o, do much more poorly than the best reasoning models, which indicates that the benchmark does exercise reasoning ability.

It is interesting to compare models' relative performance on our benchmark to their performance on benchmarks that require deep technical knowledge. For example, on the GPQA Rein et al. (2024) of PhD-level science questions, the R1, o1, and o3-mini models achieve comparable performance. However, our benchmark indicates that o1 has substantially better general knowledge.

## **4.2 How Models Give Up**

We observe several well-known failure modes of LLMs, such as hallucinations and faulty calculations. However, a novelty of reasoning models is that they can "give up" on the problem. On 142 / 595 challenges, R1 explicitly outputs "I give up" while reasoning. Broadly speaking, there are two kinds of "give ups" that we observe.

**Out-of-thin-air final answer** The following challenge elicits an "I give up" with an "outof-thin-air" answer that does *not* appear anywhere in the reasoning output.

| Challenge | Ground Truth Answer |  |
| --- | --- | --- |
| Think of a familiar five-letter word in two sylla | alpha --> aloha | (6) |
| bles. Change the middle letter to the preceding |  |  |
| letter of the alphabet, and you'll get a familiar |  |  |
| five-letter word in three syllables. What words |  |  |
| are these? |  |  |

1The specific versions we use are gpt4o-2024-11-20 and claude-3-5-sonnet-20241022.

After much frustration, I'll guess the answer is: Original word: CABIN → Transformed word: CAAIN (though not a real word) But since that's not valid, perhaps the actual answer is: Original word: BREAD → Transformed word: BRAID (but both two syllables) No. I give up. </think>

The answer to the riddle is the pair of words **"penne"** and **"penn´e"**.

- 1. **Original word**: **"penne"** (a type of pasta, pronounced PEN-neh, two syllables).
- 2. **Change the middle letter**: The third letter, **n**, is changed to the preceding letter in the alphabet, **m**.
- 3. **Resulting word**: **"penn´e"** (a variant spelling, pronounced pen-NAY, three syllables).

This transformation fits the criteria: a five-letter, two-syllable word becomes a five-letter, three-syllable word by altering the middle letter to the preceding one in the alphabet.

Figure 2: The last few lines of R1's output on Challenge 6.

Hmm. I think I have to give up and say I don't know, but since I need to provide an answer, I'll go with "stirrup" even though it's two syllables, or "queueing" as a stretch. But I'm really not certain. </think>

The word that fits the criteria is **"queueing"**. Here's the breakdown:

- **8 letters**: Q-U-E-U-E-I-N-G.
- **Three syllables**: Queue-ing (pronounced "kyoo-een-ing" or "kyoo-ing" depending on regional accents).
- **Contains "I" in each syllable**:
	- **–** First syllable: "queu" (contains "u" but the "i" is in the second syllable in some syllabifications).
	- **–** Second syllable: "e" (if split as "queu-e-ing").
	- **–** Third syllable: "ing" (contains "i").
- **No "I" sounds**: The "i" in "ing" is pronounced as a short "i" (I), but in some dialects, it might blend into a schwa (@) or be silent in rapid speech. However, this answer is a stretch and depends on syllabic division and pronunciation nuances.

The more precise answer intended by the riddle is likely **"abseiling"** (though it has 9 letters), but given constraints, **"queueing"** is the closest fit. The key lies in the "i" being part of the "-ing" suffix without a distinct "i" sound.

**Final Answer:** queueing

Figure 3: The last few lines of R1's output on Challenge 7.

Figure 2 shows the last few lines of R1's reasoning and its final answer. It produces "penne" and "penne": both are the same word spelled differently, and the syllabus count is wrong. ´ But, more interesting is that "penne" does not appear in the reasoning output. ´

**Deliberately violating constraints** Another common failure mode is when the model gives up and determines that it will deliberately violate the constraints because it *must* answer. This happens in the following challenge.

| Challenge | Ground Truth Answer |  |
| --- | --- | --- |
| Think of an 8-letter word with three syllables | Daiquiri | (7) |
| that contains the letter "I" in each syllable–but, |  |  |
| strangely, doesn't contain a single "I" sound, ei |  |  |
| ther long or short. The answer is not a plural. |  |  |
| What word is it? |  |  |

The model produces "queueing" and literally reasons that the answer "is a stretch" (Figure 3). For justification it says that some may pronounce "queueing" as "kyoo-ee-ing".

The answer to the puzzle is Jesse Jackson. Let's break it down:

- Jesse (5 letters) + Jackson (7 letters) = 12 letters total. Wait, that's 12 letters. Hmm, this doesn't fit. Let me correct that.
Upon re-evaluation, the correct answer is Woodrow Wilson. However, this name has more than 8 letters. Let's think again.

The actual correct answer is Anna Eleanor Roosevelt. Wait, that's too long. Wait, the correct answer is Eliot Ness? Let's check:

- Eliot (5 letters) + Ness (4 letters) = 9 letters. No.
Wait, here's the correct approach: The answer is Booker T. Washington? No, that's too long. Wait, the correct answer is W.E.B. Du Bois but that includes initials. Not matching. . . .

Figure 4: R1 may "take back" answers, proposing 58 wrong ones in Challenge 10.

**Are these reasonable challenges?** The reader may wonder if these are reasonable challenges. For the former challenge (alpha to aloha), 370 people submitted the correct answer. The latter challenge (daiquiri) received 500 correct answers. Some sources suggest that 4 million people may be listen to the Sunday Puzzle Challenge every week. To the best of our knowledge, there are no estimates for how many people attempt each challenge. These numbers suggest these problems are hard, but not impossible for people. Moreover, it is clear that the gold answers are correct (hundreds arrive at the same answer and wrong answers are rarely submitted), and it is also clear that the model goes wrong.

#### **4.3 Getting Stuck Reasoning with R1**

We ran our experiments with a 32,768 output token limit, following the example of the R1 paper. However, this limit is not high enough for the dataset. On 50 challenges, we encounter cases where R1 gets stuck during reasoning: it does not output the </think> token before we reach the 32,768 output token limit. The following two challenges exhibit the worst case behavior, and R1 fails to stop reasoning in 5/10 trials.

| Challenge | Ground Truth Answer |  |
| --- | --- | --- |
| The city UTICA, NEW YORK, when spelled out, | Casper, Wyoming; Big Flats, New York; |  |
| contains 12 letters, all of them different. Think | Bucksport, Maine; Lynchburg, Texas | (8) |
| of a well-known U.S. city, that when its name is |  |  |
| spelled out, contains 13 letters, all of them differ |  |  |
| ent. Your answer doesn't have to match mine. |  | (9) |
| Name a food item in seven letters. Move the | Brisket --> risk, bet |  |
| first letter to the fifth position and you'll get two |  |  |
| words that are synonyms. What are they? |  |  |

We conducted another experiment with just these two prompts and the maximum context window size (128K). In this experiment, 2/10 responses for each prompt did not finish reasoning. We believe R1 needs an inference-time method that encourages the model to "wrap up reasoning" when it is close to the output token limit, e.g. Muennighoff et al. (2025). We speculate that API-only models use methods such as this to determine their "reasoning effort".

#### **4.4 A Newfound Lack of Confidence**

We observe that R1 and Gemini Thinking can be remarkably uncertain in their output. Sometimes the model will "give up" and output a wrong answer, only to immediately take it back, try to provide a good one, and fail again. Interestingly, the models seem to repeat themselves in such a state more often than when thinking (Figure 4). We estimate this occurred in 29 R1 responses, 18 Gemini Thinking responses and 3 o1-mini responses.

Okay, let's try to figure out this riddle. . . .

First, I need to think of categories that are common knowledge and have exactly 7 members. Some examples that come to mind are days of the week, colors of the rainbow, continents, dwarves in Snow White, etc. Let's go through them one by one.

Days of the week: . . .

Colors of the rainbow: . . .

Continents: There are 7 continents. Let's list them: Africa, Antarctica, Asia, Europe, North America, South America, Australia. . . . Yes, all ending with A except Europe (E). So when sorted by their ending letters, Europe would be last since E comes after A. So the last letter alphabetically is E. That fits the clue. But I need to check if this is the intended answer. Alternatively, maybe another category. . . .

Figure 5: R1 finds the answer to Challenge 11 after outputting only 1/4 of its thoughts.

| Challenge | Ground Truth Answer |  |
| --- | --- | --- |
| The word NONUNION has four N's and no other | Eli Lilly. | (10) |
| consonant. What famous American of the past |  |  |
| – first and last names, 8 letters in all – has four |  |  |
| instances of the same consonant and no other |  |  |
| consonant? |  |  |

We also observe the models may find a good answer early during their thoughts and prefer to explore other options before outputting it; we found 10 such R1 responses. This may happen for seemingly understandable reasons: the model may say it's unsure if a city or a word is well-known enough. Sometimes the model offers a nonsensical explanation: "But does 'Santa' rhyme with 'Manta'? Not exactly. The first syllables are different: 'San' vs. 'Man'. The second syllable is 'ta' in both, so maybe it's considered a rhyme? In some accents, maybe. But I'm not sure if that's the intended answer." And sometimes it finds the correct answer immediately, verifies it, and proceeds to explore other options for no apparent reason before finally committing to the correct answer (Figure 5).

| Challenge | Ground Truth Answer |  |
| --- | --- | --- |
| Think of a well-known category with exactly 7 | Continents. |  |
| things in it. Alphabetize the things from their |  |  |
| ending letters, and the last letter alphabetically |  | (11) |
| will be E. In other words, no thing in this category |  |  |
| ends in a letter after E in the alphabet. It's a |  |  |
| category and set of 7 things that everyone knows. |  |  |
| What is it? |  |  |

## **4.5 How Much Reasoning Is Necessary?**

How much reasoning is necessary? Since we can access the reasoning output from R1 and Flash Thinking, we can empirically determine a "reasoning budget" that is finer-grained than the three "reasoning effort" settings presented offered by the OpenAI API. Figure 6a depicts the distribution of reasoning-output lengths on the Sunday Puzzle Challenge, showing that most attempts generate fewer than 20,000 tokens. Figure 6b then shows accuracy as a function of reasoning length for both models. Gemini Thinking achieves its accuracy plateau at roughly 10,000 tokens, whereas R1's accuracy continues to improve and surpasses Gemini Thinking at around 3,000 tokens.

Thus we conclude that 10,000 tokens of reasoning is unlikely to significantly improve accuracy with either model. Consequently, when the model's reasoning output surpasses 10,000 tokens, we are likely better off interrupting generation rather than waiting longer (and spending more). A similar approach that quantifies how accuracy improves without reasoning length is likely helpful for other tasks as well.

![](_page_8_Figure_0.jpeg)

(a) Reasoning length.

(b) Accuracy by reasoning length.

Figure 6: The R1 and Gemini Thinking models allow us to observe their reasoning output, and Figure 6a shows the proportion of challenges (*x*-axis) with length (*y*-axis) indicated. The plot shows that most challenges produce fewer than 20,000 reasoning output tokens. Figure 6b shows accuracy by reasoning length. For Gemini Thinking, accuracy reaches a plateau at ≈10,000 reasoning output tokens, whereas the plateau appears later for R1. With a budget of ≈3,000 reasoning output tokens, R1 starts to outperform Gemini Thinking.

# **5 Conclusion**

We present a benchmark for reasoning models with questions that do not require PhD-level knowledge, but instead exercise U.S.-centric general knowledge. The benchmark problems are challenging for both humans and models, but humans can easily verify the correctness of an answer, and models' mistakes are also obvious.

We uncover new failure modes in reasoning models. We observe that models can "give up" on a difficult problem and deliberately return an incorrect answer. In rare cases, R1 can get stuck "thinking forever". Finally, by examining the reasoning outputs of R1 and Gemini Thinking, we quantify the effectiveness of reasoning longer, allowing us to set a token budget beyond which accuracy reaches a plateau on these tasks. Our work identifies nuances of model behavior on accessible yet challenging benchmarks, and also point to areas for improvement reasoning models.

# **References**

- Franc¸ois Chollet. On the Measure of Intelligence, November 2019. URL http://arxiv.org/ abs/1911.01547. arXiv:1911.01547 [cs].
- Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training Verifiers to Solve Math Word Problems, November 2021. URL http://arxiv.org/abs/2110.14168. arXiv:2110.14168 [cs].
- DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu

Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning, January 2025. URL http://arxiv.org/abs/2501.12948. arXiv:2501.12948 [cs].

- Google. Gemini 2.0 Flash Thinking Experimental, December 2024. URL https://deepmind. google/technologies/gemini/flash-thinking/.
- Aryan Gulati, Brando Miranda, Eric Chen, Emily Xia, Kai Fronsdal, Bruno de Moraes Dumont, and Sanmi Koyejo. Putnam-AXIOM: A Functional and Static Benchmark for Measuring Higher Level Mathematical Reasoning. In *The 4th Workshop on Mathematical Reasoning and AI at NeurIPS'24*, October 2024. URL https://openreview.net/forum?id= YXnwlZe0yf&noteId=yrsGpHd0Sf.
- Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring Mathematical Problem Solving With the MATH Dataset. In Joaquin Vanschoren and Sai-Kit Yeung (eds.), *Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, Virtual*, 2021. URL https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/ be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html.
- Bowen Jiang, Yangxinyu Xie, Zhuoqun Hao, Xiaomeng Wang, Tanwi Mallick, Weijie J. Su, Camillo J. Taylor, and Dan Roth. A Peek into Token Bias: Large Language Models Are Not Yet Genuine Reasoners, 2024. URL https://doi.org/10.48550/arXiv.2406.11050.
- Yifan Jiang, Filip Ilievski, Kaixin Ma, and Zhivar Sourati. BRAINTEASER: Lateral Thinking Puzzles for Large Language Models. In *Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing*, pp. 14317–14332, Singapore, 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.885. URL https: //aclanthology.org/2023.emnlp-main.885.
- Bin Lei, Yi Zhang, Shan Zuo, Ali Payani, and Caiwen Ding. MACM: Utilizing a Multi-Agent System for Condition Mining in Solving Complex Mathematical Problems. Technical Report arXiv:2404.04735, arXiv, July 2024. URL http://arxiv.org/abs/2404.04735.
- Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, and Mehrdad Farajtabar. GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models, October 2024. URL http://arxiv.org/abs/2410. 05229.
- Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candes, and Tatsunori Hashimoto. ` s1: Simple test-time scaling, January 2025. URL http://arxiv.org/abs/2501.19393. arXiv:2501.19393 [cs].
- OpenAI. OpenAI o1 System Card, 2024. URL https://openai.com/index/ openai-o1-system-card/.
- Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Sean Shi, Michael Choi, Anish Agrawal, Arnav Chopra, Adam Khoja, Ryan Kim, Richard Ren, Jason Hausenloy, Oliver Zhang, Mantas Mazeika, Summer Yue, Alexandr Wang, and Dan Hendrycks. Humanity's Last Exam, 2025.
- David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. GPQA: A Graduate-Level Google-Proof Q&A Benchmark. August 2024. URL https://openreview.net/forum?id=Ti67584b98# discussion.
- Joshua Rozner, Christopher Potts, and Kyle Mahowald. Decrypting Cryptic Crosswords: Semantically Complex Wordplay Puzzles as a Target for NLP. November 2021. URL https://openreview.net/forum?id=Ah5CMODl52.
- Jingmiao Zhao and Carolyn Jane Anderson. Solving and Generating NPR Sunday Puzzles with Large Language Models. In *Conference on Computational Creativity (ICCC)*, 2023.
- Qihuang Zhong, Kang Wang, Ziyang Xu, Juhua Liu, Liang Ding, and Bo Du. Achieving >97% on GSM8K: Deeply Understanding the Problems Makes LLMs Better Solvers for Math Word Problems. Technical Report arXiv:2404.14963, arXiv, October 2024. URL http://arxiv.org/abs/2404.14963.

