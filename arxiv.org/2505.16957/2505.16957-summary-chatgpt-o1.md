```markdown
## Summary of Key Findings

- **Core Concept (Section 1, Introduction)**
  - The paper identifies a novel attack vector employing malicious fonts, which manipulate code-to-glyph mappings to embed hidden adversarial prompts.  
  - These “invisible prompts” are unreadable to humans but fully interpretable by Large Language Models (LLMs) when they process external web pages or documents.

- **Attack Scenarios and Threat Models (Sections 3.2–3.4)**
  1. **Malicious Content Relay**  
     - Attacker embeds concealed instructions in webpages (HTML) or PDF documents (Figure 2).  
     - LLMs “relay” hidden prompts to users, effectively spreading disinformation or harmful instructions.  
     - Key factors influencing success:  
       - Prompt design (e.g., political vs. corporate redirection).  
       - Document format (HTML vs. PDF).  
       - Injection frequency (multiple hidden prompts increase success).  
       - Strategic placement (prompts placed in headers or early sections are processed more reliably).
  2. **Sensitive Data Leakage**  
     - Targets LLMs integrated with Model Context Protocol (MCP) tools (e.g., Gmail API).  
     - Hidden prompts instruct the LLM to exfiltrate user-sensitive data (phone numbers, SSNs, etc.) through email (Figure 3).  
     - Key factors influencing success:  
       - Data sensitivity level (low, medium, high) (Appendix B).  
       - Prior legitimate email requests (condition LLM to treat emails as routine).  
       - Hidden prompt design (direct vs. indirect wording).

- **Malicious Font Mechanics (Section 3.1, Appendix A)**
  - TrueType font specification allows code segments to map character codes to specific glyphs via offsets (idDelta).  
  - Attackers modify these mappings so text such as “3” is visually rendered as “a” (Figure 1).  
  - This creates visually benign documents that embed entirely different instructions at the text level.

- **Experimental Results (Section 4)**
  1. **Scenario 1: Content Relay**  
     - PDF content yields higher success (up to ~70%) compared to HTML (Figure 5).  
     - Repetitive injection of prompts increases success rates (Figure 4).  
     - Placement matters: prompts in top sections have higher success than lower placements (Figure 6).  
     - Cross-model analysis (Figure 7) shows variability: newer or more advanced LLMs (e.g., GPT-4.1) can be more vulnerable.
  2. **Scenario 2: Data Leakage**  
     - Low-sensitivity items (e.g., names, ages) are easy to leak, with up to 100% success for indirectly phrased prompts (Figure 8).  
     - Medium-sensitivity (phone numbers, geolocations) encounters partial resistance, yet high success remains when combined with indirect prompts and prior email usage.  
     - High-sensitivity data (credit cards, SSNs) typically trigger LLM safety mechanisms, although indirect prompts can still yield non-zero success (up to 30%).

- **Security Implications (Sections 5–6)**
  - **Capability-Security Tradeoff**: More advanced LLMs are adept at interpreting complex text but ironically more prone to sophisticated adversarial manipulation.  
  - **Protocol Vulnerabilities**: MCP-based autonomous actions (e.g., sending emails) expand attack surfaces, enabling automated data exfiltration.  
  - **Insufficient Defenses**: Existing antivirus and LLM safety filters do not detect malicious font manipulations (Appendix A.3).  
  - **Need for Robust Measures**:  
    - Enhanced content semantic verification (checking consistency between visual glyphs and underlying text).  
    - Stricter model-level safety filters for invisible text.  
    - Architectural defenses (e.g., sandboxing or hashing document content) to detect divergences in code-to-glyph mappings.

- **Limitations and Ethical Considerations (Sections “Limitations” and “Ethics Statement”)**
  - Experiments focus primarily on English texts. Other languages with complex scripts may show different vulnerabilities.  
  - Rapidly evolving LLMs may change threat landscapes; results may not fully generalize to future models.  
  - Research is performed in controlled settings with synthetic data and is ethically disclosed for responsible mitigation measures.

## Conclusion
- Malicious font injections pose a significant new challenge for LLM-driven systems, particularly under protocols like MCP where LLMs can autonomously interact with external services.  
- Attack success rates vary by document format, prompt obfuscation, data sensitivity, and the LLM’s security mechanisms.  
- The study underscores the urgency of developing holistic defenses that integrate visual integrity checks, advanced filtering of external resources, and content-level authenticity verification to mitigate these emerging threats.
```